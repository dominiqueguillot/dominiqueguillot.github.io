{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c71b53b4",
   "metadata": {},
   "source": [
    "# Lab 5: Coordinate descent\n",
    "\n",
    "We saw previously how [](S-coordinate-descent) can be used to minimize a function $f(x_1,\\dots,x_p)$ of several variables. Recall that the idea is to minimize the function one variable at a time, and to repeat this process over and over:\n",
    "\n",
    "\\begin{align*}\n",
    "x^{(k+1)}_1 &= \\mathop{\\textrm{argmin}}_x f(x, x_2^{(k)}, x_3^{(k)}, \\dots, x_p^{(k)}) \\\\\n",
    "x^{(k+1)}_2 &= \\mathop{\\textrm{argmin}}_x f(x_1^{(k+1)}, x, x_3^{(k)}, \\dots, x_p^{(k)}) \\\\\n",
    "x^{(k+1)}_3 &= \\mathop{\\textrm{argmin}}_x f(x_1^{(k+1)}, x_2^{(k+1)}, x, x_4^{(k)}, \\dots, x_p^{(k)})\\\\\n",
    "&\\vdots \\\\\n",
    "x^{(k+1)}_p &= \\mathop{\\textrm{argmin}}_x f(x_1^{(k+1)}, x_2^{(k+1)}, \\dots , x_{p-1}^{(k+1)}, x).\n",
    "\\end{align*}\n",
    "\n",
    "Under certain conditions, this process converges to a local minimum of the function $f$. To illustrate how this work with Python, let us solve the linear regression problem using coordinate descent.\n",
    "\n",
    "## Linear regression via coordinate descent\n",
    "\n",
    "Consider the least square problem\n",
    "\n",
    "$$\n",
    "\\min_{\\beta \\in \\mathbb{R}^p} \\|y-X\\beta\\|_2^2, \n",
    "$$\n",
    "\n",
    "where $y \\in \\mathbb{R}^n$ and $X \\in \\mathbb{R}^{n \\times p}$ are fixed. In order to solve this problem via coordinate descent, we need to solve the one-variable problem\n",
    "\n",
    "$$\n",
    "\\min_{\\beta_i \\in \\mathbb{R}^p} \\|y-X\\beta\\|_2^2.\n",
    "$$\n",
    "\n",
    "We will do so by computing the derivative of $\\|y-X\\beta\\|_2^2$ with respect to $\\beta_i$ and setting it to $0$. Recall from [Finding the Optimal Coefficients](sec-finding-optimal-coefficients) that \n",
    "\n",
    "$$\n",
    "\\nabla_\\beta \\|y-X\\beta\\|_2^2 = -2(X^Ty-X^TX \\beta) = -2X^T(y-X\\beta). \n",
    "$$\n",
    "\n",
    "Here, $\\nabla_\\beta$ denotes the gradient vector obtained by differentiating with respect to $\\beta_1, \\dots, \\beta_p$. In particular, for $1 \\leq i \\leq p$, the $i$-th entry of the gradient is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta_i} \\|y-X\\beta\\|_2^2 = -2 X_i^T (y-X\\beta), \n",
    "$$\n",
    "\n",
    "where $X_i$ denotes the $i$-th column of $X$. Now, let $X_{-i}$ denote the matrix $X$ with the $i$-th column deleted, and let $\\beta_{-i}$ be the vector $\\beta$ with $i$-th entry deleted. Observe that $X\\beta$ = $X_{-i} \\beta_{-i} + \\beta_i X_i$. It follows that \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\beta_i} \\|y-X\\beta\\|_2^2 &= -2 X_i^T (y-X_{-i}\\beta_{-i} - \\beta_i X_i) \\\\\n",
    "&= -2(X_i^T y - X_i^T X_{-i} \\beta_{-i} - \\beta_iX_i^T X_i).\n",
    "\\end{align*}\n",
    "\n",
    "Setting the gradient to $0$ and solving for $\\beta_i$, we obtain: \n",
    "\n",
    "$$\n",
    "\\beta_i^* = \\mathop{\\rm argmin}_{\\beta_i \\in \\mathbb{R}} \\|y-X\\beta\\|_2^2 = \\frac{X_i^T(y-X_{-i}\\beta{-i})}{X_i^T X_i}.\n",
    "$$\n",
    "\n",
    "We can therefore solve the linear regression via coordinate descent as follows. \n",
    "\n",
    "```{admonition} Linear regression - coordinate descent\n",
    "\n",
    "**Input**: $y \\in \\mathbb{R}^n$, $X \\in \\mathbb{R}^{n \\times p}$, initial guess $\\beta^{(0)}$, tolerence $\\epsilon > 0$.\n",
    "\n",
    "Set $k = 0$. \n",
    "\n",
    "while $\\|\\left(\\nabla \\|y-X\\beta^{(k)}\\|_2^2\\right)\\|_2 > \\epsilon$:\n",
    "\n",
    "1. $\\beta^{(k+1)} = \\beta^{(k)}$.\n",
    "2. For $i=1,\\dots,p$: \n",
    "\n",
    "$$\n",
    "\\beta_i^{(k+1)} = \\frac{X_i^T(y-X_{-i}\\beta{-i}^{(k+1)})}{X_i^T X_i}. \n",
    "$$\n",
    "\n",
    "3. Set $k = k+1$.\n",
    "\n",
    "**Output**: A vector $\\beta = \\beta^{(k)} \\in \\mathbb{R}^p$ such that $\\|\\nabla \\left(\\|y-X\\beta\\|_2^2\\right)\\|_2 \\leq \\epsilon$. \n",
    "\n",
    "```\n",
    "\n",
    "Let us implement this algorithm in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62bcb431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the norm of the gradient function\n",
    "\n",
    "def gradnorm(X,y,beta):\n",
    "    g = -2*X.T.dot(y-X.dot(beta))\n",
    "    return(np.linalg.norm(g))\n",
    "\n",
    "\n",
    "def least_squares(X, y, beta0 = None, epsilon=5e-3, maxit=1000):\n",
    "\t[n,p] = X.shape\n",
    "\t\n",
    "\tif beta0 == None:  # No initial guess was provided. Generate one at random.\n",
    "\t\tbeta = np.random.rand(p)\n",
    "\telse:\n",
    "\t\tbeta = beta0\n",
    "\t\t\n",
    "\tk = 0\n",
    "\tgrad = gradnorm(X,y,beta)\n",
    "\tprint(\"Iter \\t Grad \\t Err\")\n",
    "\twhile k < maxit and grad > epsilon:\n",
    "\t\tbeta_old = np.copy(beta)\n",
    "\t\tfor i in range(p):\n",
    "\t\t\tind = np.arange(p)\n",
    "\t\t\tind = np.delete(ind,i) # ind now contains all indices in {1,...,p} except i.\n",
    "\t\t\tXi = X[:,i]\n",
    "\t\t\tXmi = X[:,ind]  # All columns of X except the i-th.\n",
    "\t\t\tbetami = beta[ind]\n",
    "\t\t\tbeta[i] = Xi.dot(y-Xmi.dot(betami))/(Xi.dot(Xi))\n",
    "\t\t\t\t\n",
    "\t\tgrad = gradnorm(X, y, beta)\n",
    "\t\terr = np.linalg.norm(y-X.dot(beta)) # Evaluates the norm of y-X*beta\n",
    "\t\tk = k + 1\n",
    "\t\tprint(\"%d \\t %f \\t %f\" % (k, grad, err))\n",
    "\treturn(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845a3fa",
   "metadata": {},
   "source": [
    "Let us now test our code using random data and compare its solution with the solution of the problem obtained by Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33de8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\t\n",
    "def test():\n",
    "\tX = np.random.rand(10,3)\n",
    "\ty = np.random.rand(10)\n",
    "\tmod = LinearRegression(fit_intercept = False)\n",
    "\tmod.fit(X,y)\n",
    "\tbeta1 = mod.coef_\n",
    "\tbeta2 = least_squares(X,y, epsilon=1e-6, maxit=100)\n",
    "\tprint(beta1-beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da7bb234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter \t Grad \t Err\n",
      "1 \t 0.753578 \t 1.006546\n",
      "2 \t 0.488141 \t 0.970732\n",
      "3 \t 0.336244 \t 0.958816\n",
      "4 \t 0.229610 \t 0.954215\n",
      "5 \t 0.151386 \t 0.952266\n",
      "6 \t 0.095826 \t 0.951431\n",
      "7 \t 0.058243 \t 0.951086\n",
      "8 \t 0.034018 \t 0.950952\n",
      "9 \t 0.019095 \t 0.950903\n",
      "10 \t 0.010293 \t 0.950886\n",
      "11 \t 0.005326 \t 0.950881\n",
      "12 \t 0.002657 \t 0.950879\n",
      "13 \t 0.001308 \t 0.950878\n",
      "14 \t 0.000678 \t 0.950878\n",
      "15 \t 0.000402 \t 0.950878\n",
      "16 \t 0.000268 \t 0.950878\n",
      "17 \t 0.000185 \t 0.950878\n",
      "18 \t 0.000125 \t 0.950878\n",
      "19 \t 0.000081 \t 0.950878\n",
      "20 \t 0.000051 \t 0.950878\n",
      "21 \t 0.000031 \t 0.950878\n",
      "22 \t 0.000018 \t 0.950878\n",
      "23 \t 0.000010 \t 0.950878\n",
      "24 \t 0.000005 \t 0.950878\n",
      "25 \t 0.000003 \t 0.950878\n",
      "26 \t 0.000001 \t 0.950878\n",
      "27 \t 0.000001 \t 0.950878\n",
      "[ 1.21356782e-07 -2.26722343e-07  5.11193294e-08]\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c432e48",
   "metadata": {},
   "source": [
    "We obtain the same regression coefficients (up to numerical precision).\n",
    "\n",
    "## Solving the LASSO problem using coordinate descent\n",
    "\n",
    "Let us now consider the LASSO problem\n",
    "\n",
    "$$\n",
    "\\widehat{\\beta}_\\textrm{LASSO} = \\mathop{\\textrm{argmin}}_{\\beta \\in \\mathbb{R}^p} \\|y - X\\beta\\|_2^2 + \\alpha \\|\\beta\\|_1.\n",
    "$$\n",
    "\n",
    "Recall that the updated derived in [](S-LASSO-soln) is \n",
    "\n",
    "$$\n",
    "\\beta_i \\rightarrow \\eta^S_{\\lambda/\\|X_i\\|_2^2} \\left(\\frac{2 X_i^T (y-X_{-i} \\beta_{-i}) }{X_i^T X_i}\\right).\n",
    "$$\n",
    "\n",
    "This is almost the same update as in the linear regression problem. \n",
    "\n",
    "```{admonition} Exercise\n",
    "\n",
    "Modify the above code to solve the LASSO problem instead of linear regression. Compare your solution with Scikit-learn and make sure the two versions match. Note that the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\" target=\"_blank\">Lasso object of Scikit-learn</a> solves a slightly different (but equivalent) LASSO problem\n",
    "\n",
    "$$\n",
    "\\min_{w \\in \\mathbb{R}^p} \\frac{1}{2n}  ||y - Xw||^2_2 + \\alpha  ||w||_1.\n",
    "$$\n",
    "\n",
    "Observe that setting $\\alpha = \\frac{\\lambda}{2n}$ solves \n",
    "\n",
    "$$\n",
    "\\min_{w \\in \\mathbb{R}^p} \\frac{1}{2n} ||y - Xw||^2_2 + \\frac{\\lambda}{2n}  ||w||_1 = \\frac{1}{2n} \\min_{w \\in \\mathbb{R}^p} ||y - Xw||^2_2 + \\lambda  ||w||_1, \n",
    "$$\n",
    "\n",
    "which is equivalent to our formulation.\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
