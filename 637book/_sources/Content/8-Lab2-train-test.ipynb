{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "708d8836",
   "metadata": {},
   "source": [
    "(sec-lab2)=\n",
    "# Lab 2: Training vs testing error\n",
    "\n",
    "**Goals**: \n",
    "* Explore the difference between training error and test error.\n",
    "* Learn how to standardize data.\n",
    "\n",
    "**Useful commands**:\n",
    "* sklearn.model_selection.train_test_split\n",
    "* sklearn.preprocessing.PolynomialFeatures\n",
    "* sklearn.preprocessing.StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fac552",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Training vs testing error\n",
    "\n",
    "Recall that when we train a model, we select its parameters in order to minimize the prediction error on a training set. In order to measure how well the model will do on new data, we can evaluate it on a testing set (a part of the data that the model has not seen during training). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2128bbb",
   "metadata": {},
   "source": [
    "We are going to work with the *diabetes* dataset (see the <a href=\"https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset\" target=\"_blank\">documentation</a>)\n",
    "\n",
    "Let us load the dataset and create more features using polynomial transformations: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7de8902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "dataset = load_diabetes()\n",
    "X = dataset['data']\n",
    "y = dataset['target'].reshape(-1,1)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(3,include_bias = False)\n",
    "pdata = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a27fb",
   "metadata": {},
   "source": [
    "Note: NumPy has two types of n-dimensional vectors: those with shape (n,) and those with shape (n,1). When loaded, the variables y is of (n,) type. The *StandardScaler* that we will use later works with (n,1) vectors. The *reshape* command brings the vector to the (n,1) format to make sure we do not run into problems later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a953b8f",
   "metadata": {},
   "source": [
    "Let us display the dimension of the transformed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f54c7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 285)\n"
     ]
    }
   ],
   "source": [
    "print(pdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaed8a4",
   "metadata": {},
   "source": [
    "We can now split the data into a training and a testing set. (Note: the *random_state* is used for initializing the the random number generator that decides which samples go to the training and the testing sets. Setting it to a fixed value ensures the code will always produce the same results when run again.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eebea749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(pdata, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c8786",
   "metadata": {},
   "source": [
    "Our data has now been split into training and test sets. For example, the size of the training features is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "083f1fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(331, 285)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf69a8",
   "metadata": {},
   "source": [
    "We are now going to *scale* the data (i.e., subtract the mean and divide by the standard deviation). It is usually a good idea to scale data before fitting a model, especially if some of the variables are on different scales. While we could easily do that by hand, the StandardScaler from Scikit-learn can do it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b229105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "scaler = StandardScaler().fit(X_test)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler().fit(y_train)\n",
    "y_train_scaled = scaler.transform(y_train)\n",
    "\n",
    "scaler = StandardScaler().fit(y_test)\n",
    "y_test_scaled = scaler.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f0b44",
   "metadata": {},
   "source": [
    "Let us now compare the training and the test errors made by a linear regression model using (1) only the first 10 variables, and (2) using all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e23e20d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5190341891679049\n",
      "0.4849058889476756\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train[:,0:10],y_train)\n",
    "\n",
    "print(model.score(X_train[:, 0:10], y_train))\n",
    "print(model.score(X_test[:, 0:10], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d690e1c",
   "metadata": {},
   "source": [
    "Recall that the *score* method returns the [coefficient of determination ($R^2$)](sec-r-squared) of the model.\n",
    "\n",
    "In both cases, the training and testing $R^2$ are similar, at about $0.5$. Thus, the model is doing a reasonable job at modeling the training data and at predicting new values.\n",
    "\n",
    "Let us now try with all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "286a09e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90983817573718\n",
      "-55.921870574233004\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train)\n",
    "\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a2423c",
   "metadata": {},
   "source": [
    " Notice how the model does a much better job at modeling values on the training set. This is because we are including significantly more variables. However, the $R^2$ is **negative** on the test set, meaning that the test MSE of the model is worse than a constant prediction equal to the mean y value of the test set. This strongly suggests that the model is [overfitting](S-overfitting).\n",
    "\n",
    "```{note}\n",
    "As we saw before, when training a linear regression model, the MSE of the model will always be less than the MSE of a constant model (as long as we are including an intercept in the linear model). This implies $R^2 \\geq 0$. However, when testing, there is no such guarantee anymore: the model was trained on a different dataset so its performance could be worse than a constant prediction, resulting in a negative $R^2$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e845f07d",
   "metadata": {},
   "source": [
    "```{admonition} Exercise\n",
    "Use a *loop* to fit the linear regression model to the first $k$ variables, for $k=1,\\dots, 285$. For each model, save the training and test $R^2$. Finally, use *matplotlib* to make a plot of the training and test $R^2$ as a function of $k$. \n",
    "\n",
    "You should observe that the training $R^2$ always increases, but the testing $R^2$ increases and then start decreasing as the model begins to overfit.\n",
    "```\n",
    "\n",
    "```{admonition} Exercise\n",
    "Consider the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing\" target=\"_blank\">California Housing dataset</a> available with Scikit-learn. As above, split the samples into a training and testing set. Construct different linear models to predict the price of the houses using the 8 features and measure their performance on your test set.\n",
    "```\n",
    "\n",
    "```{admonition} Assignment\n",
    "---\n",
    "class: warning\n",
    "---\n",
    "Submit your Lab 2 work (including the above two exercise) as Homework 4 on <a href=\"https://sites.udel.edu/canvas/\" target=\"_blank\">Canvas</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
