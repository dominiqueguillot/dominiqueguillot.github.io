{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a2bb472",
   "metadata": {},
   "source": [
    "(C-categorical)=\n",
    "# Analyzing categorical data\n",
    "\n",
    "Many of the problems we considered so far involved predicting a *continuous* variable (e.g., the price of a car). In many real-world problems, we are instead interested in predicting a *categorical* variable, i.e., a variable that can take only finitely many values. While the difference may seem minor from a mathematical perspective, different techniques must be used to deal with the categorical setting. \n",
    "\n",
    "## Loss function\n",
    "\n",
    "When working with continuous data, we often use the mean-squared error (MSE) \n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n (\\widehat{y}_i - y_i)^2\n",
    "$$\n",
    "\n",
    "as a measure of error, where $\\widehat{y}_i$ is the predicted value of sample $y_i$. When fitting a model (e.g., linear regression), we then choose the model parameters to minimize that error (i.e., we choose the parameters to fit the training data as best as possible). \n",
    "\n",
    "Suppose now that the output $Y$ can only take finitely many values, say $\\{1, \\dots, K\\}$. Think of these as labels for data points. For example, suppose we have pictures of cats, dogs, and birds that we would like to automatically classify using a model. Let us label the images using \"1\" for cats, \"2\" for dogs, and \"3\" for birds. We can then aim to train a model to predict the class of each image as accurately as possible. \n",
    "\n",
    "```{note}\n",
    "\n",
    "Digital images can be seen as a collection of *pixels*, small squares with different colors. An image can therefore be encoded as a matrix whose $(i,j)-th$ entry is a number representing the color of the corresponding pixel. \n",
    "\n",
    "```{figure} images/pokemon.png\n",
    "---\n",
    "width: 200 px\n",
    "---\n",
    "An illustration of the pixels of an image\n",
    "\n",
    "An image of dimension $m \\times n$ can be reshaped into an $mn \\times 1$ vector by stacking the columns (or the rows) of the corresponding matrix. We can thus think of an image as a vector. The image classification problem therefore involves taking a vector representing an image as input, and returning the correct image label.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Suppose the correct label of a sample $y_1$ is $1$ (i.e., the first image is a cat). In the MSE setting, a predicted value of $3$ (bird) for $y_1$ is worst than a predicted value of $2$ (dog) since $(3-1)^2 > (2-1)^2$. However, here, the labels $1,2,3$ are completely arbitrary so a predicted label of $3$ is not really worse than a predicted label $2$. It therefore makes sense to use a different loss function. \n",
    "\n",
    "### Cross-entropy\n",
    "\n",
    "Instead of predicting the label of a given sample, many categorical models predict a probability distribution on the different labels. In that context, a very common loss function is **cross-entropy**. Cross-entropy can be seen as a measure of distance between two probability distribution. Here, a probability distribution on $\\{1,\\dots,K\\}$ is a collection of numbers $(p_1, \\dots, p_K)$ such that \n",
    "\n",
    "1. $0 \\leq p_i \\leq 1$ for all $i=1, \\dots, K$, \n",
    "2. $\\sum_{i=1}^K p_i = 1$. \n",
    "\n",
    "```{admonition} Definition (cross-entropy)\n",
    "\n",
    "Let $p, q$ be two probability distributions on $\\{1,\\dots, K\\}$. The *cross-entropy* $H(p,q)$ is given by \n",
    "\n",
    "$$\n",
    "H(p,q) := -\\sum_{i=1}^K p_i \\log q_i, \n",
    "$$\n",
    "\n",
    "where we use the convention $0 \\cdot \\log 0 = 0$. \n",
    "\n",
    "```\n",
    "\n",
    "Now, instead of using labels such as $1, 2, 3, etc.$ for the different categories, we use a **one-hot encoding**. This means that if $y_i$ has label $j$, we represent it as a $K$ dimensional vector with a $1$ in the $j$-th position, and zeros everywhere else:\n",
    "\n",
    "\\begin{align*}\n",
    "y_i = (0,0,\\dots,0, &\\underbrace{1},0,0,\\dots,0).\\\\\n",
    "&j \\textrm{-th}\n",
    "\\end{align*}\n",
    "\n",
    "Equivalently, we can think of $y_i$ as the probability distribution taking value $j$ with probability $1$. We can now compare the predicted probability distribution\n",
    "\n",
    "$$\n",
    "\\widehat{y}_i = (\\widehat{y}_1^{(1)}, \\dots, \\widehat{y}_i^{(K)})\n",
    "$$\n",
    "\n",
    "with $y_i$ using cross-entropy: \n",
    "\n",
    "$$\n",
    "H(y_i, \\widehat{y}_i) = -\\sum_{j=1}^K y_i^{(j)} \\log \\widehat{y}_i^{(j)}. \n",
    "$$\n",
    "\n",
    "Finally, we can average the cross-entropy over all the samples to measure how the model is doing\n",
    "\n",
    "$$\n",
    "L(y, \\widehat{y}) = \\frac{1}{n} \\sum_{i=1}^n H(y_i, \\widehat{y}_i) = -\\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^K y_i^{(j)} \\log \\widehat{y}_i^{(j)}. \n",
    "$$\n",
    "\n",
    "```{note}\n",
    "\n",
    "Cross-entropy finds its origin in information theory, where it is used to measure the expected number of bits needed to encode samples from a true discrete distribution $p$ when using a coding scheme optimized for a model distribution $q$. It is commonly used in probability theory to compare probability distributions.\n",
    "\n",
    "```\n",
    "\n",
    "#### Example\n",
    "\n",
    "Suppose with work with data having $3$ different categories. Assume one sample has label $y = (1,0,0)$. Consider two possible predictions $(1/3,1/3,1/3)$ and $(1/4, 1/2, 1/4)$. Let us compute which distribution is closest to $y$ with respect to cross-entropy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53f480ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0986122886681098\n",
      "1.3862943611198906\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(p,q):\n",
    "    K = len(p)\n",
    "    CE = 0\n",
    "    for j in range(K):\n",
    "        if p[j] != 0 and q[j] != 0:\n",
    "            CE -= p[j]*np.log(q[j])\n",
    "    return(CE)\n",
    "\n",
    "y = np.array([1,0,0])\n",
    "y1 = np.array([1/3,1/3,1/3])\n",
    "y2 = np.array([1/4,1/2,1/4])\n",
    "\n",
    "CE1 = cross_entropy(y,y1)\n",
    "CE2 = cross_entropy(y,y2)\n",
    "\n",
    "print(CE1)\n",
    "print(CE2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3bdd4a",
   "metadata": {},
   "source": [
    "We conclude that $(1/3,1/3,1/3)$ is \"closer\" to $y$ than $(1/4,1/2,1/4)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3d1967",
   "metadata": {},
   "source": [
    "## The nearest neighbors model\n",
    "\n",
    "A simple approach to predict the labels in the categorical setting is to use the neighbors of a points to guide the prediction. Suppose the predictors $x_1, \\dots x_n$ belong to the $N$-dimensional Euclidean space $\\mathbb{R}^N$ and assume the response $y_1, \\dots, y_n$ are categorical and encoded using a one-hot encoding as described above. For $x \\in \\mathbb{R}^N$ and an integer $1 \\leq k \\leq n-1$, let $N_k(x)$ denote the set of $k$ nearest neighbors of $x$ (i.e., the $k$ points closer to $x$ among $x_1, \\dots, x_n$). \n",
    "\n",
    "```{admonition} Definition ($k$ nearest neighbors predictor)\n",
    "\n",
    "In the above setting, the *$k$ nearest neighbors predictor* is\n",
    "\n",
    "$$\n",
    "\\widehat{Y}(x) = \\frac{1}{k} \\sum_{x_m \\in N_k(x)} y_m.\n",
    "$$\n",
    "\n",
    "```\n",
    "\n",
    "In other words, the $k$ nearest neighbors predictor returns the proportion of samples of each category in the $k$ neighborhood of $x$. New points can then be classified using the category with the largest proportion of neighbors. The nearest neighbors approach thus uses a \"majority vote\" based on the value of the nearest neighbors to make the prediction. This makes sense in scenarios where \"similar\" points typically have the same label. \n",
    "\n",
    "```{figure} images/Fig2p2.png\n",
    "---\n",
    "width: 300 px\n",
    "---\n",
    "An illustration of the $k$ nearest neighbors classifier with $k=15$. \n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider the following categorical dataset: \n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "2 & 5 \\\\\n",
    "-1 & 3 \\\\\n",
    "4 & 0 \\\\\n",
    "0 & -2 \\\\\n",
    "3 & 1\n",
    "\\end{bmatrix}, \\quad\n",
    "y =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "2 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "2\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Let us use Python to find the $3$ nearest classifier of the new point $x = [1,1]$. We first enter the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09c10438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Feature matrix (5 samples, 2 features)\n",
    "X = np.array([\n",
    "    [2, 5],\n",
    "    [-1, 3],\n",
    "    [4, 0],\n",
    "    [0, -2],\n",
    "    [3, 1]\n",
    "])\n",
    "\n",
    "n = X.shape[0] # Number of samples\n",
    "\n",
    "# Response vector\n",
    "y = np.array([0, 2, 1, 0, 2])\n",
    "\n",
    "# Let us convert y to a one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_one_hot = encoder.fit_transform(y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e74c22",
   "metadata": {},
   "source": [
    "We can verify that $y$ was encoded properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57ae601d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c364439",
   "metadata": {},
   "source": [
    "Let us now compute the 3 nearest neighbors predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de677982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.         0.66666667]]\n",
      "Final prediction: \n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Point to predict\n",
    "x = np.array([1,1])\n",
    "\n",
    "# Compute the distance between x and the samples\n",
    "\n",
    "d = np.zeros((n,1))\n",
    "for i in range(n):\n",
    "    d[i] = np.linalg.norm(x - X[i,:])\n",
    "\n",
    "# Find the index of the 3 nearest neighbors\n",
    "I = np.argsort(d, axis=0)  # Returns the indices of d from smallest to largest\n",
    "\n",
    "I3= I[0:3]  # 3 nearest neighbors\n",
    "\n",
    "# Average the labels of the nearest neighbors\n",
    "yhat = y_one_hot[I3,:].mean(axis=0)\n",
    "\n",
    "print(yhat)\n",
    "\n",
    "print(\"Final prediction: \")\n",
    "print(np.argmax(yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50adfd6a",
   "metadata": {},
   "source": [
    "Since the label with the largest proportion of $3$-neighbors of $x$ is \"2\", we classify the new point $x$ as \"2\". In some sense, we are relatively confident in our classification since $2/3$ of the closest neighbors have this label. \n",
    "\n",
    "```{note} \n",
    "One can think of several variants of nearest neighbors. For example, instead of using the $k$ nearest neighbors, one could use all the neighbors at distance at most $d$ for some given value of $d > 0$. In that case, if the number of such neighbors is small for a given $x$, one can decide to return no classification as there are not enough neighbors to make an informed decision. Another variant involves using all neighbors in the prediction, but to weight them with a weight that decreases with distance, say\n",
    "\n",
    "$$\n",
    "\\widehat{Y}(x) = \\sum_{i=1}^n d_i y_i, \n",
    "$$\n",
    "\n",
    "where $d_i = d_i(x) \\geq 0$. The classical $k$ nearest neighbors classifier arises in that way by setting $d_i = 1/k$ for the $k$ nearest neighbors and $d_i = 0$ otherwise. Another possible choice for $d_i$ is \n",
    "\n",
    "$$\n",
    "d_i(x) = \\frac{e^{-\\|x-x_i\\|_2^2}}{\\sum_{i=1}^n e^{-\\|x-x_i\\|_2^2}}, \n",
    "$$\n",
    "\n",
    "where points in the training set have a weight that decreases exponentially fast with distance. The denominator is used to make the weights sum to $1$. \n",
    "```\n",
    "\n",
    "```{admonition} Exercise\n",
    "\n",
    "Implement your own nearest neighbors classifier in Python. Your function should take $X, y, k, x$ as input and return the $k$ nearest neighbors classifier for $x$. Also implement some of the variants described above.\n",
    "```\n",
    "\n",
    "When using nearest neighbors, a value of $k$ needs to be chosen carefully. Although a small $k$ leads to a small training error, the model may not generalize well (large test error). The value of $k$ is typically chosen using [](sec-cross-validation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
