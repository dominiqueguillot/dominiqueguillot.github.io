
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>6. Learning outside the training set &#8212; Math 637 Mathematical Techniques in Data Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Content/7-Learning-outside-training';</script>
    <link rel="canonical" href="/637book/Content/7-Learning-outside-training.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7. Lab 2: Training vs testing error" href="8-Lab2-train-test.html" />
    <link rel="prev" title="5. Lab 1: Linear regression and the cars data" href="6-Lab-Cars.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="1-intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/637-logo.png" class="logo__image only-light" alt="Math 637 Mathematical Techniques in Data Science - Home"/>
    <script>document.write(`<img src="../_static/637-logo.png" class="logo__image only-dark" alt="Math 637 Mathematical Techniques in Data Science - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="1-intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2-Python.html">1. Setting up Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="3-Basic-Python.html">2. Basic Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="4-Supervised-Unsupervised.html">3. Supervised vs Unsupervised learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="5-Linear-regression.html">4. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="6-Lab-Cars.html">5. Lab 1: Linear regression and the cars data</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">6. Learning outside the training set</a></li>
<li class="toctree-l1"><a class="reference internal" href="8-Lab2-train-test.html">7. Lab 2: Training vs testing error</a></li>
<li class="toctree-l1"><a class="reference internal" href="9-BLUE.html">8. Best linear unbiased estimator and the bias-variance decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-Lab3-gradient-descent.html">9. Lab 3: Gradient descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="11-Shrinkage-methods.html">10. Improving linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-Model-selection.html">11. Model selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-Lab4-lasso.html">12. Lab 4: using the LASSO</a></li>
<li class="toctree-l1"><a class="reference internal" href="14-Computing-lasso.html">13. Computing the LASSO solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-Lab5-coordinate-descent.html">14. Lab 5: Coordinate descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="16-LASSO-theoretical.html">15. Theoretical guarantees for the LASSO</a></li>
<li class="toctree-l1"><a class="reference internal" href="17-categorical-data.html">16. Analyzing categorical data</a></li>
<li class="toctree-l1"><a class="reference internal" href="18-Lab6-nearest-neighbors.html">17. Lab 6: nearest neighbors</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Content/7-Learning-outside-training.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Learning outside the training set</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selecting-a-good-hypothesis-set">6.1. Selecting a good hypothesis set</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-least-squares">6.2. Example: least squares</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-test-error">6.3. Training and test error</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-coefficient-of-determination">6.4. The coefficient of determination</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation">6.5. Cross validation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-no-free-lunch-theorem">6.6. The no free lunch theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#theoretical-guarantees">6.7. Theoretical guarantees</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-learning-a-boolean-function-abu-mostafa-et-al-section-1-3-1">6.7.1. Example: learning a boolean function (Abu-Mostafa et al., Section 1.3.1)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-probabilistic-approach">6.7.2. A probabilistic approach</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sampling-marbles">6.7.2.1. Example: sampling marbles</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#back-to-predicting-outside-the-training-set">6.7.2.2. Back to predicting outside the training set</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="learning-outside-the-training-set">
<span id="c-learning-outside"></span><h1><span class="section-number">6. </span>Learning outside the training set<a class="headerlink" href="#learning-outside-the-training-set" title="Link to this heading">#</a></h1>
<p>We will now examine in more details the learning problem and its different components.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="../_images/learning_pb.png"><img alt="../_images/learning_pb.png" src="../_images/learning_pb.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 6.1 </span><span class="caption-text">Source: Abu-Mostafa et al., ``Learning from data’’.</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In a typical learning problem, one is interested to learn a function <span class="math notranslate nohighlight">\(f: \mathcal{X} \to \mathcal{Y}\)</span> connecting inputs to outputs. For example, the input could be a grayscale image containing a handwritten digit and the output the corresponding digit. One could imagine there is some optimal function <span class="math notranslate nohighlight">\(f\)</span> connecting such inputs to outputs. In order for the machine to approximate that function, several training pairs of inputs/outputs are provided.</p>
<p>The <em>hypothesis set</em> is a set of function that is being considered for <span class="math notranslate nohighlight">\(f\)</span>. This is often a set of functions parametrized by a finite dimensional vector space. For example, in linear regression, the hypothesis set <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is the set of linear functions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In machine learning, we usually restrict ourselves to a particular hypothesis set as the set of all functions from <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> to <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> is usually too big to search.</p>
</div>
<p>Once a hypothesis set has been selected, a learning algorithm searches the hypothesis set to find a function <span class="math notranslate nohighlight">\(g \in \mathcal{H}\)</span> that best matches the data. We call this function the final hypothesis.</p>
<section id="selecting-a-good-hypothesis-set">
<h2><span class="section-number">6.1. </span>Selecting a good hypothesis set<a class="headerlink" href="#selecting-a-good-hypothesis-set" title="Link to this heading">#</a></h2>
<p>When choosing a hypothesis set, one needs to find a good trade-off between underfitting and overfitting:</p>
<ul class="simple">
<li><p><strong>Underfitting</strong>: a model that is too simple will fail to capture the complexity of the data.</p></li>
<li><p><strong>Overfitting</strong>: a model that is too complex will learn all the small variations in the data and will generalize poorly to new data.</p></li>
</ul>
<p>To help guide this choice and provide a better guarantee that the model will generalize well, we proceed as follows:</p>
<ol class="arabic simple">
<li><p>We split our sample into 2 parts (training data and test data) as
uniformly as possible. People often use 75% training, 25% test.</p></li>
<li><p>We fit our model using the training data only. (This minimizes
the <strong>training error</strong>).</p></li>
<li><p>We use the fitted model to predict values of the test data and
compute the <strong>test error</strong> (This estimates the performance of the model on new data that were not seen during training).</p></li>
</ol>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/train-test.jpg"><img alt="../_images/train-test.jpg" src="../_images/train-test.jpg" style="width: 500px;" />
</a>
</figure>
</section>
<section id="example-least-squares">
<h2><span class="section-number">6.2. </span>Example: least squares<a class="headerlink" href="#example-least-squares" title="Link to this heading">#</a></h2>
<p>In the case of least squares, the regression coefficients estimated on the training set are given by (assuming the matrix is invertible):</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta} = (X^T_{\textrm{train}} X_{\textrm{train}})^{-1} X_{\textrm{train}}^T Y_{\textrm{train}}.
\]</div>
<p>The predicted values on the test set are therefore:</p>
<div class="math notranslate nohighlight">
\[
\widehat{Y}_{\textrm{test}} = X_{\textrm{test}}\hat{\beta}.
\]</div>
<p>Finally, the test error is:</p>
<div class="math notranslate nohighlight">
\[
\textrm{MSE}_{\textrm{test}} = \frac{1}{n_2} \sum_{i=1}^{n_2} (\widehat{Y}_{\textrm{test},i} - Y_{\textrm{test},i})^2.
\]</div>
<p>If we have different competing linear regression models (obtained from, say, different transformations of the variables), <strong>the test error can be used to guide the choice of the final model</strong>.</p>
</section>
<section id="training-and-test-error">
<h2><span class="section-number">6.3. </span>Training and test error<a class="headerlink" href="#training-and-test-error" title="Link to this heading">#</a></h2>
<p>The following figure illustrates the typical behavior of the training and test errors as a function of the complexity of the model.</p>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="../_images/ESL-Fig2p11.png"><img alt="../_images/ESL-Fig2p11.png" src="../_images/ESL-Fig2p11.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 6.2 </span><span class="caption-text">Source: ESL, Figure 2.11.</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>For example, in a linear regression model:</p>
<ul class="simple">
<li><p>As we keep adding more variables, the training error <strong>always</strong> decreases. This is because, when we add variables, the original model is contained in the bigger one (just set the regression coefficients corresponding to the new variables to <span class="math notranslate nohighlight">\(0\)</span>).</p></li>
<li><p>However, typically, the test error will start increasing at some point. This is because the model with too many variables becomes very flexible and starts learning irrelevant patterns in the data (the model is <strong>overfitting</strong>).</p></li>
</ul>
<div class="admonition-splitting-a-dataset-into-training-and-test-sets-with-python admonition" id="r-train-test-split">
<p class="admonition-title">Splitting a dataset into training and test sets with Python</p>
<p>Scikit-learn provides a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank">train_test_split</a> function to split the data automatically for
us. Try it to measure the test error of the different linear models you previously built for the JSE_Car_Lab dataset!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="c1"># Split data into training and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span>
<span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
<span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># Fit model on training data</span>
<span class="n">lin_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lin_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="c1"># Returns the coefficient of determination R^2.</span>
<span class="n">lin_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="the-coefficient-of-determination">
<span id="sec-r-squared"></span><h2><span class="section-number">6.4. </span>The coefficient of determination<a class="headerlink" href="#the-coefficient-of-determination" title="Link to this heading">#</a></h2>
<p>Regression models are often ranked using the <em>coefficient of determination</em> called “R squared” and denoted <span class="math notranslate nohighlight">\(R^2\)</span>. Suppose <span class="math notranslate nohighlight">\(y_1, y_2, \dots, y_n\)</span> are the outputs of a dataset and <span class="math notranslate nohighlight">\(\widehat{y}_1, \widehat{y}_2, \dots, \widehat{y}_n\)</span> are the associated values predicted by a model. Then the <span class="math notranslate nohighlight">\(R^2\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
R^2 = 1 - \frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{\sum_{i=1}^n (y_i-\overline{y})^2}, 
\]</div>
<p>where <span class="math notranslate nohighlight">\(\overline{y} := \frac{1}{n} \sum_{i=1}^n y_i\)</span> is the average of the <span class="math notranslate nohighlight">\(y_i\)</span>’s. Equivalently, the <span class="math notranslate nohighlight">\(R^2\)</span> equals 1 minus the ratio of the MSE of the model, divided by the MSE of a model that predicts <span class="math notranslate nohighlight">\(\widehat{y}_i = \overline{y}\)</span> for all <span class="math notranslate nohighlight">\(i=1,\dots,n\)</span>. Observe that <span class="math notranslate nohighlight">\(0 \leq R^2 \leq 1\)</span> for any linear regression model (assuming the model has an intercept). This is because the model is guaranteed to have a MSE smaller than a model with only an intercept.</p>
<p>When the <span class="math notranslate nohighlight">\(R^2\)</span> is close to <span class="math notranslate nohighlight">\(0\)</span>, the linear regression model is not doing much better than a constant prediction. The model becomes better as the <span class="math notranslate nohighlight">\(R^2\)</span> approaches <span class="math notranslate nohighlight">\(1\)</span>.</p>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>Consider the linear regression model <span class="math notranslate nohighlight">\(f(x) = \beta_0\)</span> (i.e., a model making a constant prediction as in the calculation of the <span class="math notranslate nohighlight">\(R^2\)</span>). It is not difficult to show that, given a training dataset</p>
<div class="math notranslate nohighlight">
\[
(\mathbf{x_1}, y_1), (\mathbf{x_2}, y_2), \dots, (\mathbf{x_n}, y_n), 
\]</div>
<p>the intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> that minimizes the MSE is <span class="math notranslate nohighlight">\(\overline{y}\)</span>. Indeed, let</p>
<div class="math notranslate nohighlight">
\[
g(\beta_0) = \frac{1}{n} \sum_{i=1}^n (y_i - \beta_0)^2. 
\]</div>
<p>Use the first derivative test to show that <span class="math notranslate nohighlight">\(g\)</span> is minimized when <span class="math notranslate nohighlight">\(\beta_0 = \overline{y}\)</span>.</p>
</div>
<p>The <em>score</em> method in <em>scikit-learn</em> returns the <span class="math notranslate nohighlight">\(R^2\)</span> as a measure of how well the model is doing (see <a class="reference internal" href="#r-train-test-split"><span class="std std-ref">(Splitting a dataset into training and test sets with Python)</span></a>).</p>
</section>
<section id="cross-validation">
<span id="sec-cross-validation"></span><h2><span class="section-number">6.5. </span>Cross validation<a class="headerlink" href="#cross-validation" title="Link to this heading">#</a></h2>
<p>A refined approach to splitting a dataset into a training and testing set is to use <strong>cross-validation</strong>. In cross-validation (CV), the samples are split into several subsets (called <em>folds</em>) that have roughly equal size. The model is fitted to all the folds <em>except the first one</em>. The testing error is then computed on the first fold. This process is repeated with the second fold removed, the third fold removed, etc.. At the end of the process, the average testing error can be computed and provides a rough estimate of how the model will perform on new data.</p>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="../_images/cv.png"><img alt="../_images/cv.png" src="../_images/cv.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 6.3 </span><span class="caption-text">Cross-validation with <span class="math notranslate nohighlight">\(5\)</span> folds where the third fold is currently used for testing.</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The <span class="math notranslate nohighlight">\(K\)</span>-fold cross-validation process is summarized below.</p>
<ol class="arabic simple">
<li><p>Split the samples into <span class="math notranslate nohighlight">\(K\)</span> equal (or almost equal) parts/folds at random.</p></li>
<li><p>FOR <span class="math notranslate nohighlight">\(j=1,\dots,K\)</span>:</p></li>
<li><p><span class="math notranslate nohighlight">\(\ \ \ \)</span> Fit the model on the data with fold <span class="math notranslate nohighlight">\(j\)</span> removed.</p></li>
<li><p><span class="math notranslate nohighlight">\(\ \ \ \)</span> Test the model on the <span class="math notranslate nohighlight">\(j\)</span>-th fold <span class="math notranslate nohighlight">\(\rightarrow\)</span> This is the <span class="math notranslate nohighlight">\(j\)</span>-th test error.</p></li>
<li><p>ENDFOR</p></li>
<li><p>Compute the average of the test errors obtained for each fold.</p></li>
</ol>
<p>The average error computed in the last step is called the <em>cross-validation error</em>. As we will see later, the cross-validation error provides a rigorous way to compare models and to pick the best one (in terms of its ability to predict new values well).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A typical choice for <span class="math notranslate nohighlight">\(K\)</span> is <span class="math notranslate nohighlight">\(10\)</span>. However, if the number of samples is small, one may want to pick a smaller value (e.g. 3 or 5) to make sure enough samples are available for training and testing during the cross-validation process.</p>
</div>
</section>
<section id="the-no-free-lunch-theorem">
<h2><span class="section-number">6.6. </span>The no free lunch theorem<a class="headerlink" href="#the-no-free-lunch-theorem" title="Link to this heading">#</a></h2>
<p>So far, we have only looked at the linear regression model. We will examine many other models in the upcoming chapters. One may wonder if there is a model that is uniformly superior to all others. For example, for learning things around us, we like to think our brain is pretty effective. An interesting theorem proved in 1996 by Wolpert shows that, in some sense, no learning algorithm is superior to all the others.</p>
<div class="admonition-no-free-lunch-theorem-wolpert-1996 admonition">
<p class="admonition-title">No Free Lunch Theorem (Wolpert, 1996)</p>
<p>Averaged over all possible data-generating distribution, every classification algorithm has the same error rate when classifying previously unobserved points.</p>
</div>
<p>We are not going to rigorously define what it means to “average over all possible data-generating distribution”, but roughly speaking, the theorem says that if you generate data accoding to a very large number of probability distributions, then the average test error of all models is about the same. Hence, given <strong>any</strong> learning algorithm, there is at least one dataset where it will not perform well. There is thus no <em>universal</em> learning algorithm that performs well in all situations.</p>
<p>In practice, however, we do not care about doing well on <strong>all</strong> datasets. A more reasonable goal is to understand what kind of data distributions are
relevant to the “real world” or in a certain field of study, and find learning algorithms that perform well on such datasets. For example, one can look for learning algorithms that work well for detecting objects in images.</p>
<p>In some sense, even our brain is very specialized: it works well for learning tasks involving the objects we regularly interact with (e.g., for distinguishing different animals). However, our brain is not effective for any task. For example, the images below contain 51% and 49% white pixels respectively. Is our brain really effective at learning to differentiate the two types of pictures? Would looking at thousands of images of each kind really help?</p>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="../_images/pixels-51.png"><img alt="../_images/pixels-51.png" src="../_images/pixels-51.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 6.4 </span><span class="caption-text">An image with 51% white pixels</span><a class="headerlink" href="#id4" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id5">
<a class="reference internal image-reference" href="../_images/pixels-49.png"><img alt="../_images/pixels-49.png" src="../_images/pixels-49.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 6.5 </span><span class="caption-text">An image with 49% white pixels</span><a class="headerlink" href="#id5" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="theoretical-guarantees">
<h2><span class="section-number">6.7. </span>Theoretical guarantees<a class="headerlink" href="#theoretical-guarantees" title="Link to this heading">#</a></h2>
<p>We now discuss how some theoretical guarantees can be provided that a model will perform well if it is trained on a large random sample.</p>
<p>As above, let <span class="math notranslate nohighlight">\(f: \mathcal{X} \to \mathcal{Y}\)</span> be a target function we are trying to approximate (e.g. how images of hand-written digits are related to the actual digit). Let <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> be our training dataset.</p>
<p><strong>Question:</strong> Does the training data tells us anything <em>outside</em> <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>?</p>
<section id="example-learning-a-boolean-function-abu-mostafa-et-al-section-1-3-1">
<h3><span class="section-number">6.7.1. </span>Example: learning a boolean function (Abu-Mostafa et al., Section 1.3.1)<a class="headerlink" href="#example-learning-a-boolean-function-abu-mostafa-et-al-section-1-3-1" title="Link to this heading">#</a></h3>
<p>Suppose <span class="math notranslate nohighlight">\(\mathcal{X} = \{0,1\}^3\)</span> and <span class="math notranslate nohighlight">\(\mathcal{Y} = \{0,1\}\)</span>, i.e., we are trying to learn a boolean function on <span class="math notranslate nohighlight">\(\{0,1\}^3\)</span>.</p>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>Explain why there are <span class="math notranslate nohighlight">\(2^{2^3} = 256\)</span> such functions.</p>
</div>
<p>Suppose we are given the following training data, where the black dots indicate a value of <span class="math notranslate nohighlight">\(0\)</span> and the white dots a value of <span class="math notranslate nohighlight">\(1\)</span>:</p>
<figure class="align-default" id="id6">
<a class="reference internal image-reference" href="../_images/Learning_1.3.1.png"><img alt="../_images/Learning_1.3.1.png" src="../_images/Learning_1.3.1.png" style="width: 150px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 6.6 </span><span class="caption-text">Source: Abu-Mostafa et al., ``Learning from data’’.</span><a class="headerlink" href="#id6" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Observe that we know the values of <span class="math notranslate nohighlight">\(f\)</span> on <span class="math notranslate nohighlight">\(5\)</span> of the <span class="math notranslate nohighlight">\(2^3 = 8\)</span> inputs (62.5% of the possible values). Does that tell us anything about the values of <span class="math notranslate nohighlight">\(f\)</span> on the remaining three possible inputs? The figure below displays all the functions that match out training data</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/Learning_1.3.3_b.png"><img alt="../_images/Learning_1.3.3_b.png" src="../_images/Learning_1.3.3_b.png" style="width: 500px;" />
</a>
</figure>
<p>Clearly, we are free to choose the values of <span class="math notranslate nohighlight">\(f\)</span> on the remaining 3 points of <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> arbitrarily.</p>
</section>
<section id="a-probabilistic-approach">
<h3><span class="section-number">6.7.2. </span>A probabilistic approach<a class="headerlink" href="#a-probabilistic-approach" title="Link to this heading">#</a></h3>
<p>The previous example shows that, in general, there is no hope to learn the unknown function <span class="math notranslate nohighlight">\(f\)</span> exactly in a given learning problem. Even if our final hypothesis <span class="math notranslate nohighlight">\(g\)</span> does well on the training set, there is no guarantee it will do well outside the training set. However, we can show that with a large enough dataset, we can learn <span class="math notranslate nohighlight">\(f\)</span> well enough <em>outside <span class="math notranslate nohighlight">\(\mathcal{D}\)</span></em> with <strong>high probability</strong>.</p>
<section id="example-sampling-marbles">
<h4><span class="section-number">6.7.2.1. </span>Example: sampling marbles<a class="headerlink" href="#example-sampling-marbles" title="Link to this heading">#</a></h4>
<p>Consider a bin with red and green marbles. Let <span class="math notranslate nohighlight">\(\mu\)</span> denote the exact proportion of red marbles in the bin. Assume this number is unknown to us. Naturally, we can sample marbles at random and use the proportion of red marbles in that sample as an estimate for <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<figure class="align-default" id="id7">
<a class="reference internal image-reference" href="../_images/Learning_1.3.2.png"><img alt="../_images/Learning_1.3.2.png" src="../_images/Learning_1.3.2.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 6.7 </span><span class="caption-text">Source: Abu-Mostafa et al., ``Learning from data’’.</span><a class="headerlink" href="#id7" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Notice how the above problem arises when performing a survey to estimate the proportion of people having a given opinion (e.g. in an election poll).</p>
</div>
<p>Suppose we sample <span class="math notranslate nohighlight">\(N\)</span> marbles with replacement to infer the value of <span class="math notranslate nohighlight">\(\mu\)</span>. Note that there is no guarantee that we can learn <span class="math notranslate nohighlight">\(\mu\)</span> <em>exactly</em>. For example, even if <span class="math notranslate nohighlight">\(\mu = 0.1\)</span>, we could end up picking the same red marble <span class="math notranslate nohighlight">\(N\)</span> times and conclude <span class="math notranslate nohighlight">\(\mu = 1\)</span>. However, the probability of the above happening is ridiculously small. We <em>can</em> learn <span class="math notranslate nohighlight">\(\mu\)</span> <em>with high probability</em> if we sample enough marbles.</p>
<p>The following inequality provides a bound on how good our estimate of <span class="math notranslate nohighlight">\(\mu\)</span> is as a function of the number of marbles <span class="math notranslate nohighlight">\(N\)</span> we sampled.</p>
<div class="admonition-theorem-hoeffding-s-inequality admonition">
<p class="admonition-title"><strong>Theorem:</strong> (Hoeffding’s Inequality)</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \dots, X_N\)</span> be independent random variables such that <span class="math notranslate nohighlight">\(a_i \leq X_i \leq b_i\)</span> almost surely. Consider the sum</p>
<div class="math notranslate nohighlight">
\[
S_N = X_1 + X_2 + \dots + X_N.
\]</div>
<p>Then, for any <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>,</p>
<div class="math notranslate nohighlight">
\[
P\left(|S_N - E(S_N)| \geq \epsilon \right) \leq 2 \exp\left(-\frac{2 \epsilon^2}{\sum_{i=1}^N (b_i-a_i)^2}\right).
\]</div>
</div>
<p>Let us see how we can apply Hoeffding’s Inequality to our problem. Let</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X_i = \begin{cases}
1 &amp; \textrm{if the } i\textrm{-th marble is red} \\
0 &amp; \textrm{otherwise}.
\end{cases}
\end{split}\]</div>
<p>We have:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S_N = \)</span> number of red marbles picked (random)</p></li>
<li><p><span class="math notranslate nohighlight">\(\nu := S_N / N = \)</span> estimated value of <span class="math notranslate nohighlight">\(\mu\)</span> (random).</p></li>
</ul>
<p>Observe that:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E(X_i) = 1 \times \mu + 0 \times (1-\mu) = \mu\)</span> and so <span class="math notranslate nohighlight">\(E(\nu) = E(S_N/N) = \mu\)</span>.</p></li>
<li><p>It is natural to assume <span class="math notranslate nohighlight">\(X_1, X_2, \dots, X_N\)</span> are independent.</p></li>
<li><p>We have <span class="math notranslate nohighlight">\(0 \leq X_i \leq 1\)</span>.</p></li>
</ul>
<p>Thus, by Hoeffding’s inequality, for any <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[
P(|\nu - \mu| &gt; \epsilon) = P(|S_N - E(S_N)| \geq \epsilon N) \leq 2\exp\left(-2 \frac{\epsilon^2 N^2}{N}\right) = 2 e^{-2 \epsilon^2 N}. 
\]</div>
<p>In particular, for any <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
P(|\nu - \mu| &gt; \epsilon) \to 0 \textrm{ as } N \to \infty. 
\]</div>
<p>Thus, for any choice of error <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>, the probability tha the estimated value <span class="math notranslate nohighlight">\(\nu\)</span> differs from <span class="math notranslate nohighlight">\(\mu\)</span> by more than <span class="math notranslate nohighlight">\(\epsilon\)</span> becomes very small as the number of samples <span class="math notranslate nohighlight">\(N\)</span> increases.</p>
<p>We can say even more. Suppose we want to make sure our estimate is accurate at <span class="math notranslate nohighlight">\(\pm 5\%\)</span> with a <span class="math notranslate nohighlight">\(95\%\)</span> probability. Can we pick the value of <span class="math notranslate nohighlight">\(N\)</span> to guarantee that? In that case, we have</p>
<div class="math notranslate nohighlight">
\[
P(|\nu - \mu| &gt; 0.05) \leq 2 e^{-2 \times 0.05^2 N}. 
\]</div>
<p>Hence, we want to pick <span class="math notranslate nohighlight">\(N\)</span> large enough so that <span class="math notranslate nohighlight">\(2 e^{-2 \times 0.05^2 N} \geq 0.95\)</span>. Solving for <span class="math notranslate nohighlight">\(N\)</span>, we obtain <span class="math notranslate nohighlight">\(N \geq  149\)</span>. We can thus guarantee that our estimate is accurate at <span class="math notranslate nohighlight">\(\pm 5\%\)</span> with probability <span class="math notranslate nohighlight">\(95\%\)</span> if <span class="math notranslate nohighlight">\(N \geq 149\)</span>.</p>
<p>In general, observe that if <span class="math notranslate nohighlight">\(\epsilon\)</span> is very small (the estimate is very precise), the sample size <span class="math notranslate nohighlight">\(N\)</span> needs to be larger to guarantee accuracy of the estimator.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Suppose for example that we want to survey the University’s population to know if people are in favor of a given project (say, having more online courses). The above calculation shows that if we survey at least <span class="math notranslate nohighlight">\(149\)</span> people (as randomly as possible), the estimated proportion of people in favor will be accurate at <span class="math notranslate nohighlight">\(\pm 5\%\)</span> with a <span class="math notranslate nohighlight">\(95\%\)</span> probability.</p>
</div>
</section>
<section id="back-to-predicting-outside-the-training-set">
<h4><span class="section-number">6.7.2.2. </span>Back to predicting outside the training set<a class="headerlink" href="#back-to-predicting-outside-the-training-set" title="Link to this heading">#</a></h4>
<p>We will now show how the above arguments can be applied to the learning problem. Assume in our learning problem that the data is given to us at random according to some unknown probability distribution. Also assume we already have a training dataset of size <span class="math notranslate nohighlight">\(N\)</span>. We want to know if a model trainined on the training set is guaranteed to perform well on the test set.</p>
<p>We will use the following notation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
[[\textrm{statement}]] = \begin{cases}
1 &amp; \textrm{ if statement is true and }\\
0 &amp; \textrm{ otherwise}.
\end{cases}
\end{split}\]</div>
<p>We define the:</p>
<ul class="simple">
<li><p>In-sample error (training error):</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
E_{\textrm{in}}(g) &amp;= (\textrm{fraction of $\mathcal{D}$ where $f$ and $g$ disagree}) \\
&amp;= \frac{1}{N} \sum_{n=1}^N [[g({\bf x_n}) \ne f({\bf x_n})]]
\end{align*}\]</div>
<ul class="simple">
<li><p>Out-of-sample error:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
E_{\textrm{out}}(g) = P(g({\bf x}) \ne f({\bf x})), 
\]</div>
<p>where <span class="math notranslate nohighlight">\({\bf x}\)</span> is sampled according to the unknown data distribution.</p>
<p>Our goal is to:</p>
<ul class="simple">
<li><p>Find an hypothesis <span class="math notranslate nohighlight">\(g\)</span> for which <span class="math notranslate nohighlight">\(E_{\textrm{in}}(g)\)</span> is small (good training error).</p></li>
<li><p>Prove that <span class="math notranslate nohighlight">\(E_{\textrm{out}}(g)\)</span> and <span class="math notranslate nohighlight">\(E_{\textrm{in}}(g)\)</span> are not too different with high probability.</p></li>
<li><p>Conclude that <span class="math notranslate nohighlight">\(E_{\textrm{out}}(g)\)</span> is small with high probability.</p></li>
</ul>
<p>We do this by relating the problem to the problem of sampling marbles described above.</p>
<p>To simplify, let us assume there are only a finite number of hypotheses:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{H} = \{h_1, h_2, \dots, h_M\}.
\]</div>
<p>(Recall that the hypothesis set is the set of functions we are considering to approximate <span class="math notranslate nohighlight">\(f\)</span>.)</p>
<p>Let us pick the hypothesis that does the best on the training set (i.e., <span class="math notranslate nohighlight">\(E_{\textrm{in}}(g)\)</span> is small). We would like to guarantee that <span class="math notranslate nohighlight">\(E_{\textrm{out}}(g)\)</span> is small as well.</p>
<p>Each <span class="math notranslate nohighlight">\(h_i\)</span> disagrees with <span class="math notranslate nohighlight">\(f\)</span> at certain points <span class="math notranslate nohighlight">\({\bf x_i}\)</span> (red marble) and agrees with <span class="math notranslate nohighlight">\(f\)</span> at certain points (green marble):</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/Fig-1.10.png"><img alt="../_images/Fig-1.10.png" src="../_images/Fig-1.10.png" style="width: 500px;" />
</a>
</figure>
<p>Observe how our training set provides a random sample of marbles for which we know the proportion of red balls. Hoeffding’s inequality holds in each bin <em>individually</em>:</p>
<div class="math notranslate nohighlight">
\[
P(|E_{\textrm{in}}(h_i) - E_{\textrm{out}}(h_i)| &gt; \epsilon) \leq 2e^{-2\epsilon^2 N} \qquad (i=1,\dots, M)
\]</div>
<p>and guarantees that if a model does well on the training set (proportion of red marbles is small), then it will do well on the test set (assuming <span class="math notranslate nohighlight">\(N\)</span> is large enough).</p>
<p>There is just one more subtlety that we need to address before concluding out model will do well on the test set: the above bound assumes <span class="math notranslate nohighlight">\(h_i\)</span> is fixed before the sample is seen. However, in practice, we want to pick the “best” <span class="math notranslate nohighlight">\(h_i\)</span> (the one with the smallest in-sample error). We therefore decide which <span class="math notranslate nohighlight">\(h_i\)</span> to pick <em>after</em> seeing the data so the above bound does not immediately apply. Right now, we know:</p>
<div class="math notranslate nohighlight">
\[
P(|E_{\textrm{in}}(h_i) - E_{\textrm{out}}(h_i)| &gt; \epsilon) \textrm{ is small for any particular } h_i.
\]</div>
<p>What we want is:</p>
<div class="math notranslate nohighlight">
\[
P(|E_{\textrm{in}}(g) - E_{\textrm{out}}(g)| &gt; \epsilon) \textrm{ is small for the final hypothesis } g, 
\]</div>
<p>where <span class="math notranslate nohighlight">\(g\)</span> is the <span class="math notranslate nohighlight">\(h_i\)</span> with the smallest training error. We can easily fix the problem using a <em>union bound</em></p>
<div class="admonition-the-union-bound-in-probability-theory admonition">
<p class="admonition-title">The union bound in probability theory:</p>
<p>Suppose <span class="math notranslate nohighlight">\(E_1, E_2, \dots, E_M\)</span> are events. Then the probability of their union (at least one of the events happens) is bounded by</p>
<div class="math notranslate nohighlight">
\[
P(E_1 \cup E_2 \cup \dots \cup E_M) \leq \sum_{i=1}^M P(E_i). 
\]</div>
</div>
<p>We apply the union bound as follows:</p>
<p>If</p>
<div class="math notranslate nohighlight">
\[
|E_{\textrm{in}}(g) - E_{\textrm{out}}(g)| &gt; \epsilon
\]</div>
<p>then we must have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\ \  |E_{\textrm{in}}(h_1) - E_{\textrm{out}}(h_1)| &gt; \epsilon \\
&amp;\textrm{or } |E_{\textrm{in}}(h_2) - E_{\textrm{out}}(h_2)| &gt; \epsilon \\
&amp; \dots \\
&amp;\textrm{or } |E_{\textrm{in}}(h_M) - E_{\textrm{out}}(h_M)| &gt; \epsilon.
\end{align*}\]</div>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{B}_i\)</span> be the event:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{B}_i := |E_{\textrm{in}}(h_i) - E_{\textrm{out}}(h_i)| &gt; \epsilon.
\]</div>
<p>Then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;P(|E_{\textrm{in}}(g) - E_{\textrm{out}}(g)| &gt; \epsilon) \leq P(\mathcal{B}_1 \cup \mathcal{B}_2 \cup \dots \cup \mathcal{B}_M) \\
&amp;\leq \sum_{i=1}^M P(\mathcal{B}_i) = \sum_{i=1}^M P(|E_{\textrm{in}}(h_i) - E_{\textrm{out}}(h_i)| &gt; \epsilon) \\
&amp;\leq \sum_{i=1}^M 2e^{-2\epsilon^2 N} \\
&amp;= 2M e^{-2\epsilon^2 N}.
\end{align*}\]</div>
<p><strong>Conclusion:</strong> We get <span class="math notranslate nohighlight">\(P(|E_{\textrm{in}}(g) - E_{\textrm{out}}(g)| &gt; \epsilon) \to 0\)</span> as <span class="math notranslate nohighlight">\(N \to \infty\)</span> (with an explicit control on the error).</p>
<p>The downside of the above argument is it requires <span class="math notranslate nohighlight">\(M\)</span> to be finite (and we get a looser bound). This can be improved with more work.</p>
<p>The above calculations show that (under the assumptions we made), we are guaranteed that a chosen model that performs well on a large enough training set will also perform well on new data with high probability. This provides a <strong>theoretical guarantee</strong> that learning the function <span class="math notranslate nohighlight">\(f\)</span> is possible (with high probability). We note, however, the the kind of bounds we obtained are typically not very useful in practice in the learning problem (for example, to guide the choice of the training set size, as we did above for the pooling problem).</p>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="6-Lab-Cars.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Lab 1: Linear regression and the cars data</p>
      </div>
    </a>
    <a class="right-next"
       href="8-Lab2-train-test.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Lab 2: Training vs testing error</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selecting-a-good-hypothesis-set">6.1. Selecting a good hypothesis set</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-least-squares">6.2. Example: least squares</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-test-error">6.3. Training and test error</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-coefficient-of-determination">6.4. The coefficient of determination</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation">6.5. Cross validation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-no-free-lunch-theorem">6.6. The no free lunch theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#theoretical-guarantees">6.7. Theoretical guarantees</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-learning-a-boolean-function-abu-mostafa-et-al-section-1-3-1">6.7.1. Example: learning a boolean function (Abu-Mostafa et al., Section 1.3.1)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-probabilistic-approach">6.7.2. A probabilistic approach</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sampling-marbles">6.7.2.1. Example: sampling marbles</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#back-to-predicting-outside-the-training-set">6.7.2.2. Back to predicting outside the training set</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dominique Guillot
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>