
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>9. Lab 3: Gradient descent &#8212; Math 637 Mathematical Techniques in Data Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Content/10-Lab3-gradient-descent';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="10. Improving linear regression" href="11-Shrinkage-methods.html" />
    <link rel="prev" title="8. Best linear unbiased estimator and the bias-variance decomposition" href="9-BLUE.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="1-intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/637-logo.png" class="logo__image only-light" alt="Math 637 Mathematical Techniques in Data Science - Home"/>
    <script>document.write(`<img src="../_static/637-logo.png" class="logo__image only-dark" alt="Math 637 Mathematical Techniques in Data Science - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="1-intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2-Python.html">1. Setting up Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="3-Basic-Python.html">2. Basic Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="4-Supervised-Unsupervised.html">3. Supervised vs Unsupervised learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="5-Linear-regression.html">4. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="6-Lab-Cars.html">5. Lab 1: Linear regression and the cars data</a></li>
<li class="toctree-l1"><a class="reference internal" href="7-Learning-outside-training.html">6. Learning outside the training set</a></li>
<li class="toctree-l1"><a class="reference internal" href="8-Lab2-train-test.html">7. Lab 2: Training vs testing error</a></li>
<li class="toctree-l1"><a class="reference internal" href="9-BLUE.html">8. Best linear unbiased estimator and the bias-variance decomposition</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">9. Lab 3: Gradient descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="11-Shrinkage-methods.html">10. Improving linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-Model-selection.html">11. Model selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-Lab4-lasso.html">12. Lab 4: using the LASSO</a></li>
<li class="toctree-l1"><a class="reference internal" href="14-Computing-lasso.html">13. Computing the LASSO solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-Lab5-coordinate-descent.html">14. Lab 5: Coordinate descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="16-LASSO-theoretical.html">15. Theoretical guarantees for the LASSO</a></li>
<li class="toctree-l1"><a class="reference internal" href="17-categorical-data.html">16. Analyzing categorical data</a></li>
<li class="toctree-l1"><a class="reference internal" href="18-Lab6-nearest-neighbors.html">17. Lab 6: nearest neighbors</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Content/10-Lab3-gradient-descent.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lab 3: Gradient descent</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-direction-of-steepest-descent">9.1. The direction of steepest descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gradient-descent-algorithm">9.2. The gradient descent algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-a-basic-gradient-descent">9.3. Implementing a basic gradient descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-pytorch">9.4. Using PyTorch</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="lab-3-gradient-descent">
<h1><span class="section-number">9. </span>Lab 3: Gradient descent<a class="headerlink" href="#lab-3-gradient-descent" title="Link to this heading">#</a></h1>
<p><strong>Goals</strong>:</p>
<ul class="simple">
<li><p>Explore how gradient descent can be used to minimize a function</p></li>
<li><p>Learn to use PyTorch to optimize functions</p></li>
</ul>
<p>As we saw before, the parameters of models for data are typically estimated by minimizing a <a class="reference internal" href="5-Linear-regression.html#sec-loss-function"><span class="std std-ref">loss function</span></a>. It is thus very important to have tools to efficiently minimize functions.</p>
<p>A very popular approach in machine learning is to use the <em>gradient descent</em> algorithm. This is a <em>first-order method</em> in the sense that it only uses the first derivatives of the function to perform the optimization. While many higher-order methods also exist, they are often too computationally intensive to be applied to large datasets.</p>
<section id="the-direction-of-steepest-descent">
<h2><span class="section-number">9.1. </span>The direction of steepest descent<a class="headerlink" href="#the-direction-of-steepest-descent" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> be a function of <span class="math notranslate nohighlight">\(n\)</span> variables. Recall that its gradient of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span> is the vector</p>
<div class="math notranslate nohighlight">
\[
\nabla f(x) = \left(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n}\right)^T.
\]</div>
<p>The gradient is closely related to the <em>directional derivative</em>, i.e., the derivative of <span class="math notranslate nohighlight">\(f\)</span> in a given direction <span class="math notranslate nohighlight">\(v \in \mathbb{R}^n\)</span>. Typically, we assume the direction is which we take the derivative is given by a vector of length <span class="math notranslate nohighlight">\(1\)</span>, i.e., <span class="math notranslate nohighlight">\(\|v\|_2 = (\sum_{i=1}^n v_i^2)^{1/2} = 1\)</span>. The directional derivative of <span class="math notranslate nohighlight">\(f\)</span> in the direction <span class="math notranslate nohighlight">\(v\)</span> at the point <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span>, denoted by <span class="math notranslate nohighlight">\(D_v f(x)\)</span> is then given by</p>
<div class="math notranslate nohighlight">
\[
D_v f(x) = \lim_{h \to 0} \frac{f(x+h v)-f(x)}{h}. 
\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As a special case, if <span class="math notranslate nohighlight">\(f: \mathbb{R}^2 \to \mathbb{R}\)</span> is a function of two variables, then for <span class="math notranslate nohighlight">\(v = (1,0)^T\)</span>, we have <span class="math notranslate nohighlight">\(D_v f(x,y) = \frac{\partial f}{\partial x}\)</span>. Similarly, when <span class="math notranslate nohighlight">\(v = (0,1)^T\)</span>, we obtain <span class="math notranslate nohighlight">\(D_v f(x,y) = \frac{\partial f}{\partial y}\)</span>. These are the derivatives of <span class="math notranslate nohighlight">\(f\)</span> taken along the <span class="math notranslate nohighlight">\(x\)</span> and the <span class="math notranslate nohighlight">\(y\)</span> axis respectively.</p>
</div>
<p>One can show that the directional derivative in the direction <span class="math notranslate nohighlight">\(v\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
D_v f(x) = \nabla f(x) \cdot v, 
\]</div>
<p>where, for vectors <span class="math notranslate nohighlight">\(u, v \in \mathbb{R}^n\)</span>, we use the notation <span class="math notranslate nohighlight">\(u \cdot v = \sum_{i=1}^n u_i v_i\)</span> for the <em>dot-product</em> of <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(v\)</span>. As a consequence of this formula, we can show that the directional derivative is maximal in the direction of the gradient. This is the direction of <em>steepest ascent</em>, i.e., the direction where the function <span class="math notranslate nohighlight">\(f\)</span> increases the most around the point <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="admonition-theorem-direction-of-steepest-ascent admonition">
<p class="admonition-title">Theorem: (Direction of steepest ascent)</p>
<p>Let <span class="math notranslate nohighlight">\(f: \mathbb{R}^n \to \mathbb{R}\)</span>. Then the direction <span class="math notranslate nohighlight">\(v \in \mathbb{R}^n\)</span> with <span class="math notranslate nohighlight">\(\|v\|_2 = 1\)</span> for which <span class="math notranslate nohighlight">\(D_v f(x)\)</span> is the largest is</p>
<div class="math notranslate nohighlight">
\[
v = \frac{\nabla f(x)}{\|\nabla f(x)\|}.  
\]</div>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Proof:</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Since <span class="math notranslate nohighlight">\(D_v f(x) = \nabla f(x) \cdot v\)</span>, using the <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality" target="_blank"> Cauchy-Schwarz inequality</a>, we obtain</p>
<div class="math notranslate nohighlight">
\[
|D_v f(x)| = |\nabla f(x) \cdot v| \leq \|\nabla f(x)\|_2 \cdot \|v\|_2 = \|\nabla f(x)\|_2, 
\]</div>
<p class="sd-card-text">where the last equality holds since <span class="math notranslate nohighlight">\(\|v\|_2 = 1\)</span>. Using the equality case of the Cauchy–Schwarz inequality, equality holds if and only if <span class="math notranslate nohighlight">\(v = \lambda \nabla f(x)\)</span> for some <span class="math notranslate nohighlight">\(\lambda \in \mathbb{R}\)</span>. Since <span class="math notranslate nohighlight">\(\|v\|_2 = 1\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
1 = \|v\|_2 = |\lambda| \cdot \|\nabla f(x)\|_2. 
\]</div>
<p class="sd-card-text">Thus, the directional derivative is maximal when <span class="math notranslate nohighlight">\(\lambda = \frac{1}{\|\nabla f(x)\|_2}\)</span>, i.e., when <span class="math notranslate nohighlight">\(v = \frac{\nabla f(x)}{\|\nabla f(x)\|}\)</span>. For that choice of <span class="math notranslate nohighlight">\(v\)</span>, we obtain <span class="math notranslate nohighlight">\(D_v f(x) = \|\nabla f(x)\|_2\)</span>, the maximal possible value of the directional derivative.</p>
</div>
</details><p>As a consequence of the above theorem, since the vector <span class="math notranslate nohighlight">\(\nabla f(x)\)</span> points in the direction where <span class="math notranslate nohighlight">\(f\)</span> grows the most around <span class="math notranslate nohighlight">\(x\)</span>, the reverse direction, <span class="math notranslate nohighlight">\(-\nabla f(x)\)</span>, is the direction where <span class="math notranslate nohighlight">\(f\)</span> <strong>decreases</strong> the most in a small neighborhood of <span class="math notranslate nohighlight">\(x\)</span>. This is the direction of <strong>steepest descent</strong>.</p>
</section>
<section id="the-gradient-descent-algorithm">
<h2><span class="section-number">9.2. </span>The gradient descent algorithm<a class="headerlink" href="#the-gradient-descent-algorithm" title="Link to this heading">#</a></h2>
<p>The gradient descent algorithm minimizes a function <span class="math notranslate nohighlight">\(f\)</span> by starting at some location provided by the user, and making small steps in the direction of steepest descent to decrease the value of the function. This is a <em>greedy algorithm</em> in the sense that it always makes the best local move to decrease the value of <span class="math notranslate nohighlight">\(f\)</span>.</p>
<div class="admonition-the-gradient-descent-algorithm admonition">
<p class="admonition-title">The gradient descent algorithm</p>
<p><strong>Input:</strong> A function <span class="math notranslate nohighlight">\(f: \mathbb{R}^n \to \mathbb{R}\)</span>, a starting point <span class="math notranslate nohighlight">\(x_0 \in \mathbb{R}^n\)</span>, a step size <span class="math notranslate nohighlight">\(\eta &gt; 0\)</span>, and a tolerence <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>.</p>
<p><span class="math notranslate nohighlight">\(i = 0\)</span></p>
<p><strong>while</strong> <span class="math notranslate nohighlight">\(\|\nabla f(x_i)\|_2 &gt; \epsilon\)</span>:</p>
<p><span class="math notranslate nohighlight">\(\ \ \ x_{i+1} \leftarrow x_i - \eta \nabla f(x_i)\)</span></p>
<p><span class="math notranslate nohighlight">\(\ \ \ i \leftarrow i+1\)</span></p>
<p><strong>output:</strong> A point <span class="math notranslate nohighlight">\(x_n \in \mathbb{R}^n\)</span> such that <span class="math notranslate nohighlight">\(\|\nabla f(x_n)\|_2 \leq \epsilon\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The starting point <span class="math notranslate nohighlight">\(x_0\)</span> in gradient descent needs to be provided by the user. This is the initial guess to start the process. The starting point can be chosen using some heuristic or some previous knowledge when available. Otherwise, it can be a fixed number or can be chosen at random. Note that the starting point can have a big impact on the convergence of gradient descent.</p>
<p>Observe that the gradient descent algorithm converges to points where the gradient of <span class="math notranslate nohighlight">\(f\)</span> is approximately <span class="math notranslate nohighlight">\(0\)</span>. These may be local min, local max, or neither of them. Recall that if <span class="math notranslate nohighlight">\(f\)</span> has a local minimum or maximum at <span class="math notranslate nohighlight">\(x\)</span>, then <span class="math notranslate nohighlight">\(\nabla f(x) = 0\)</span>. When <span class="math notranslate nohighlight">\(f\)</span> has several local min/max, the gradient descent algorithm typically converges to a local min/max near the starting point. It is therefore important to run gradient descent with many different starting points if one does not know that the function to optimize has a unique min/max.</p>
</div>
</section>
<section id="implementing-a-basic-gradient-descent">
<h2><span class="section-number">9.3. </span>Implementing a basic gradient descent<a class="headerlink" href="#implementing-a-basic-gradient-descent" title="Link to this heading">#</a></h2>
<p><strong>Exercise:</strong> Implement a simple gradient descent algorithm in Python to minimize the function <span class="math notranslate nohighlight">\(f(x) = x^2\)</span>.</p>
<p>Note:</p>
<ol class="arabic simple">
<li><p>Your algorithm needs <span class="math notranslate nohighlight">\(x_0, \eta\)</span>, and <span class="math notranslate nohighlight">\(\epsilon\)</span> as inputs.</p></li>
<li><p>You may want to set a maximum number of iterations for the algorithm to prevent it from running for a long time if <span class="math notranslate nohighlight">\(\epsilon\)</span> is very small.</p></li>
<li><p>If you want, you can define f and grad_f as <a class="reference external" href="https://www.w3schools.com/python/python_lambda.asp">lambda functions</a>, and then write a general gradient descent algorithm that takes f and grad_f as inputs.</p></li>
</ol>
<p>After trying by yourself, click to see a possible solution.</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Define f and grad f</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="n">grad_f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> 

<span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">grad_f</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">eta</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">):</span> 
    <span class="c1"># Evaluate the norm of the gradient at x_0</span>
    <span class="n">grad_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad_f</span><span class="p">(</span><span class="n">x_0</span><span class="p">))</span>
    <span class="n">it</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Counter to keep track of current iterations</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x_0</span> <span class="c1"># Initially, we are at x_0</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Current x </span><span class="se">\t</span><span class="s2"> Norm gradient&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%1.4f</span><span class="s2"> </span><span class="se">\t</span><span class="s2"> </span><span class="si">%1.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">grad_norm</span><span class="p">))</span>
    <span class="k">while</span> <span class="n">grad_norm</span> <span class="o">&gt;</span> <span class="n">epsilon</span> <span class="ow">and</span> <span class="n">it</span> <span class="o">&lt;</span> <span class="n">max_iterations</span><span class="p">:</span> 
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># Move towards the negative gradient direction</span>
        <span class="n">grad_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># Update the norm of the gradient</span>
        <span class="n">it</span> <span class="o">=</span> <span class="n">it</span> <span class="o">+</span> <span class="mi">1</span>   <span class="c1"># Increase iterations by 1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%1.4f</span><span class="s2"> </span><span class="se">\t</span><span class="s2"> </span><span class="si">%1.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">grad_norm</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">it</span> <span class="o">==</span> <span class="n">max_iterations</span><span class="p">:</span> 
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warning: maximum number of iterations reached. Increase the step size or the number of iterations.&quot;</span><span class="p">)</span>
    
    <span class="k">return</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Run the gradient descent</span>
<span class="c1"># Let&#39;s say our initial guess for the minimum of 1.2. </span>
<span class="n">x</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">grad_f</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Current x 	 Norm gradient
1.2000 	 2.4000
1.1976 	 2.3952
1.1952 	 2.3904
1.1928 	 2.3856
1.1904 	 2.3809
1.1880 	 2.3761
1.1857 	 2.3713
1.1833 	 2.3666
1.1809 	 2.3619
1.1786 	 2.3571
1.1762 	 2.3524
1.1739 	 2.3477
1.1715 	 2.3430
1.1692 	 2.3383
1.1668 	 2.3337
1.1645 	 2.3290
1.1622 	 2.3243
1.1598 	 2.3197
1.1575 	 2.3151
1.1552 	 2.3104
1.1529 	 2.3058
1.1506 	 2.3012
1.1483 	 2.2966
1.1460 	 2.2920
1.1437 	 2.2874
1.1414 	 2.2828
1.1391 	 2.2783
1.1369 	 2.2737
1.1346 	 2.2692
1.1323 	 2.2646
1.1300 	 2.2601
1.1278 	 2.2556
1.1255 	 2.2511
1.1233 	 2.2466
1.1210 	 2.2421
1.1188 	 2.2376
1.1166 	 2.2331
1.1143 	 2.2286
1.1121 	 2.2242
1.1099 	 2.2197
1.1077 	 2.2153
1.1054 	 2.2109
1.1032 	 2.2064
1.1010 	 2.2020
1.0988 	 2.1976
1.0966 	 2.1932
1.0944 	 2.1889
1.0922 	 2.1845
1.0901 	 2.1801
1.0879 	 2.1757
1.0857 	 2.1714
1.0835 	 2.1670
1.0814 	 2.1627
1.0792 	 2.1584
1.0770 	 2.1541
1.0749 	 2.1498
1.0727 	 2.1455
1.0706 	 2.1412
1.0684 	 2.1369
1.0663 	 2.1326
1.0642 	 2.1284
1.0620 	 2.1241
1.0599 	 2.1198
1.0578 	 2.1156
1.0557 	 2.1114
1.0536 	 2.1072
1.0515 	 2.1029
1.0494 	 2.0987
1.0473 	 2.0945
1.0452 	 2.0903
1.0431 	 2.0862
1.0410 	 2.0820
1.0389 	 2.0778
1.0368 	 2.0737
1.0348 	 2.0695
1.0327 	 2.0654
1.0306 	 2.0613
1.0286 	 2.0571
1.0265 	 2.0530
1.0245 	 2.0489
1.0224 	 2.0448
1.0204 	 2.0407
1.0183 	 2.0366
1.0163 	 2.0326
1.0143 	 2.0285
1.0122 	 2.0245
1.0102 	 2.0204
1.0082 	 2.0164
1.0062 	 2.0123
1.0042 	 2.0083
1.0021 	 2.0043
1.0001 	 2.0003
0.9981 	 1.9963
0.9961 	 1.9923
0.9942 	 1.9883
0.9922 	 1.9843
0.9902 	 1.9804
0.9882 	 1.9764
0.9862 	 1.9724
0.9842 	 1.9685
0.9823 	 1.9646
0.9803 	 1.9606
0.9784 	 1.9567
0.9764 	 1.9528
0.9744 	 1.9489
0.9725 	 1.9450
0.9706 	 1.9411
0.9686 	 1.9372
0.9667 	 1.9333
0.9647 	 1.9295
0.9628 	 1.9256
0.9609 	 1.9218
0.9590 	 1.9179
0.9570 	 1.9141
0.9551 	 1.9103
0.9532 	 1.9064
0.9513 	 1.9026
0.9494 	 1.8988
0.9475 	 1.8950
0.9456 	 1.8912
0.9437 	 1.8875
0.9418 	 1.8837
0.9400 	 1.8799
0.9381 	 1.8762
0.9362 	 1.8724
0.9343 	 1.8687
0.9325 	 1.8649
0.9306 	 1.8612
0.9287 	 1.8575
0.9269 	 1.8537
0.9250 	 1.8500
0.9232 	 1.8463
0.9213 	 1.8426
0.9195 	 1.8390
0.9176 	 1.8353
0.9158 	 1.8316
0.9140 	 1.8280
0.9121 	 1.8243
0.9103 	 1.8206
0.9085 	 1.8170
0.9067 	 1.8134
0.9049 	 1.8097
0.9031 	 1.8061
0.9013 	 1.8025
0.8995 	 1.7989
0.8977 	 1.7953
0.8959 	 1.7917
0.8941 	 1.7881
0.8923 	 1.7846
0.8905 	 1.7810
0.8887 	 1.7774
0.8869 	 1.7739
0.8852 	 1.7703
0.8834 	 1.7668
0.8816 	 1.7633
0.8799 	 1.7597
0.8781 	 1.7562
0.8763 	 1.7527
0.8746 	 1.7492
0.8728 	 1.7457
0.8711 	 1.7422
0.8694 	 1.7387
0.8676 	 1.7352
0.8659 	 1.7318
0.8642 	 1.7283
0.8624 	 1.7248
0.8607 	 1.7214
0.8590 	 1.7180
0.8573 	 1.7145
0.8555 	 1.7111
0.8538 	 1.7077
0.8521 	 1.7043
0.8504 	 1.7008
0.8487 	 1.6974
0.8470 	 1.6940
0.8453 	 1.6907
0.8436 	 1.6873
0.8420 	 1.6839
0.8403 	 1.6805
0.8386 	 1.6772
0.8369 	 1.6738
0.8352 	 1.6705
0.8336 	 1.6671
0.8319 	 1.6638
0.8302 	 1.6605
0.8286 	 1.6571
0.8269 	 1.6538
0.8253 	 1.6505
0.8236 	 1.6472
0.8220 	 1.6439
0.8203 	 1.6406
0.8187 	 1.6374
0.8170 	 1.6341
0.8154 	 1.6308
0.8138 	 1.6276
0.8122 	 1.6243
0.8105 	 1.6211
0.8089 	 1.6178
0.8073 	 1.6146
0.8057 	 1.6113
0.8041 	 1.6081
0.8025 	 1.6049
0.8008 	 1.6017
0.7992 	 1.5985
0.7976 	 1.5953
0.7961 	 1.5921
0.7945 	 1.5889
0.7929 	 1.5857
0.7913 	 1.5826
0.7897 	 1.5794
0.7881 	 1.5762
0.7865 	 1.5731
0.7850 	 1.5700
0.7834 	 1.5668
0.7818 	 1.5637
0.7803 	 1.5605
0.7787 	 1.5574
0.7772 	 1.5543
0.7756 	 1.5512
0.7741 	 1.5481
0.7725 	 1.5450
0.7710 	 1.5419
0.7694 	 1.5388
0.7679 	 1.5358
0.7663 	 1.5327
0.7648 	 1.5296
0.7633 	 1.5266
0.7618 	 1.5235
0.7602 	 1.5205
0.7587 	 1.5174
0.7572 	 1.5144
0.7557 	 1.5114
0.7542 	 1.5083
0.7527 	 1.5053
0.7512 	 1.5023
0.7496 	 1.4993
0.7482 	 1.4963
0.7467 	 1.4933
0.7452 	 1.4903
0.7437 	 1.4873
0.7422 	 1.4844
0.7407 	 1.4814
0.7392 	 1.4784
0.7377 	 1.4755
0.7363 	 1.4725
0.7348 	 1.4696
0.7333 	 1.4666
0.7319 	 1.4637
0.7304 	 1.4608
0.7289 	 1.4579
0.7275 	 1.4549
0.7260 	 1.4520
0.7246 	 1.4491
0.7231 	 1.4462
0.7217 	 1.4433
0.7202 	 1.4405
0.7188 	 1.4376
0.7173 	 1.4347
0.7159 	 1.4318
0.7145 	 1.4290
0.7131 	 1.4261
0.7116 	 1.4233
0.7102 	 1.4204
0.7088 	 1.4176
0.7074 	 1.4147
0.7060 	 1.4119
0.7045 	 1.4091
0.7031 	 1.4063
0.7017 	 1.4034
0.7003 	 1.4006
0.6989 	 1.3978
0.6975 	 1.3950
0.6961 	 1.3923
0.6947 	 1.3895
0.6933 	 1.3867
0.6920 	 1.3839
0.6906 	 1.3811
0.6892 	 1.3784
0.6878 	 1.3756
0.6864 	 1.3729
0.6851 	 1.3701
0.6837 	 1.3674
0.6823 	 1.3647
0.6810 	 1.3619
0.6796 	 1.3592
0.6782 	 1.3565
0.6769 	 1.3538
0.6755 	 1.3511
0.6742 	 1.3484
0.6728 	 1.3457
0.6715 	 1.3430
0.6701 	 1.3403
0.6688 	 1.3376
0.6675 	 1.3349
0.6661 	 1.3323
0.6648 	 1.3296
0.6635 	 1.3269
0.6621 	 1.3243
0.6608 	 1.3216
0.6595 	 1.3190
0.6582 	 1.3164
0.6569 	 1.3137
0.6555 	 1.3111
0.6542 	 1.3085
0.6529 	 1.3059
0.6516 	 1.3032
0.6503 	 1.3006
0.6490 	 1.2980
0.6477 	 1.2954
0.6464 	 1.2929
0.6451 	 1.2903
0.6438 	 1.2877
0.6426 	 1.2851
0.6413 	 1.2825
0.6400 	 1.2800
0.6387 	 1.2774
0.6374 	 1.2749
0.6362 	 1.2723
0.6349 	 1.2698
0.6336 	 1.2672
0.6323 	 1.2647
0.6311 	 1.2622
0.6298 	 1.2596
0.6286 	 1.2571
0.6273 	 1.2546
0.6260 	 1.2521
0.6248 	 1.2496
0.6235 	 1.2471
0.6223 	 1.2446
0.6211 	 1.2421
0.6198 	 1.2396
0.6186 	 1.2371
0.6173 	 1.2347
0.6161 	 1.2322
0.6149 	 1.2297
0.6136 	 1.2273
0.6124 	 1.2248
0.6112 	 1.2224
0.6100 	 1.2199
0.6087 	 1.2175
0.6075 	 1.2151
0.6063 	 1.2126
0.6051 	 1.2102
0.6039 	 1.2078
0.6027 	 1.2054
0.6015 	 1.2030
0.6003 	 1.2005
0.5991 	 1.1981
0.5979 	 1.1957
0.5967 	 1.1934
0.5955 	 1.1910
0.5943 	 1.1886
0.5931 	 1.1862
0.5919 	 1.1838
0.5907 	 1.1815
0.5896 	 1.1791
0.5884 	 1.1767
0.5872 	 1.1744
0.5860 	 1.1720
0.5849 	 1.1697
0.5837 	 1.1674
0.5825 	 1.1650
0.5813 	 1.1627
0.5802 	 1.1604
0.5790 	 1.1581
0.5779 	 1.1557
0.5767 	 1.1534
0.5756 	 1.1511
0.5744 	 1.1488
0.5733 	 1.1465
0.5721 	 1.1442
0.5710 	 1.1419
0.5698 	 1.1397
0.5687 	 1.1374
0.5675 	 1.1351
0.5664 	 1.1328
0.5653 	 1.1306
0.5642 	 1.1283
0.5630 	 1.1260
0.5619 	 1.1238
0.5608 	 1.1215
0.5597 	 1.1193
0.5585 	 1.1171
0.5574 	 1.1148
0.5563 	 1.1126
0.5552 	 1.1104
0.5541 	 1.1082
0.5530 	 1.1059
0.5519 	 1.1037
0.5508 	 1.1015
0.5497 	 1.0993
0.5486 	 1.0971
0.5475 	 1.0949
0.5464 	 1.0927
0.5453 	 1.0905
0.5442 	 1.0884
0.5431 	 1.0862
0.5420 	 1.0840
0.5409 	 1.0818
0.5398 	 1.0797
0.5388 	 1.0775
0.5377 	 1.0754
0.5366 	 1.0732
0.5355 	 1.0711
0.5345 	 1.0689
0.5334 	 1.0668
0.5323 	 1.0647
0.5313 	 1.0625
0.5302 	 1.0604
0.5291 	 1.0583
0.5281 	 1.0562
0.5270 	 1.0541
0.5260 	 1.0519
0.5249 	 1.0498
0.5239 	 1.0477
0.5228 	 1.0456
0.5218 	 1.0436
0.5207 	 1.0415
0.5197 	 1.0394
0.5187 	 1.0373
0.5176 	 1.0352
0.5166 	 1.0332
0.5155 	 1.0311
0.5145 	 1.0290
0.5135 	 1.0270
0.5125 	 1.0249
0.5114 	 1.0229
0.5104 	 1.0208
0.5094 	 1.0188
0.5084 	 1.0167
0.5074 	 1.0147
0.5063 	 1.0127
0.5053 	 1.0107
0.5043 	 1.0086
0.5033 	 1.0066
0.5023 	 1.0046
0.5013 	 1.0026
0.5003 	 1.0006
0.4993 	 0.9986
0.4983 	 0.9966
0.4973 	 0.9946
0.4963 	 0.9926
0.4953 	 0.9906
0.4943 	 0.9886
0.4933 	 0.9867
0.4923 	 0.9847
0.4914 	 0.9827
0.4904 	 0.9808
0.4894 	 0.9788
0.4884 	 0.9768
0.4874 	 0.9749
0.4865 	 0.9729
0.4855 	 0.9710
0.4845 	 0.9691
0.4836 	 0.9671
0.4826 	 0.9652
0.4816 	 0.9632
0.4807 	 0.9613
0.4797 	 0.9594
0.4787 	 0.9575
0.4778 	 0.9556
0.4768 	 0.9537
0.4759 	 0.9517
0.4749 	 0.9498
0.4740 	 0.9479
0.4730 	 0.9460
0.4721 	 0.9442
0.4711 	 0.9423
0.4702 	 0.9404
0.4693 	 0.9385
0.4683 	 0.9366
0.4674 	 0.9348
0.4664 	 0.9329
0.4655 	 0.9310
0.4646 	 0.9292
0.4636 	 0.9273
0.4627 	 0.9254
0.4618 	 0.9236
0.4609 	 0.9217
0.4599 	 0.9199
0.4590 	 0.9181
0.4581 	 0.9162
0.4572 	 0.9144
0.4563 	 0.9126
0.4554 	 0.9107
0.4545 	 0.9089
0.4535 	 0.9071
0.4526 	 0.9053
0.4517 	 0.9035
0.4508 	 0.9017
0.4499 	 0.8999
0.4490 	 0.8981
0.4481 	 0.8963
0.4472 	 0.8945
0.4463 	 0.8927
0.4455 	 0.8909
0.4446 	 0.8891
0.4437 	 0.8873
0.4428 	 0.8856
0.4419 	 0.8838
0.4410 	 0.8820
0.4401 	 0.8803
0.4393 	 0.8785
0.4384 	 0.8767
0.4375 	 0.8750
0.4366 	 0.8732
0.4357 	 0.8715
0.4349 	 0.8698
0.4340 	 0.8680
0.4331 	 0.8663
0.4323 	 0.8645
0.4314 	 0.8628
0.4305 	 0.8611
0.4297 	 0.8594
0.4288 	 0.8576
0.4280 	 0.8559
0.4271 	 0.8542
0.4263 	 0.8525
0.4254 	 0.8508
0.4246 	 0.8491
0.4237 	 0.8474
0.4229 	 0.8457
0.4220 	 0.8440
0.4212 	 0.8423
0.4203 	 0.8406
0.4195 	 0.8390
0.4186 	 0.8373
0.4178 	 0.8356
0.4170 	 0.8339
0.4161 	 0.8323
0.4153 	 0.8306
0.4145 	 0.8290
0.4136 	 0.8273
0.4128 	 0.8256
0.4120 	 0.8240
0.4112 	 0.8223
0.4103 	 0.8207
0.4095 	 0.8191
0.4087 	 0.8174
0.4079 	 0.8158
0.4071 	 0.8141
0.4063 	 0.8125
0.4054 	 0.8109
0.4046 	 0.8093
0.4038 	 0.8077
0.4030 	 0.8060
0.4022 	 0.8044
0.4014 	 0.8028
0.4006 	 0.8012
0.3998 	 0.7996
0.3990 	 0.7980
0.3982 	 0.7964
0.3974 	 0.7948
0.3966 	 0.7932
0.3958 	 0.7916
0.3950 	 0.7901
0.3942 	 0.7885
0.3935 	 0.7869
0.3927 	 0.7853
0.3919 	 0.7838
0.3911 	 0.7822
0.3903 	 0.7806
0.3895 	 0.7791
0.3888 	 0.7775
0.3880 	 0.7760
0.3872 	 0.7744
0.3864 	 0.7729
0.3857 	 0.7713
0.3849 	 0.7698
0.3841 	 0.7682
0.3833 	 0.7667
0.3826 	 0.7652
0.3818 	 0.7636
0.3810 	 0.7621
0.3803 	 0.7606
0.3795 	 0.7591
0.3788 	 0.7575
0.3780 	 0.7560
0.3773 	 0.7545
0.3765 	 0.7530
0.3757 	 0.7515
0.3750 	 0.7500
0.3742 	 0.7485
0.3735 	 0.7470
0.3727 	 0.7455
0.3720 	 0.7440
0.3713 	 0.7425
0.3705 	 0.7410
0.3698 	 0.7396
0.3690 	 0.7381
0.3683 	 0.7366
0.3676 	 0.7351
0.3668 	 0.7337
0.3661 	 0.7322
0.3654 	 0.7307
0.3646 	 0.7293
0.3639 	 0.7278
0.3632 	 0.7263
0.3624 	 0.7249
0.3617 	 0.7234
0.3610 	 0.7220
0.3603 	 0.7206
0.3596 	 0.7191
0.3588 	 0.7177
0.3581 	 0.7162
0.3574 	 0.7148
0.3567 	 0.7134
0.3560 	 0.7120
0.3553 	 0.7105
0.3546 	 0.7091
0.3538 	 0.7077
0.3531 	 0.7063
0.3524 	 0.7049
0.3517 	 0.7034
0.3510 	 0.7020
0.3503 	 0.7006
0.3496 	 0.6992
0.3489 	 0.6978
0.3482 	 0.6964
0.3475 	 0.6951
0.3468 	 0.6937
0.3461 	 0.6923
0.3454 	 0.6909
0.3448 	 0.6895
0.3441 	 0.6881
0.3434 	 0.6868
0.3427 	 0.6854
0.3420 	 0.6840
0.3413 	 0.6826
0.3406 	 0.6813
0.3400 	 0.6799
0.3393 	 0.6786
0.3386 	 0.6772
0.3379 	 0.6758
0.3372 	 0.6745
0.3366 	 0.6731
0.3359 	 0.6718
0.3352 	 0.6704
0.3346 	 0.6691
0.3339 	 0.6678
0.3332 	 0.6664
0.3326 	 0.6651
0.3319 	 0.6638
0.3312 	 0.6624
0.3306 	 0.6611
0.3299 	 0.6598
0.3292 	 0.6585
0.3286 	 0.6572
0.3279 	 0.6558
0.3273 	 0.6545
0.3266 	 0.6532
0.3260 	 0.6519
0.3253 	 0.6506
0.3247 	 0.6493
0.3240 	 0.6480
0.3234 	 0.6467
0.3227 	 0.6454
0.3221 	 0.6441
0.3214 	 0.6428
0.3208 	 0.6416
0.3201 	 0.6403
0.3195 	 0.6390
0.3189 	 0.6377
0.3182 	 0.6364
0.3176 	 0.6352
0.3170 	 0.6339
0.3163 	 0.6326
0.3157 	 0.6314
0.3151 	 0.6301
0.3144 	 0.6288
0.3138 	 0.6276
0.3132 	 0.6263
0.3125 	 0.6251
0.3119 	 0.6238
0.3113 	 0.6226
0.3107 	 0.6213
0.3100 	 0.6201
0.3094 	 0.6189
0.3088 	 0.6176
0.3082 	 0.6164
0.3076 	 0.6151
0.3070 	 0.6139
0.3063 	 0.6127
0.3057 	 0.6115
0.3051 	 0.6102
0.3045 	 0.6090
0.3039 	 0.6078
0.3033 	 0.6066
0.3027 	 0.6054
0.3021 	 0.6042
0.3015 	 0.6030
0.3009 	 0.6017
0.3003 	 0.6005
0.2997 	 0.5993
0.2991 	 0.5981
0.2985 	 0.5969
0.2979 	 0.5958
0.2973 	 0.5946
0.2967 	 0.5934
0.2961 	 0.5922
0.2955 	 0.5910
0.2949 	 0.5898
0.2943 	 0.5886
0.2937 	 0.5875
0.2931 	 0.5863
0.2926 	 0.5851
0.2920 	 0.5839
0.2914 	 0.5828
0.2908 	 0.5816
0.2902 	 0.5805
0.2896 	 0.5793
0.2891 	 0.5781
0.2885 	 0.5770
0.2879 	 0.5758
0.2873 	 0.5747
0.2868 	 0.5735
0.2862 	 0.5724
0.2856 	 0.5712
0.2850 	 0.5701
0.2845 	 0.5689
0.2839 	 0.5678
0.2833 	 0.5667
0.2828 	 0.5655
0.2822 	 0.5644
0.2816 	 0.5633
0.2811 	 0.5622
0.2805 	 0.5610
0.2800 	 0.5599
0.2794 	 0.5588
0.2788 	 0.5577
0.2783 	 0.5566
0.2777 	 0.5554
0.2772 	 0.5543
0.2766 	 0.5532
0.2761 	 0.5521
0.2755 	 0.5510
0.2750 	 0.5499
0.2744 	 0.5488
0.2739 	 0.5477
0.2733 	 0.5466
0.2728 	 0.5455
0.2722 	 0.5444
0.2717 	 0.5433
0.2711 	 0.5423
0.2706 	 0.5412
0.2700 	 0.5401
0.2695 	 0.5390
0.2690 	 0.5379
0.2684 	 0.5369
0.2679 	 0.5358
0.2674 	 0.5347
0.2668 	 0.5336
0.2663 	 0.5326
0.2658 	 0.5315
0.2652 	 0.5304
0.2647 	 0.5294
0.2642 	 0.5283
0.2636 	 0.5273
0.2631 	 0.5262
0.2626 	 0.5252
0.2621 	 0.5241
0.2615 	 0.5231
0.2610 	 0.5220
0.2605 	 0.5210
0.2600 	 0.5199
0.2594 	 0.5189
0.2589 	 0.5179
0.2584 	 0.5168
0.2579 	 0.5158
0.2574 	 0.5148
0.2569 	 0.5137
0.2563 	 0.5127
0.2558 	 0.5117
0.2553 	 0.5106
0.2548 	 0.5096
0.2543 	 0.5086
0.2538 	 0.5076
0.2533 	 0.5066
0.2528 	 0.5056
0.2523 	 0.5045
0.2518 	 0.5035
0.2513 	 0.5025
0.2508 	 0.5015
0.2503 	 0.5005
0.2498 	 0.4995
0.2493 	 0.4985
0.2488 	 0.4975
0.2483 	 0.4965
0.2478 	 0.4955
0.2473 	 0.4945
0.2468 	 0.4936
0.2463 	 0.4926
0.2458 	 0.4916
0.2453 	 0.4906
0.2448 	 0.4896
0.2443 	 0.4886
0.2438 	 0.4877
0.2433 	 0.4867
0.2429 	 0.4857
0.2424 	 0.4847
0.2419 	 0.4838
0.2414 	 0.4828
0.2409 	 0.4818
0.2404 	 0.4809
0.2400 	 0.4799
0.2395 	 0.4790
0.2390 	 0.4780
0.2385 	 0.4770
0.2380 	 0.4761
0.2376 	 0.4751
0.2371 	 0.4742
0.2366 	 0.4732
0.2361 	 0.4723
0.2357 	 0.4713
0.2352 	 0.4704
0.2347 	 0.4695
0.2343 	 0.4685
0.2338 	 0.4676
0.2333 	 0.4667
0.2329 	 0.4657
0.2324 	 0.4648
0.2319 	 0.4639
0.2315 	 0.4629
0.2310 	 0.4620
0.2305 	 0.4611
0.2301 	 0.4602
0.2296 	 0.4592
0.2292 	 0.4583
0.2287 	 0.4574
0.2282 	 0.4565
0.2278 	 0.4556
0.2273 	 0.4547
0.2269 	 0.4538
0.2264 	 0.4528
0.2260 	 0.4519
0.2255 	 0.4510
0.2251 	 0.4501
0.2246 	 0.4492
0.2242 	 0.4483
0.2237 	 0.4474
0.2233 	 0.4465
0.2228 	 0.4457
0.2224 	 0.4448
0.2219 	 0.4439
0.2215 	 0.4430
0.2210 	 0.4421
0.2206 	 0.4412
0.2202 	 0.4403
0.2197 	 0.4395
0.2193 	 0.4386
0.2188 	 0.4377
0.2184 	 0.4368
0.2180 	 0.4359
0.2175 	 0.4351
0.2171 	 0.4342
0.2167 	 0.4333
0.2162 	 0.4325
0.2158 	 0.4316
0.2154 	 0.4307
0.2149 	 0.4299
0.2145 	 0.4290
0.2141 	 0.4282
0.2137 	 0.4273
0.2132 	 0.4265
0.2128 	 0.4256
0.2124 	 0.4247
0.2119 	 0.4239
0.2115 	 0.4230
0.2111 	 0.4222
0.2107 	 0.4214
0.2103 	 0.4205
0.2098 	 0.4197
0.2094 	 0.4188
0.2090 	 0.4180
0.2086 	 0.4172
0.2082 	 0.4163
0.2077 	 0.4155
0.2073 	 0.4147
0.2069 	 0.4138
0.2065 	 0.4130
0.2061 	 0.4122
0.2057 	 0.4114
0.2053 	 0.4105
0.2049 	 0.4097
0.2044 	 0.4089
0.2040 	 0.4081
0.2036 	 0.4073
0.2032 	 0.4064
0.2028 	 0.4056
0.2024 	 0.4048
0.2020 	 0.4040
0.2016 	 0.4032
0.2012 	 0.4024
0.2008 	 0.4016
0.2004 	 0.4008
0.2000 	 0.4000
0.1996 	 0.3992
0.1992 	 0.3984
0.1988 	 0.3976
0.1984 	 0.3968
0.1980 	 0.3960
0.1976 	 0.3952
0.1972 	 0.3944
0.1968 	 0.3936
0.1964 	 0.3928
0.1960 	 0.3921
0.1956 	 0.3913
0.1952 	 0.3905
0.1949 	 0.3897
0.1945 	 0.3889
0.1941 	 0.3882
0.1937 	 0.3874
0.1933 	 0.3866
0.1929 	 0.3858
0.1925 	 0.3851
0.1921 	 0.3843
0.1918 	 0.3835
0.1914 	 0.3828
0.1910 	 0.3820
0.1906 	 0.3812
0.1902 	 0.3805
0.1898 	 0.3797
0.1895 	 0.3789
0.1891 	 0.3782
0.1887 	 0.3774
0.1883 	 0.3767
0.1880 	 0.3759
0.1876 	 0.3752
0.1872 	 0.3744
0.1868 	 0.3737
0.1865 	 0.3729
0.1861 	 0.3722
0.1857 	 0.3714
0.1853 	 0.3707
0.1850 	 0.3699
0.1846 	 0.3692
0.1842 	 0.3685
0.1839 	 0.3677
0.1835 	 0.3670
0.1831 	 0.3663
0.1828 	 0.3655
0.1824 	 0.3648
0.1820 	 0.3641
0.1817 	 0.3633
0.1813 	 0.3626
0.1809 	 0.3619
0.1806 	 0.3612
0.1802 	 0.3604
0.1799 	 0.3597
0.1795 	 0.3590
0.1791 	 0.3583
0.1788 	 0.3576
0.1784 	 0.3569
0.1781 	 0.3561
0.1777 	 0.3554
0.1774 	 0.3547
0.1770 	 0.3540
0.1766 	 0.3533
0.1763 	 0.3526
0.1759 	 0.3519
0.1756 	 0.3512
0.1752 	 0.3505
0.1749 	 0.3498
0.1745 	 0.3491
0.1742 	 0.3484
0.1738 	 0.3477
0.1735 	 0.3470
0.1731 	 0.3463
0.1728 	 0.3456
0.1725 	 0.3449
0.1721 	 0.3442
0.1718 	 0.3435
0.1714 	 0.3428
0.1711 	 0.3422
0.1707 	 0.3415
0.1704 	 0.3408
0.1701 	 0.3401
0.1697 	 0.3394
0.1694 	 0.3388
0.1690 	 0.3381
0.1687 	 0.3374
0.1684 	 0.3367
0.1680 	 0.3360
0.1677 	 0.3354
0.1674 	 0.3347
0.1670 	 0.3340
0.1667 	 0.3334
0.1664 	 0.3327
0.1660 	 0.3320
0.1657 	 0.3314
0.1654 	 0.3307
0.1650 	 0.3300
0.1647 	 0.3294
0.1644 	 0.3287
0.1640 	 0.3281
0.1637 	 0.3274
0.1634 	 0.3268
0.1631 	 0.3261
0.1627 	 0.3255
0.1624 	 0.3248
0.1621 	 0.3242
0.1618 	 0.3235
0.1614 	 0.3229
0.1611 	 0.3222
0.1608 	 0.3216
0.1605 	 0.3209
0.1601 	 0.3203
0.1598 	 0.3196
0.1595 	 0.3190
0.1592 	 0.3184
0.1589 	 0.3177
0.1585 	 0.3171
0.1582 	 0.3165
0.1579 	 0.3158
0.1576 	 0.3152
0.1573 	 0.3146
0.1570 	 0.3139
0.1567 	 0.3133
0.1563 	 0.3127
0.1560 	 0.3121
0.1557 	 0.3114
0.1554 	 0.3108
0.1551 	 0.3102
0.1548 	 0.3096
0.1545 	 0.3089
0.1542 	 0.3083
0.1539 	 0.3077
0.1535 	 0.3071
0.1532 	 0.3065
0.1529 	 0.3059
0.1526 	 0.3053
0.1523 	 0.3046
0.1520 	 0.3040
0.1517 	 0.3034
0.1514 	 0.3028
0.1511 	 0.3022
0.1508 	 0.3016
0.1505 	 0.3010
0.1502 	 0.3004
0.1499 	 0.2998
0.1496 	 0.2992
0.1493 	 0.2986
0.1490 	 0.2980
0.1487 	 0.2974
0.1484 	 0.2968
0.1481 	 0.2962
0.1478 	 0.2956
0.1475 	 0.2950
0.1472 	 0.2945
0.1469 	 0.2939
0.1466 	 0.2933
0.1463 	 0.2927
0.1461 	 0.2921
0.1458 	 0.2915
0.1455 	 0.2909
0.1452 	 0.2904
0.1449 	 0.2898
0.1446 	 0.2892
0.1443 	 0.2886
0.1440 	 0.2880
0.1437 	 0.2875
0.1434 	 0.2869
0.1432 	 0.2863
0.1429 	 0.2857
0.1426 	 0.2852
0.1423 	 0.2846
0.1420 	 0.2840
0.1417 	 0.2835
0.1414 	 0.2829
0.1412 	 0.2823
0.1409 	 0.2818
0.1406 	 0.2812
0.1403 	 0.2806
0.1400 	 0.2801
0.1398 	 0.2795
0.1395 	 0.2790
0.1392 	 0.2784
0.1389 	 0.2778
0.1386 	 0.2773
0.1384 	 0.2767
0.1381 	 0.2762
0.1378 	 0.2756
0.1375 	 0.2751
0.1373 	 0.2745
0.1370 	 0.2740
0.1367 	 0.2734
0.1364 	 0.2729
0.1362 	 0.2723
0.1359 	 0.2718
0.1356 	 0.2713
0.1354 	 0.2707
0.1351 	 0.2702
0.1348 	 0.2696
0.1345 	 0.2691
0.1343 	 0.2685
0.1340 	 0.2680
0.1337 	 0.2675
0.1335 	 0.2669
0.1332 	 0.2664
0.1329 	 0.2659
0.1327 	 0.2653
0.1324 	 0.2648
0.1321 	 0.2643
0.1319 	 0.2638
0.1316 	 0.2632
0.1313 	 0.2627
0.1311 	 0.2622
0.1308 	 0.2616
0.1306 	 0.2611
0.1303 	 0.2606
0.1300 	 0.2601
0.1298 	 0.2596
0.1295 	 0.2590
0.1293 	 0.2585
0.1290 	 0.2580
0.1287 	 0.2575
0.1285 	 0.2570
0.1282 	 0.2565
0.1280 	 0.2560
0.1277 	 0.2554
0.1275 	 0.2549
0.1272 	 0.2544
0.1270 	 0.2539
0.1267 	 0.2534
0.1264 	 0.2529
0.1262 	 0.2524
0.1259 	 0.2519
0.1257 	 0.2514
0.1254 	 0.2509
0.1252 	 0.2504
0.1249 	 0.2499
0.1247 	 0.2494
0.1244 	 0.2489
0.1242 	 0.2484
0.1239 	 0.2479
0.1237 	 0.2474
0.1234 	 0.2469
0.1232 	 0.2464
0.1230 	 0.2459
0.1227 	 0.2454
0.1225 	 0.2449
0.1222 	 0.2444
0.1220 	 0.2439
0.1217 	 0.2435
0.1215 	 0.2430
0.1212 	 0.2425
0.1210 	 0.2420
0.1208 	 0.2415
0.1205 	 0.2410
0.1203 	 0.2405
0.1200 	 0.2401
0.1198 	 0.2396
0.1196 	 0.2391
0.1193 	 0.2386
0.1191 	 0.2382
0.1188 	 0.2377
0.1186 	 0.2372
0.1184 	 0.2367
0.1181 	 0.2363
0.1179 	 0.2358
0.1177 	 0.2353
0.1174 	 0.2348
0.1172 	 0.2344
0.1170 	 0.2339
0.1167 	 0.2334
0.1165 	 0.2330
0.1162 	 0.2325
0.1160 	 0.2320
0.1158 	 0.2316
0.1156 	 0.2311
0.1153 	 0.2306
0.1151 	 0.2302
0.1149 	 0.2297
0.1146 	 0.2293
0.1144 	 0.2288
0.1142 	 0.2283
0.1139 	 0.2279
0.1137 	 0.2274
0.1135 	 0.2270
0.1133 	 0.2265
0.1130 	 0.2261
0.1128 	 0.2256
0.1126 	 0.2252
0.1124 	 0.2247
0.1121 	 0.2243
0.1119 	 0.2238
0.1117 	 0.2234
0.1115 	 0.2229
0.1112 	 0.2225
0.1110 	 0.2220
0.1108 	 0.2216
0.1106 	 0.2211
0.1104 	 0.2207
0.1101 	 0.2203
0.1099 	 0.2198
0.1097 	 0.2194
0.1095 	 0.2189
0.1093 	 0.2185
0.1090 	 0.2181
0.1088 	 0.2176
0.1086 	 0.2172
0.1084 	 0.2168
0.1082 	 0.2163
0.1079 	 0.2159
0.1077 	 0.2155
0.1075 	 0.2150
0.1073 	 0.2146
0.1071 	 0.2142
0.1069 	 0.2137
0.1067 	 0.2133
0.1064 	 0.2129
0.1062 	 0.2125
0.1060 	 0.2120
0.1058 	 0.2116
0.1056 	 0.2112
0.1054 	 0.2108
0.1052 	 0.2104
0.1050 	 0.2099
0.1048 	 0.2095
0.1045 	 0.2091
0.1043 	 0.2087
0.1041 	 0.2083
0.1039 	 0.2078
0.1037 	 0.2074
0.1035 	 0.2070
0.1033 	 0.2066
0.1031 	 0.2062
0.1029 	 0.2058
0.1027 	 0.2054
0.1025 	 0.2049
0.1023 	 0.2045
0.1021 	 0.2041
0.1019 	 0.2037
0.1017 	 0.2033
0.1015 	 0.2029
0.1013 	 0.2025
0.1010 	 0.2021
0.1008 	 0.2017
0.1006 	 0.2013
0.1004 	 0.2009
0.1002 	 0.2005
0.1000 	 0.2001
0.0998 	 0.1997
0.0996 	 0.1993
0.0994 	 0.1989
0.0992 	 0.1985
0.0990 	 0.1981
0.0988 	 0.1977
0.0986 	 0.1973
0.0985 	 0.1969
0.0983 	 0.1965
0.0981 	 0.1961
0.0979 	 0.1957
0.0977 	 0.1953
0.0975 	 0.1949
0.0973 	 0.1946
0.0971 	 0.1942
0.0969 	 0.1938
0.0967 	 0.1934
0.0965 	 0.1930
0.0963 	 0.1926
0.0961 	 0.1922
0.0959 	 0.1918
0.0957 	 0.1915
0.0955 	 0.1911
0.0953 	 0.1907
0.0952 	 0.1903
0.0950 	 0.1899
0.0948 	 0.1896
0.0946 	 0.1892
0.0944 	 0.1888
0.0942 	 0.1884
0.0940 	 0.1880
0.0938 	 0.1877
0.0936 	 0.1873
0.0935 	 0.1869
0.0933 	 0.1865
0.0931 	 0.1862
0.0929 	 0.1858
0.0927 	 0.1854
0.0925 	 0.1851
0.0923 	 0.1847
0.0922 	 0.1843
0.0920 	 0.1839
0.0918 	 0.1836
0.0916 	 0.1832
0.0914 	 0.1828
0.0912 	 0.1825
0.0911 	 0.1821
0.0909 	 0.1818
0.0907 	 0.1814
0.0905 	 0.1810
0.0903 	 0.1807
0.0902 	 0.1803
0.0900 	 0.1799
0.0898 	 0.1796
0.0896 	 0.1792
0.0894 	 0.1789
0.0893 	 0.1785
0.0891 	 0.1781
0.0889 	 0.1778
0.0887 	 0.1774
0.0885 	 0.1771
0.0884 	 0.1767
0.0882 	 0.1764
0.0880 	 0.1760
0.0878 	 0.1757
0.0877 	 0.1753
0.0875 	 0.1750
0.0873 	 0.1746
0.0871 	 0.1743
0.0870 	 0.1739
0.0868 	 0.1736
0.0866 	 0.1732
0.0864 	 0.1729
0.0863 	 0.1725
0.0861 	 0.1722
0.0859 	 0.1718
0.0858 	 0.1715
0.0856 	 0.1712
0.0854 	 0.1708
0.0852 	 0.1705
0.0851 	 0.1701
0.0849 	 0.1698
0.0847 	 0.1695
0.0846 	 0.1691
0.0844 	 0.1688
0.0842 	 0.1684
0.0841 	 0.1681
0.0839 	 0.1678
0.0837 	 0.1674
0.0835 	 0.1671
0.0834 	 0.1668
0.0832 	 0.1664
0.0830 	 0.1661
0.0829 	 0.1658
0.0827 	 0.1654
0.0825 	 0.1651
0.0824 	 0.1648
0.0822 	 0.1644
0.0821 	 0.1641
0.0819 	 0.1638
0.0817 	 0.1635
0.0816 	 0.1631
0.0814 	 0.1628
0.0812 	 0.1625
0.0811 	 0.1622
0.0809 	 0.1618
0.0808 	 0.1615
0.0806 	 0.1612
0.0804 	 0.1609
0.0803 	 0.1605
0.0801 	 0.1602
0.0799 	 0.1599
0.0798 	 0.1596
0.0796 	 0.1593
0.0795 	 0.1589
0.0793 	 0.1586
0.0792 	 0.1583
0.0790 	 0.1580
0.0788 	 0.1577
0.0787 	 0.1574
0.0785 	 0.1570
0.0784 	 0.1567
0.0782 	 0.1564
0.0780 	 0.1561
0.0779 	 0.1558
0.0777 	 0.1555
0.0776 	 0.1552
0.0774 	 0.1549
0.0773 	 0.1545
0.0771 	 0.1542
0.0770 	 0.1539
0.0768 	 0.1536
0.0767 	 0.1533
0.0765 	 0.1530
0.0763 	 0.1527
0.0762 	 0.1524
0.0760 	 0.1521
0.0759 	 0.1518
0.0757 	 0.1515
0.0756 	 0.1512
0.0754 	 0.1509
0.0753 	 0.1506
0.0751 	 0.1503
0.0750 	 0.1500
0.0748 	 0.1497
0.0747 	 0.1494
0.0745 	 0.1491
0.0744 	 0.1488
0.0742 	 0.1485
0.0741 	 0.1482
0.0739 	 0.1479
0.0738 	 0.1476
0.0736 	 0.1473
0.0735 	 0.1470
0.0734 	 0.1467
0.0732 	 0.1464
0.0731 	 0.1461
0.0729 	 0.1458
0.0728 	 0.1455
0.0726 	 0.1452
0.0725 	 0.1450
0.0723 	 0.1447
0.0722 	 0.1444
0.0720 	 0.1441
0.0719 	 0.1438
0.0718 	 0.1435
0.0716 	 0.1432
0.0715 	 0.1429
0.0713 	 0.1427
0.0712 	 0.1424
0.0710 	 0.1421
0.0709 	 0.1418
0.0708 	 0.1415
0.0706 	 0.1412
0.0705 	 0.1409
0.0703 	 0.1407
0.0702 	 0.1404
0.0701 	 0.1401
0.0699 	 0.1398
0.0698 	 0.1395
0.0696 	 0.1393
0.0695 	 0.1390
0.0694 	 0.1387
0.0692 	 0.1384
0.0691 	 0.1382
0.0689 	 0.1379
0.0688 	 0.1376
0.0687 	 0.1373
0.0685 	 0.1371
0.0684 	 0.1368
0.0683 	 0.1365
0.0681 	 0.1362
0.0680 	 0.1360
0.0678 	 0.1357
0.0677 	 0.1354
0.0676 	 0.1351
0.0674 	 0.1349
0.0673 	 0.1346
0.0672 	 0.1343
0.0670 	 0.1341
0.0669 	 0.1338
0.0668 	 0.1335
0.0666 	 0.1333
0.0665 	 0.1330
0.0664 	 0.1327
0.0662 	 0.1325
0.0661 	 0.1322
0.0660 	 0.1319
0.0658 	 0.1317
0.0657 	 0.1314
0.0656 	 0.1311
0.0654 	 0.1309
0.0653 	 0.1306
0.0652 	 0.1304
0.0651 	 0.1301
0.0649 	 0.1298
0.0648 	 0.1296
0.0647 	 0.1293
0.0645 	 0.1291
0.0644 	 0.1288
0.0643 	 0.1285
0.0641 	 0.1283
0.0640 	 0.1280
0.0639 	 0.1278
0.0638 	 0.1275
0.0636 	 0.1273
0.0635 	 0.1270
0.0634 	 0.1268
0.0633 	 0.1265
0.0631 	 0.1263
0.0630 	 0.1260
0.0629 	 0.1257
0.0627 	 0.1255
0.0626 	 0.1252
0.0625 	 0.1250
0.0624 	 0.1247
0.0622 	 0.1245
0.0621 	 0.1242
0.0620 	 0.1240
0.0619 	 0.1237
0.0618 	 0.1235
0.0616 	 0.1233
0.0615 	 0.1230
0.0614 	 0.1228
0.0613 	 0.1225
0.0611 	 0.1223
0.0610 	 0.1220
0.0609 	 0.1218
0.0608 	 0.1215
0.0606 	 0.1213
0.0605 	 0.1211
0.0604 	 0.1208
0.0603 	 0.1206
0.0602 	 0.1203
0.0600 	 0.1201
0.0599 	 0.1198
0.0598 	 0.1196
0.0597 	 0.1194
0.0596 	 0.1191
0.0594 	 0.1189
0.0593 	 0.1187
0.0592 	 0.1184
0.0591 	 0.1182
0.0590 	 0.1179
0.0589 	 0.1177
0.0587 	 0.1175
0.0586 	 0.1172
0.0585 	 0.1170
0.0584 	 0.1168
0.0583 	 0.1165
0.0582 	 0.1163
0.0580 	 0.1161
0.0579 	 0.1158
0.0578 	 0.1156
0.0577 	 0.1154
0.0576 	 0.1151
0.0575 	 0.1149
0.0573 	 0.1147
0.0572 	 0.1145
0.0571 	 0.1142
0.0570 	 0.1140
0.0569 	 0.1138
0.0568 	 0.1135
0.0567 	 0.1133
0.0565 	 0.1131
0.0564 	 0.1129
0.0563 	 0.1126
0.0562 	 0.1124
0.0561 	 0.1122
0.0560 	 0.1120
0.0559 	 0.1117
0.0558 	 0.1115
0.0556 	 0.1113
0.0555 	 0.1111
0.0554 	 0.1108
0.0553 	 0.1106
0.0552 	 0.1104
0.0551 	 0.1102
0.0550 	 0.1100
0.0549 	 0.1097
0.0548 	 0.1095
0.0547 	 0.1093
0.0545 	 0.1091
0.0544 	 0.1089
0.0543 	 0.1086
0.0542 	 0.1084
0.0541 	 0.1082
0.0540 	 0.1080
0.0539 	 0.1078
0.0538 	 0.1076
0.0537 	 0.1074
0.0536 	 0.1071
0.0535 	 0.1069
0.0534 	 0.1067
0.0532 	 0.1065
0.0531 	 0.1063
0.0530 	 0.1061
0.0529 	 0.1059
0.0528 	 0.1056
0.0527 	 0.1054
0.0526 	 0.1052
0.0525 	 0.1050
0.0524 	 0.1048
0.0523 	 0.1046
0.0522 	 0.1044
0.0521 	 0.1042
0.0520 	 0.1040
0.0519 	 0.1038
0.0518 	 0.1036
0.0517 	 0.1033
0.0516 	 0.1031
0.0515 	 0.1029
0.0514 	 0.1027
0.0513 	 0.1025
0.0512 	 0.1023
0.0511 	 0.1021
0.0510 	 0.1019
0.0509 	 0.1017
0.0508 	 0.1015
0.0506 	 0.1013
0.0505 	 0.1011
0.0504 	 0.1009
0.0503 	 0.1007
0.0502 	 0.1005
0.0501 	 0.1003
0.0500 	 0.1001
0.0499 	 0.0999
0.0498 	 0.0997
0.0497 	 0.0995
0.0496 	 0.0993
0.0495 	 0.0991
0.0494 	 0.0989
0.0493 	 0.0987
0.0492 	 0.0985
0.0492 	 0.0983
0.0491 	 0.0981
0.0490 	 0.0979
0.0489 	 0.0977
0.0488 	 0.0975
0.0487 	 0.0973
0.0486 	 0.0971
0.0485 	 0.0969
0.0484 	 0.0967
0.0483 	 0.0965
0.0482 	 0.0964
0.0481 	 0.0962
0.0480 	 0.0960
0.0479 	 0.0958
0.0478 	 0.0956
0.0477 	 0.0954
0.0476 	 0.0952
0.0475 	 0.0950
0.0474 	 0.0948
0.0473 	 0.0946
0.0472 	 0.0944
0.0471 	 0.0943
0.0470 	 0.0941
0.0469 	 0.0939
0.0468 	 0.0937
0.0468 	 0.0935
0.0467 	 0.0933
0.0466 	 0.0931
0.0465 	 0.0929
0.0464 	 0.0928
0.0463 	 0.0926
0.0462 	 0.0924
0.0461 	 0.0922
0.0460 	 0.0920
0.0459 	 0.0918
0.0458 	 0.0916
0.0457 	 0.0915
0.0456 	 0.0913
0.0455 	 0.0911
0.0455 	 0.0909
0.0454 	 0.0907
0.0453 	 0.0906
0.0452 	 0.0904
0.0451 	 0.0902
0.0450 	 0.0900
0.0449 	 0.0898
0.0448 	 0.0897
0.0447 	 0.0895
0.0446 	 0.0893
0.0446 	 0.0891
0.0445 	 0.0889
0.0444 	 0.0888
0.0443 	 0.0886
0.0442 	 0.0884
0.0441 	 0.0882
0.0440 	 0.0881
0.0439 	 0.0879
0.0438 	 0.0877
0.0438 	 0.0875
0.0437 	 0.0873
0.0436 	 0.0872
0.0435 	 0.0870
0.0434 	 0.0868
0.0433 	 0.0867
0.0432 	 0.0865
0.0432 	 0.0863
0.0431 	 0.0861
0.0430 	 0.0860
0.0429 	 0.0858
0.0428 	 0.0856
0.0427 	 0.0854
0.0426 	 0.0853
0.0426 	 0.0851
0.0425 	 0.0849
0.0424 	 0.0848
0.0423 	 0.0846
0.0422 	 0.0844
0.0421 	 0.0843
0.0420 	 0.0841
0.0420 	 0.0839
0.0419 	 0.0838
0.0418 	 0.0836
0.0417 	 0.0834
0.0416 	 0.0833
0.0415 	 0.0831
0.0415 	 0.0829
0.0414 	 0.0828
0.0413 	 0.0826
0.0412 	 0.0824
0.0411 	 0.0823
0.0410 	 0.0821
0.0410 	 0.0819
0.0409 	 0.0818
0.0408 	 0.0816
0.0407 	 0.0814
0.0406 	 0.0813
0.0406 	 0.0811
0.0405 	 0.0810
0.0404 	 0.0808
0.0403 	 0.0806
0.0402 	 0.0805
0.0402 	 0.0803
0.0401 	 0.0801
0.0400 	 0.0800
0.0399 	 0.0798
0.0398 	 0.0797
0.0398 	 0.0795
0.0397 	 0.0793
0.0396 	 0.0792
0.0395 	 0.0790
0.0394 	 0.0789
0.0394 	 0.0787
0.0393 	 0.0786
0.0392 	 0.0784
0.0391 	 0.0782
0.0390 	 0.0781
0.0390 	 0.0779
0.0389 	 0.0778
0.0388 	 0.0776
0.0387 	 0.0775
0.0387 	 0.0773
0.0386 	 0.0772
0.0385 	 0.0770
0.0384 	 0.0768
0.0383 	 0.0767
0.0383 	 0.0765
0.0382 	 0.0764
0.0381 	 0.0762
0.0380 	 0.0761
0.0380 	 0.0759
0.0379 	 0.0758
0.0378 	 0.0756
0.0377 	 0.0755
0.0377 	 0.0753
0.0376 	 0.0752
0.0375 	 0.0750
0.0374 	 0.0749
0.0374 	 0.0747
0.0373 	 0.0746
0.0372 	 0.0744
0.0371 	 0.0743
0.0371 	 0.0741
0.0370 	 0.0740
0.0369 	 0.0738
0.0368 	 0.0737
0.0368 	 0.0735
0.0367 	 0.0734
0.0366 	 0.0732
0.0365 	 0.0731
0.0365 	 0.0729
0.0364 	 0.0728
0.0363 	 0.0727
0.0363 	 0.0725
0.0362 	 0.0724
0.0361 	 0.0722
0.0360 	 0.0721
0.0360 	 0.0719
0.0359 	 0.0718
0.0358 	 0.0716
0.0358 	 0.0715
0.0357 	 0.0714
0.0356 	 0.0712
0.0355 	 0.0711
0.0355 	 0.0709
0.0354 	 0.0708
0.0353 	 0.0706
0.0353 	 0.0705
0.0352 	 0.0704
0.0351 	 0.0702
0.0350 	 0.0701
0.0350 	 0.0699
0.0349 	 0.0698
0.0348 	 0.0697
0.0348 	 0.0695
0.0347 	 0.0694
0.0346 	 0.0692
0.0346 	 0.0691
0.0345 	 0.0690
0.0344 	 0.0688
0.0343 	 0.0687
0.0343 	 0.0686
0.0342 	 0.0684
0.0341 	 0.0683
0.0341 	 0.0681
0.0340 	 0.0680
0.0339 	 0.0679
0.0339 	 0.0677
0.0338 	 0.0676
0.0337 	 0.0675
0.0337 	 0.0673
0.0336 	 0.0672
0.0335 	 0.0671
0.0335 	 0.0669
0.0334 	 0.0668
0.0333 	 0.0667
0.0333 	 0.0665
0.0332 	 0.0664
0.0331 	 0.0663
0.0331 	 0.0661
0.0330 	 0.0660
0.0329 	 0.0659
0.0329 	 0.0657
0.0328 	 0.0656
0.0327 	 0.0655
0.0327 	 0.0653
0.0326 	 0.0652
0.0325 	 0.0651
0.0325 	 0.0649
0.0324 	 0.0648
0.0323 	 0.0647
0.0323 	 0.0646
0.0322 	 0.0644
0.0322 	 0.0643
0.0321 	 0.0642
0.0320 	 0.0640
0.0320 	 0.0639
0.0319 	 0.0638
0.0318 	 0.0637
0.0318 	 0.0635
0.0317 	 0.0634
0.0316 	 0.0633
0.0316 	 0.0632
0.0315 	 0.0630
0.0315 	 0.0629
0.0314 	 0.0628
0.0313 	 0.0627
0.0313 	 0.0625
0.0312 	 0.0624
0.0311 	 0.0623
0.0311 	 0.0622
0.0310 	 0.0620
0.0310 	 0.0619
0.0309 	 0.0618
0.0308 	 0.0617
0.0308 	 0.0615
0.0307 	 0.0614
0.0306 	 0.0613
0.0306 	 0.0612
0.0305 	 0.0610
0.0305 	 0.0609
0.0304 	 0.0608
0.0303 	 0.0607
0.0303 	 0.0606
0.0302 	 0.0604
0.0302 	 0.0603
0.0301 	 0.0602
0.0300 	 0.0601
0.0300 	 0.0600
0.0299 	 0.0598
0.0299 	 0.0597
0.0298 	 0.0596
0.0297 	 0.0595
0.0297 	 0.0594
0.0296 	 0.0592
0.0296 	 0.0591
0.0295 	 0.0590
0.0294 	 0.0589
0.0294 	 0.0588
0.0293 	 0.0586
0.0293 	 0.0585
0.0292 	 0.0584
0.0291 	 0.0583
0.0291 	 0.0582
0.0290 	 0.0581
0.0290 	 0.0579
0.0289 	 0.0578
0.0289 	 0.0577
0.0288 	 0.0576
0.0287 	 0.0575
0.0287 	 0.0574
0.0286 	 0.0573
0.0286 	 0.0571
0.0285 	 0.0570
0.0285 	 0.0569
0.0284 	 0.0568
0.0283 	 0.0567
0.0283 	 0.0566
0.0282 	 0.0565
0.0282 	 0.0563
0.0281 	 0.0562
0.0281 	 0.0561
0.0280 	 0.0560
0.0279 	 0.0559
0.0279 	 0.0558
0.0278 	 0.0557
0.0278 	 0.0556
0.0277 	 0.0554
0.0277 	 0.0553
0.0276 	 0.0552
0.0276 	 0.0551
0.0275 	 0.0550
0.0274 	 0.0549
0.0274 	 0.0548
0.0273 	 0.0547
0.0273 	 0.0546
0.0272 	 0.0545
0.0272 	 0.0543
0.0271 	 0.0542
0.0271 	 0.0541
0.0270 	 0.0540
0.0270 	 0.0539
0.0269 	 0.0538
0.0269 	 0.0537
0.0268 	 0.0536
0.0267 	 0.0535
0.0267 	 0.0534
0.0266 	 0.0533
0.0266 	 0.0532
0.0265 	 0.0531
0.0265 	 0.0530
0.0264 	 0.0528
0.0264 	 0.0527
0.0263 	 0.0526
0.0263 	 0.0525
0.0262 	 0.0524
0.0262 	 0.0523
0.0261 	 0.0522
0.0261 	 0.0521
0.0260 	 0.0520
0.0260 	 0.0519
0.0259 	 0.0518
0.0258 	 0.0517
0.0258 	 0.0516
0.0257 	 0.0515
0.0257 	 0.0514
0.0256 	 0.0513
0.0256 	 0.0512
0.0255 	 0.0511
0.0255 	 0.0510
0.0254 	 0.0509
0.0254 	 0.0508
0.0253 	 0.0507
0.0253 	 0.0506
0.0252 	 0.0505
0.0252 	 0.0504
0.0251 	 0.0503
0.0251 	 0.0502
0.0250 	 0.0501
0.0250 	 0.0500
0.0249 	 0.0499
0.0249 	 0.0498
0.0248 	 0.0497
0.0248 	 0.0496
0.0247 	 0.0495
0.0247 	 0.0494
0.0246 	 0.0493
0.0246 	 0.0492
0.0245 	 0.0491
0.0245 	 0.0490
0.0244 	 0.0489
0.0244 	 0.0488
0.0243 	 0.0487
0.0243 	 0.0486
0.0242 	 0.0485
0.0242 	 0.0484
0.0241 	 0.0483
0.0241 	 0.0482
0.0241 	 0.0481
0.0240 	 0.0480
0.0240 	 0.0479
0.0239 	 0.0478
0.0239 	 0.0477
0.0238 	 0.0476
0.0238 	 0.0475
0.0237 	 0.0474
0.0237 	 0.0473
0.0236 	 0.0472
0.0236 	 0.0471
0.0235 	 0.0471
0.0235 	 0.0470
0.0234 	 0.0469
0.0234 	 0.0468
0.0233 	 0.0467
0.0233 	 0.0466
0.0232 	 0.0465
0.0232 	 0.0464
0.0232 	 0.0463
0.0231 	 0.0462
0.0231 	 0.0461
0.0230 	 0.0460
0.0230 	 0.0459
0.0229 	 0.0458
0.0229 	 0.0458
0.0228 	 0.0457
0.0228 	 0.0456
0.0227 	 0.0455
0.0227 	 0.0454
0.0226 	 0.0453
0.0226 	 0.0452
0.0226 	 0.0451
0.0225 	 0.0450
0.0225 	 0.0449
0.0224 	 0.0448
0.0224 	 0.0448
0.0223 	 0.0447
0.0223 	 0.0446
0.0222 	 0.0445
0.0222 	 0.0444
0.0222 	 0.0443
0.0221 	 0.0442
0.0221 	 0.0441
0.0220 	 0.0440
0.0220 	 0.0440
0.0219 	 0.0439
0.0219 	 0.0438
0.0218 	 0.0437
0.0218 	 0.0436
0.0218 	 0.0435
0.0217 	 0.0434
0.0217 	 0.0433
0.0216 	 0.0433
0.0216 	 0.0432
0.0215 	 0.0431
0.0215 	 0.0430
0.0215 	 0.0429
0.0214 	 0.0428
0.0214 	 0.0427
0.0213 	 0.0427
0.0213 	 0.0426
0.0212 	 0.0425
0.0212 	 0.0424
0.0212 	 0.0423
0.0211 	 0.0422
0.0211 	 0.0421
0.0210 	 0.0421
0.0210 	 0.0420
0.0209 	 0.0419
0.0209 	 0.0418
0.0209 	 0.0417
0.0208 	 0.0416
0.0208 	 0.0416
0.0207 	 0.0415
0.0207 	 0.0414
0.0207 	 0.0413
0.0206 	 0.0412
0.0206 	 0.0411
0.0205 	 0.0411
0.0205 	 0.0410
0.0205 	 0.0409
0.0204 	 0.0408
0.0204 	 0.0407
0.0203 	 0.0407
0.0203 	 0.0406
0.0202 	 0.0405
0.0202 	 0.0404
0.0202 	 0.0403
0.0201 	 0.0403
0.0201 	 0.0402
0.0200 	 0.0401
0.0200 	 0.0400
0.0200 	 0.0399
0.0199 	 0.0399
0.0199 	 0.0398
0.0198 	 0.0397
0.0198 	 0.0396
0.0198 	 0.0395
0.0197 	 0.0395
0.0197 	 0.0394
0.0196 	 0.0393
0.0196 	 0.0392
0.0196 	 0.0391
0.0195 	 0.0391
0.0195 	 0.0390
0.0195 	 0.0389
0.0194 	 0.0388
0.0194 	 0.0387
0.0193 	 0.0387
0.0193 	 0.0386
0.0193 	 0.0385
0.0192 	 0.0384
0.0192 	 0.0384
0.0191 	 0.0383
0.0191 	 0.0382
0.0191 	 0.0381
0.0190 	 0.0381
0.0190 	 0.0380
0.0190 	 0.0379
0.0189 	 0.0378
0.0189 	 0.0378
0.0188 	 0.0377
0.0188 	 0.0376
0.0188 	 0.0375
0.0187 	 0.0375
0.0187 	 0.0374
0.0187 	 0.0373
0.0186 	 0.0372
0.0186 	 0.0372
0.0185 	 0.0371
0.0185 	 0.0370
0.0185 	 0.0369
0.0184 	 0.0369
0.0184 	 0.0368
0.0184 	 0.0367
0.0183 	 0.0366
0.0183 	 0.0366
0.0182 	 0.0365
0.0182 	 0.0364
0.0182 	 0.0363
0.0181 	 0.0363
0.0181 	 0.0362
0.0181 	 0.0361
0.0180 	 0.0361
0.0180 	 0.0360
0.0180 	 0.0359
0.0179 	 0.0358
0.0179 	 0.0358
0.0178 	 0.0357
0.0178 	 0.0356
0.0178 	 0.0356
0.0177 	 0.0355
0.0177 	 0.0354
0.0177 	 0.0353
0.0176 	 0.0353
0.0176 	 0.0352
0.0176 	 0.0351
0.0175 	 0.0351
0.0175 	 0.0350
0.0175 	 0.0349
0.0174 	 0.0348
0.0174 	 0.0348
0.0174 	 0.0347
0.0173 	 0.0346
0.0173 	 0.0346
0.0173 	 0.0345
0.0172 	 0.0344
0.0172 	 0.0344
0.0171 	 0.0343
0.0171 	 0.0342
0.0171 	 0.0342
0.0170 	 0.0341
0.0170 	 0.0340
0.0170 	 0.0340
0.0169 	 0.0339
0.0169 	 0.0338
0.0169 	 0.0337
0.0168 	 0.0337
0.0168 	 0.0336
0.0168 	 0.0335
0.0167 	 0.0335
0.0167 	 0.0334
0.0167 	 0.0333
0.0166 	 0.0333
0.0166 	 0.0332
0.0166 	 0.0331
0.0165 	 0.0331
0.0165 	 0.0330
0.0165 	 0.0329
0.0164 	 0.0329
0.0164 	 0.0328
0.0164 	 0.0328
0.0163 	 0.0327
0.0163 	 0.0326
0.0163 	 0.0326
0.0162 	 0.0325
0.0162 	 0.0324
0.0162 	 0.0324
0.0161 	 0.0323
0.0161 	 0.0322
0.0161 	 0.0322
0.0161 	 0.0321
0.0160 	 0.0320
0.0160 	 0.0320
0.0160 	 0.0319
0.0159 	 0.0318
0.0159 	 0.0318
0.0159 	 0.0317
0.0158 	 0.0317
0.0158 	 0.0316
0.0158 	 0.0315
0.0157 	 0.0315
0.0157 	 0.0314
0.0157 	 0.0313
0.0156 	 0.0313
0.0156 	 0.0312
0.0156 	 0.0312
0.0155 	 0.0311
0.0155 	 0.0310
0.0155 	 0.0310
0.0155 	 0.0309
0.0154 	 0.0308
0.0154 	 0.0308
0.0154 	 0.0307
0.0153 	 0.0307
0.0153 	 0.0306
0.0153 	 0.0305
0.0152 	 0.0305
0.0152 	 0.0304
0.0152 	 0.0304
0.0151 	 0.0303
0.0151 	 0.0302
0.0151 	 0.0302
0.0151 	 0.0301
0.0150 	 0.0300
0.0150 	 0.0300
0.0150 	 0.0299
0.0149 	 0.0299
0.0149 	 0.0298
0.0149 	 0.0298
0.0148 	 0.0297
0.0148 	 0.0296
0.0148 	 0.0296
0.0148 	 0.0295
0.0147 	 0.0295
0.0147 	 0.0294
0.0147 	 0.0293
0.0146 	 0.0293
0.0146 	 0.0292
0.0146 	 0.0292
0.0146 	 0.0291
0.0145 	 0.0290
0.0145 	 0.0290
0.0145 	 0.0289
0.0144 	 0.0289
0.0144 	 0.0288
0.0144 	 0.0288
0.0143 	 0.0287
0.0143 	 0.0286
0.0143 	 0.0286
0.0143 	 0.0285
0.0142 	 0.0285
0.0142 	 0.0284
0.0142 	 0.0284
0.0141 	 0.0283
0.0141 	 0.0282
0.0141 	 0.0282
0.0141 	 0.0281
0.0140 	 0.0281
0.0140 	 0.0280
0.0140 	 0.0280
0.0140 	 0.0279
0.0139 	 0.0278
0.0139 	 0.0278
0.0139 	 0.0277
0.0138 	 0.0277
0.0138 	 0.0276
0.0138 	 0.0276
0.0138 	 0.0275
0.0137 	 0.0275
0.0137 	 0.0274
0.0137 	 0.0274
0.0136 	 0.0273
0.0136 	 0.0272
0.0136 	 0.0272
0.0136 	 0.0271
0.0135 	 0.0271
0.0135 	 0.0270
0.0135 	 0.0270
0.0135 	 0.0269
0.0134 	 0.0269
0.0134 	 0.0268
0.0134 	 0.0268
0.0134 	 0.0267
0.0133 	 0.0266
0.0133 	 0.0266
0.0133 	 0.0265
0.0132 	 0.0265
0.0132 	 0.0264
0.0132 	 0.0264
0.0132 	 0.0263
0.0131 	 0.0263
0.0131 	 0.0262
0.0131 	 0.0262
0.0131 	 0.0261
0.0130 	 0.0261
0.0130 	 0.0260
0.0130 	 0.0260
0.0130 	 0.0259
0.0129 	 0.0259
0.0129 	 0.0258
0.0129 	 0.0258
0.0129 	 0.0257
0.0128 	 0.0257
0.0128 	 0.0256
0.0128 	 0.0256
0.0127 	 0.0255
0.0127 	 0.0254
0.0127 	 0.0254
0.0127 	 0.0253
0.0126 	 0.0253
0.0126 	 0.0252
0.0126 	 0.0252
0.0126 	 0.0251
0.0125 	 0.0251
0.0125 	 0.0250
0.0125 	 0.0250
0.0125 	 0.0249
0.0124 	 0.0249
0.0124 	 0.0248
0.0124 	 0.0248
0.0124 	 0.0247
0.0123 	 0.0247
0.0123 	 0.0246
0.0123 	 0.0246
0.0123 	 0.0245
0.0122 	 0.0245
0.0122 	 0.0245
0.0122 	 0.0244
0.0122 	 0.0244
0.0122 	 0.0243
0.0121 	 0.0243
0.0121 	 0.0242
0.0121 	 0.0242
0.0121 	 0.0241
0.0120 	 0.0241
0.0120 	 0.0240
0.0120 	 0.0240
0.0120 	 0.0239
0.0119 	 0.0239
0.0119 	 0.0238
0.0119 	 0.0238
0.0119 	 0.0237
0.0118 	 0.0237
0.0118 	 0.0236
0.0118 	 0.0236
0.0118 	 0.0235
0.0117 	 0.0235
0.0117 	 0.0234
0.0117 	 0.0234
0.0117 	 0.0233
0.0117 	 0.0233
0.0116 	 0.0233
0.0116 	 0.0232
0.0116 	 0.0232
0.0116 	 0.0231
0.0115 	 0.0231
0.0115 	 0.0230
0.0115 	 0.0230
0.0115 	 0.0229
0.0114 	 0.0229
0.0114 	 0.0228
0.0114 	 0.0228
0.0114 	 0.0227
0.0114 	 0.0227
0.0113 	 0.0227
0.0113 	 0.0226
0.0113 	 0.0226
0.0113 	 0.0225
0.0112 	 0.0225
0.0112 	 0.0224
0.0112 	 0.0224
0.0112 	 0.0223
0.0111 	 0.0223
0.0111 	 0.0223
0.0111 	 0.0222
0.0111 	 0.0222
0.0111 	 0.0221
0.0110 	 0.0221
0.0110 	 0.0220
0.0110 	 0.0220
0.0110 	 0.0219
0.0110 	 0.0219
0.0109 	 0.0219
0.0109 	 0.0218
0.0109 	 0.0218
0.0109 	 0.0217
0.0108 	 0.0217
0.0108 	 0.0216
0.0108 	 0.0216
0.0108 	 0.0216
0.0108 	 0.0215
0.0107 	 0.0215
0.0107 	 0.0214
0.0107 	 0.0214
0.0107 	 0.0213
0.0106 	 0.0213
0.0106 	 0.0213
0.0106 	 0.0212
0.0106 	 0.0212
0.0106 	 0.0211
0.0105 	 0.0211
0.0105 	 0.0210
0.0105 	 0.0210
0.0105 	 0.0210
0.0105 	 0.0209
0.0104 	 0.0209
0.0104 	 0.0208
0.0104 	 0.0208
0.0104 	 0.0207
0.0104 	 0.0207
0.0103 	 0.0207
0.0103 	 0.0206
0.0103 	 0.0206
0.0103 	 0.0205
0.0103 	 0.0205
0.0102 	 0.0205
0.0102 	 0.0204
0.0102 	 0.0204
0.0102 	 0.0203
0.0101 	 0.0203
0.0101 	 0.0203
0.0101 	 0.0202
0.0101 	 0.0202
0.0101 	 0.0201
0.0100 	 0.0201
0.0100 	 0.0201
0.0100 	 0.0200
0.0100 	 0.0200
0.0100 	 0.0199
0.0099 	 0.0199
0.0099 	 0.0199
0.0099 	 0.0198
0.0099 	 0.0198
0.0099 	 0.0197
0.0098 	 0.0197
0.0098 	 0.0197
0.0098 	 0.0196
0.0098 	 0.0196
0.0098 	 0.0195
0.0097 	 0.0195
0.0097 	 0.0195
0.0097 	 0.0194
0.0097 	 0.0194
0.0097 	 0.0193
0.0097 	 0.0193
0.0096 	 0.0193
0.0096 	 0.0192
0.0096 	 0.0192
0.0096 	 0.0192
0.0096 	 0.0191
0.0095 	 0.0191
0.0095 	 0.0190
0.0095 	 0.0190
0.0095 	 0.0190
0.0095 	 0.0189
0.0094 	 0.0189
0.0094 	 0.0188
0.0094 	 0.0188
0.0094 	 0.0188
0.0094 	 0.0187
0.0093 	 0.0187
0.0093 	 0.0187
0.0093 	 0.0186
0.0093 	 0.0186
0.0093 	 0.0185
0.0093 	 0.0185
0.0092 	 0.0185
0.0092 	 0.0184
0.0092 	 0.0184
0.0092 	 0.0184
0.0092 	 0.0183
0.0091 	 0.0183
0.0091 	 0.0183
0.0091 	 0.0182
0.0091 	 0.0182
0.0091 	 0.0181
0.0091 	 0.0181
0.0090 	 0.0181
0.0090 	 0.0180
0.0090 	 0.0180
0.0090 	 0.0180
0.0090 	 0.0179
0.0089 	 0.0179
0.0089 	 0.0179
0.0089 	 0.0178
0.0089 	 0.0178
0.0089 	 0.0177
0.0089 	 0.0177
0.0088 	 0.0177
0.0088 	 0.0176
0.0088 	 0.0176
0.0088 	 0.0176
0.0088 	 0.0175
0.0088 	 0.0175
0.0087 	 0.0175
0.0087 	 0.0174
0.0087 	 0.0174
0.0087 	 0.0174
0.0087 	 0.0173
0.0086 	 0.0173
0.0086 	 0.0173
0.0086 	 0.0172
0.0086 	 0.0172
0.0086 	 0.0172
0.0086 	 0.0171
0.0085 	 0.0171
0.0085 	 0.0171
0.0085 	 0.0170
0.0085 	 0.0170
0.0085 	 0.0170
0.0085 	 0.0169
0.0084 	 0.0169
0.0084 	 0.0168
0.0084 	 0.0168
0.0084 	 0.0168
0.0084 	 0.0167
0.0084 	 0.0167
0.0083 	 0.0167
0.0083 	 0.0166
0.0083 	 0.0166
0.0083 	 0.0166
0.0083 	 0.0165
0.0083 	 0.0165
0.0082 	 0.0165
0.0082 	 0.0164
0.0082 	 0.0164
0.0082 	 0.0164
0.0082 	 0.0164
0.0082 	 0.0163
0.0081 	 0.0163
0.0081 	 0.0163
0.0081 	 0.0162
0.0081 	 0.0162
0.0081 	 0.0162
0.0081 	 0.0161
0.0080 	 0.0161
0.0080 	 0.0161
0.0080 	 0.0160
0.0080 	 0.0160
0.0080 	 0.0160
0.0080 	 0.0159
0.0079 	 0.0159
0.0079 	 0.0159
0.0079 	 0.0158
0.0079 	 0.0158
0.0079 	 0.0158
0.0079 	 0.0157
0.0079 	 0.0157
0.0078 	 0.0157
0.0078 	 0.0156
0.0078 	 0.0156
0.0078 	 0.0156
0.0078 	 0.0156
0.0078 	 0.0155
0.0077 	 0.0155
0.0077 	 0.0155
0.0077 	 0.0154
0.0077 	 0.0154
0.0077 	 0.0154
0.0077 	 0.0153
0.0077 	 0.0153
0.0076 	 0.0153
0.0076 	 0.0152
0.0076 	 0.0152
0.0076 	 0.0152
0.0076 	 0.0152
0.0076 	 0.0151
0.0075 	 0.0151
0.0075 	 0.0151
0.0075 	 0.0150
0.0075 	 0.0150
0.0075 	 0.0150
0.0075 	 0.0149
0.0075 	 0.0149
0.0074 	 0.0149
0.0074 	 0.0149
0.0074 	 0.0148
0.0074 	 0.0148
0.0074 	 0.0148
0.0074 	 0.0147
0.0074 	 0.0147
0.0073 	 0.0147
0.0073 	 0.0146
0.0073 	 0.0146
0.0073 	 0.0146
0.0073 	 0.0146
0.0073 	 0.0145
0.0072 	 0.0145
0.0072 	 0.0145
0.0072 	 0.0144
0.0072 	 0.0144
0.0072 	 0.0144
0.0072 	 0.0144
0.0072 	 0.0143
0.0071 	 0.0143
0.0071 	 0.0143
0.0071 	 0.0142
0.0071 	 0.0142
0.0071 	 0.0142
0.0071 	 0.0142
0.0071 	 0.0141
0.0070 	 0.0141
0.0070 	 0.0141
0.0070 	 0.0140
0.0070 	 0.0140
0.0070 	 0.0140
0.0070 	 0.0140
0.0070 	 0.0139
0.0070 	 0.0139
0.0069 	 0.0139
0.0069 	 0.0138
0.0069 	 0.0138
0.0069 	 0.0138
0.0069 	 0.0138
0.0069 	 0.0137
0.0069 	 0.0137
0.0068 	 0.0137
0.0068 	 0.0137
0.0068 	 0.0136
0.0068 	 0.0136
0.0068 	 0.0136
0.0068 	 0.0135
0.0068 	 0.0135
0.0067 	 0.0135
0.0067 	 0.0135
0.0067 	 0.0134
0.0067 	 0.0134
0.0067 	 0.0134
0.0067 	 0.0134
0.0067 	 0.0133
0.0067 	 0.0133
0.0066 	 0.0133
0.0066 	 0.0133
0.0066 	 0.0132
0.0066 	 0.0132
0.0066 	 0.0132
0.0066 	 0.0131
0.0066 	 0.0131
0.0065 	 0.0131
0.0065 	 0.0131
0.0065 	 0.0130
0.0065 	 0.0130
0.0065 	 0.0130
0.0065 	 0.0130
0.0065 	 0.0129
0.0065 	 0.0129
0.0064 	 0.0129
0.0064 	 0.0129
0.0064 	 0.0128
0.0064 	 0.0128
0.0064 	 0.0128
0.0064 	 0.0128
0.0064 	 0.0127
0.0064 	 0.0127
0.0063 	 0.0127
0.0063 	 0.0127
0.0063 	 0.0126
0.0063 	 0.0126
0.0063 	 0.0126
0.0063 	 0.0126
0.0063 	 0.0125
0.0063 	 0.0125
0.0062 	 0.0125
0.0062 	 0.0125
0.0062 	 0.0124
0.0062 	 0.0124
0.0062 	 0.0124
0.0062 	 0.0124
0.0062 	 0.0123
0.0062 	 0.0123
0.0061 	 0.0123
0.0061 	 0.0123
0.0061 	 0.0122
0.0061 	 0.0122
0.0061 	 0.0122
0.0061 	 0.0122
0.0061 	 0.0121
0.0061 	 0.0121
0.0060 	 0.0121
0.0060 	 0.0121
0.0060 	 0.0120
0.0060 	 0.0120
0.0060 	 0.0120
0.0060 	 0.0120
0.0060 	 0.0119
0.0060 	 0.0119
0.0059 	 0.0119
0.0059 	 0.0119
0.0059 	 0.0118
0.0059 	 0.0118
0.0059 	 0.0118
0.0059 	 0.0118
0.0059 	 0.0118
0.0059 	 0.0117
0.0059 	 0.0117
0.0058 	 0.0117
0.0058 	 0.0117
0.0058 	 0.0116
0.0058 	 0.0116
0.0058 	 0.0116
0.0058 	 0.0116
0.0058 	 0.0115
0.0058 	 0.0115
0.0057 	 0.0115
0.0057 	 0.0115
0.0057 	 0.0114
0.0057 	 0.0114
0.0057 	 0.0114
0.0057 	 0.0114
0.0057 	 0.0114
0.0057 	 0.0113
0.0057 	 0.0113
0.0056 	 0.0113
0.0056 	 0.0113
0.0056 	 0.0112
0.0056 	 0.0112
0.0056 	 0.0112
0.0056 	 0.0112
0.0056 	 0.0112
0.0056 	 0.0111
0.0056 	 0.0111
0.0055 	 0.0111
0.0055 	 0.0111
0.0055 	 0.0110
0.0055 	 0.0110
0.0055 	 0.0110
0.0055 	 0.0110
0.0055 	 0.0110
0.0055 	 0.0109
0.0055 	 0.0109
0.0054 	 0.0109
0.0054 	 0.0109
0.0054 	 0.0108
0.0054 	 0.0108
0.0054 	 0.0108
0.0054 	 0.0108
0.0054 	 0.0108
0.0054 	 0.0107
0.0054 	 0.0107
0.0053 	 0.0107
0.0053 	 0.0107
0.0053 	 0.0107
0.0053 	 0.0106
0.0053 	 0.0106
0.0053 	 0.0106
0.0053 	 0.0106
0.0053 	 0.0105
0.0053 	 0.0105
0.0053 	 0.0105
0.0052 	 0.0105
0.0052 	 0.0105
0.0052 	 0.0104
0.0052 	 0.0104
0.0052 	 0.0104
0.0052 	 0.0104
0.0052 	 0.0104
0.0052 	 0.0103
0.0052 	 0.0103
0.0051 	 0.0103
0.0051 	 0.0103
0.0051 	 0.0103
0.0051 	 0.0102
0.0051 	 0.0102
0.0051 	 0.0102
0.0051 	 0.0102
0.0051 	 0.0102
0.0051 	 0.0101
0.0051 	 0.0101
0.0050 	 0.0101
0.0050 	 0.0101
0.0050 	 0.0101
0.0050 	 0.0100
0.0050 	 0.0100
0.0050 	 0.0100
</pre></div>
</div>
</div>
</details>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the code above, <em>default arguments</em> are passsed to the function <em>gradient_descent</em>. Please review <a href="https://lectures.scientific-python.org/intro/language/functions.html" target="_blank">Defining functions</a> in the Python tutorial for more details.</p>
</div>
<p>Try the code with other functions. Are you able to find local minima? How does the starting point affects the convergence of the algorithm?</p>
<div class="admonition-the-adam-optimizer admonition">
<p class="admonition-title">The Adam optimizer</p>
<p>A popular variant of the gradient descent algorithm that is used a lot in data science is the <strong>Adam</strong> (Adaptive Moment Estimation) optimizer. Adam uses <em>momentum</em> and <em>Root Mean Square Propagation</em> (RMSProp) to speed up convergence.</p>
<ol class="arabic simple">
<li><p>In the momentum approach a combination of the previous gradient and the new gradient is used to make updates. This helps smoothing out noisy gradients and provides a more stable path for optimization.</p></li>
<li><p>RMSProp adjusts the learning rate depending on the magnitude of the gradient (large gradient: smaller learning rate, small gradient: larger learning rate). This prevents overshooting and slow convergence.</p></li>
</ol>
</div>
</section>
<section id="using-pytorch">
<h2><span class="section-number">9.4. </span>Using PyTorch<a class="headerlink" href="#using-pytorch" title="Link to this heading">#</a></h2>
<p>A popular Python package that can be used to construct models for data is <strong>PyTorch</strong>. The package was developed by Facebook AI Research lab. One of its main strenghts is that it automatically computes the gradient of functions <em>symbolically</em> . This greatly simplify optimizing functions such as loss functions. As we perform calculations, PyTorch keeps track of all the operations performed in the form of a <em>computational graph</em>.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="../_images/f_x_y_graph.png"><img alt="../_images/f_x_y_graph.png" src="../_images/f_x_y_graph.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 9.1 </span><span class="caption-text">Example of a computational graph for evaluating <span class="math notranslate nohighlight">\(w = \log v = \log(xy)\)</span>.</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Using the computational graph, PyTorch can automatically evaluate gradients symbolically by applying the <em>chain rule</em>. PyTorch also supports multi-dimensional arrays (tensors) and is optimized for both CPU and GPU.</p>
<p>The following code shows how one can use PyTorch to minimize the function <span class="math notranslate nohighlight">\(f(x) = x^2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Define the function f(x) = x^2</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
     <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Initial value of x</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># We use the Adam optimizer (lr = learning rate)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([{</span><span class="s1">&#39;params&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="n">x</span><span class="p">]}],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="n">maxit</span> <span class="o">=</span> <span class="mi">2000</span> <span class="c1"># Maximum number of optimization steps</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Iteration counter</span>
<span class="n">grad</span> <span class="o">=</span> <span class="mf">1e6</span>  <span class="c1"># Keeps track of derivative with respect to x</span>

<span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">maxit</span> <span class="ow">and</span> <span class="nb">abs</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">1e-3</span><span class="p">:</span> 
     <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span> 
     <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># Clear the gradients</span>
     <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Calculate the function value</span>
     <span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Compute the gradients</span>
     <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Update the parameters (here: the value of x)</span>
     <span class="n">grad</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
     <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Iter: </span><span class="si">{}</span><span class="s2"> </span><span class="se">\t</span><span class="s2"> x= </span><span class="si">{:.6f}</span><span class="s2"> </span><span class="se">\t</span><span class="s2"> y= </span><span class="si">{:.6f}</span><span class="s2"> </span><span class="se">\t</span><span class="s2"> grad= </span><span class="si">{:.6f}</span><span class="s2">&quot;</span>
                 <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">grad</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iter: 1 	 x= 0.990000 	 y= 1.000000 	 grad= 2.000000
Iter: 2 	 x= 0.980003 	 y= 0.980100 	 grad= 1.980000
Iter: 3 	 x= 0.970010 	 y= 0.960405 	 grad= 1.960006
Iter: 4 	 x= 0.960024 	 y= 0.940920 	 grad= 1.940020
Iter: 5 	 x= 0.950046 	 y= 0.921646 	 grad= 1.920048
Iter: 6 	 x= 0.940079 	 y= 0.902588 	 grad= 1.900092
Iter: 7 	 x= 0.930123 	 y= 0.883748 	 grad= 1.880157
Iter: 8 	 x= 0.920182 	 y= 0.865130 	 grad= 1.860247
Iter: 9 	 x= 0.910257 	 y= 0.846735 	 grad= 1.840364
Iter: 10 	 x= 0.900350 	 y= 0.828568 	 grad= 1.820514
Iter: 11 	 x= 0.890462 	 y= 0.810630 	 grad= 1.800699
Iter: 12 	 x= 0.880596 	 y= 0.792923 	 grad= 1.780924
Iter: 13 	 x= 0.870754 	 y= 0.775450 	 grad= 1.761193
Iter: 14 	 x= 0.860937 	 y= 0.758213 	 grad= 1.741508
Iter: 15 	 x= 0.851147 	 y= 0.741212 	 grad= 1.721874
Iter: 16 	 x= 0.841386 	 y= 0.724452 	 grad= 1.702294
Iter: 17 	 x= 0.831656 	 y= 0.707931 	 grad= 1.682773
Iter: 18 	 x= 0.821958 	 y= 0.691652 	 grad= 1.663312
Iter: 19 	 x= 0.812294 	 y= 0.675615 	 grad= 1.643916
Iter: 20 	 x= 0.802666 	 y= 0.659822 	 grad= 1.624589
Iter: 21 	 x= 0.793076 	 y= 0.644273 	 grad= 1.605333
Iter: 22 	 x= 0.783524 	 y= 0.628969 	 grad= 1.586151
Iter: 23 	 x= 0.774012 	 y= 0.613910 	 grad= 1.567048
Iter: 24 	 x= 0.764543 	 y= 0.599095 	 grad= 1.548025
Iter: 25 	 x= 0.755116 	 y= 0.584526 	 grad= 1.529085
Iter: 26 	 x= 0.745735 	 y= 0.570201 	 grad= 1.510233
Iter: 27 	 x= 0.736399 	 y= 0.556120 	 grad= 1.491470
Iter: 28 	 x= 0.727111 	 y= 0.542284 	 grad= 1.472798
Iter: 29 	 x= 0.717871 	 y= 0.528690 	 grad= 1.454222
Iter: 30 	 x= 0.708681 	 y= 0.515339 	 grad= 1.435742
Iter: 31 	 x= 0.699542 	 y= 0.502229 	 grad= 1.417362
Iter: 32 	 x= 0.690455 	 y= 0.489359 	 grad= 1.399084
Iter: 33 	 x= 0.681421 	 y= 0.476728 	 grad= 1.380910
Iter: 34 	 x= 0.672442 	 y= 0.464335 	 grad= 1.362843
Iter: 35 	 x= 0.663517 	 y= 0.452178 	 grad= 1.344883
Iter: 36 	 x= 0.654649 	 y= 0.440255 	 grad= 1.327034
Iter: 37 	 x= 0.645837 	 y= 0.428565 	 grad= 1.309298
Iter: 38 	 x= 0.637084 	 y= 0.417106 	 grad= 1.291675
Iter: 39 	 x= 0.628389 	 y= 0.405876 	 grad= 1.274168
Iter: 40 	 x= 0.619754 	 y= 0.394873 	 grad= 1.256778
Iter: 41 	 x= 0.611179 	 y= 0.384095 	 grad= 1.239507
Iter: 42 	 x= 0.602664 	 y= 0.373539 	 grad= 1.222357
Iter: 43 	 x= 0.594212 	 y= 0.363204 	 grad= 1.205329
Iter: 44 	 x= 0.585822 	 y= 0.353088 	 grad= 1.188424
Iter: 45 	 x= 0.577495 	 y= 0.343187 	 grad= 1.171644
Iter: 46 	 x= 0.569231 	 y= 0.333500 	 grad= 1.154989
Iter: 47 	 x= 0.561031 	 y= 0.324024 	 grad= 1.138461
Iter: 48 	 x= 0.552896 	 y= 0.314756 	 grad= 1.122062
Iter: 49 	 x= 0.544826 	 y= 0.305694 	 grad= 1.105791
Iter: 50 	 x= 0.536821 	 y= 0.296835 	 grad= 1.089651
Iter: 51 	 x= 0.528883 	 y= 0.288177 	 grad= 1.073642
Iter: 52 	 x= 0.521010 	 y= 0.279717 	 grad= 1.057765
Iter: 53 	 x= 0.513205 	 y= 0.271452 	 grad= 1.042021
Iter: 54 	 x= 0.505467 	 y= 0.263379 	 grad= 1.026410
Iter: 55 	 x= 0.497796 	 y= 0.255497 	 grad= 1.010934
Iter: 56 	 x= 0.490193 	 y= 0.247801 	 grad= 0.995592
Iter: 57 	 x= 0.482659 	 y= 0.240289 	 grad= 0.980386
Iter: 58 	 x= 0.475192 	 y= 0.232959 	 grad= 0.965317
Iter: 59 	 x= 0.467794 	 y= 0.225808 	 grad= 0.950384
Iter: 60 	 x= 0.460466 	 y= 0.218832 	 grad= 0.935589
Iter: 61 	 x= 0.453206 	 y= 0.212029 	 grad= 0.920931
Iter: 62 	 x= 0.446016 	 y= 0.205396 	 grad= 0.906412
Iter: 63 	 x= 0.438895 	 y= 0.198930 	 grad= 0.892031
Iter: 64 	 x= 0.431844 	 y= 0.192629 	 grad= 0.877790
Iter: 65 	 x= 0.424862 	 y= 0.186489 	 grad= 0.863687
Iter: 66 	 x= 0.417951 	 y= 0.180508 	 grad= 0.849724
Iter: 67 	 x= 0.411109 	 y= 0.174683 	 grad= 0.835901
Iter: 68 	 x= 0.404338 	 y= 0.169011 	 grad= 0.822218
Iter: 69 	 x= 0.397637 	 y= 0.163489 	 grad= 0.808676
Iter: 70 	 x= 0.391006 	 y= 0.158115 	 grad= 0.795273
Iter: 71 	 x= 0.384445 	 y= 0.152885 	 grad= 0.782011
Iter: 72 	 x= 0.377955 	 y= 0.147798 	 grad= 0.768890
Iter: 73 	 x= 0.371535 	 y= 0.142850 	 grad= 0.755910
Iter: 74 	 x= 0.365185 	 y= 0.138038 	 grad= 0.743070
Iter: 75 	 x= 0.358906 	 y= 0.133360 	 grad= 0.730370
Iter: 76 	 x= 0.352697 	 y= 0.128813 	 grad= 0.717812
Iter: 77 	 x= 0.346558 	 y= 0.124395 	 grad= 0.705394
Iter: 78 	 x= 0.340489 	 y= 0.120102 	 grad= 0.693116
Iter: 79 	 x= 0.334491 	 y= 0.115933 	 grad= 0.680979
Iter: 80 	 x= 0.328562 	 y= 0.111884 	 grad= 0.668982
Iter: 81 	 x= 0.322704 	 y= 0.107953 	 grad= 0.657125
Iter: 82 	 x= 0.316915 	 y= 0.104138 	 grad= 0.645408
Iter: 83 	 x= 0.311196 	 y= 0.100435 	 grad= 0.633830
Iter: 84 	 x= 0.305546 	 y= 0.096843 	 grad= 0.622392
Iter: 85 	 x= 0.299966 	 y= 0.093359 	 grad= 0.611093
Iter: 86 	 x= 0.294455 	 y= 0.089980 	 grad= 0.599932
Iter: 87 	 x= 0.289013 	 y= 0.086704 	 grad= 0.588910
Iter: 88 	 x= 0.283640 	 y= 0.083529 	 grad= 0.578026
Iter: 89 	 x= 0.278336 	 y= 0.080452 	 grad= 0.567280
Iter: 90 	 x= 0.273099 	 y= 0.077471 	 grad= 0.556671
Iter: 91 	 x= 0.267932 	 y= 0.074583 	 grad= 0.546199
Iter: 92 	 x= 0.262832 	 y= 0.071787 	 grad= 0.535863
Iter: 93 	 x= 0.257799 	 y= 0.069080 	 grad= 0.525663
Iter: 94 	 x= 0.252835 	 y= 0.066461 	 grad= 0.515599
Iter: 95 	 x= 0.247937 	 y= 0.063925 	 grad= 0.505669
Iter: 96 	 x= 0.243106 	 y= 0.061473 	 grad= 0.495874
Iter: 97 	 x= 0.238342 	 y= 0.059101 	 grad= 0.486213
Iter: 98 	 x= 0.233644 	 y= 0.056807 	 grad= 0.476684
Iter: 99 	 x= 0.229012 	 y= 0.054590 	 grad= 0.467289
Iter: 100 	 x= 0.224446 	 y= 0.052447 	 grad= 0.458025
Iter: 101 	 x= 0.219945 	 y= 0.050376 	 grad= 0.448892
Iter: 102 	 x= 0.215509 	 y= 0.048376 	 grad= 0.439890
Iter: 103 	 x= 0.211138 	 y= 0.046444 	 grad= 0.431018
Iter: 104 	 x= 0.206830 	 y= 0.044579 	 grad= 0.422275
Iter: 105 	 x= 0.202587 	 y= 0.042779 	 grad= 0.413661
Iter: 106 	 x= 0.198407 	 y= 0.041041 	 grad= 0.405174
Iter: 107 	 x= 0.194290 	 y= 0.039365 	 grad= 0.396814
Iter: 108 	 x= 0.190236 	 y= 0.037749 	 grad= 0.388580
Iter: 109 	 x= 0.186244 	 y= 0.036190 	 grad= 0.380471
Iter: 110 	 x= 0.182313 	 y= 0.034687 	 grad= 0.372487
Iter: 111 	 x= 0.178444 	 y= 0.033238 	 grad= 0.364627
Iter: 112 	 x= 0.174636 	 y= 0.031842 	 grad= 0.356889
Iter: 113 	 x= 0.170889 	 y= 0.030498 	 grad= 0.349273
Iter: 114 	 x= 0.167201 	 y= 0.029203 	 grad= 0.341778
Iter: 115 	 x= 0.163573 	 y= 0.027956 	 grad= 0.334403
Iter: 116 	 x= 0.160005 	 y= 0.026756 	 grad= 0.327147
Iter: 117 	 x= 0.156494 	 y= 0.025601 	 grad= 0.320009
Iter: 118 	 x= 0.153042 	 y= 0.024490 	 grad= 0.312989
Iter: 119 	 x= 0.149648 	 y= 0.023422 	 grad= 0.306084
Iter: 120 	 x= 0.146310 	 y= 0.022394 	 grad= 0.299295
Iter: 121 	 x= 0.143030 	 y= 0.021407 	 grad= 0.292621
Iter: 122 	 x= 0.139805 	 y= 0.020457 	 grad= 0.286059
Iter: 123 	 x= 0.136636 	 y= 0.019545 	 grad= 0.279610
Iter: 124 	 x= 0.133522 	 y= 0.018669 	 grad= 0.273272
Iter: 125 	 x= 0.130462 	 y= 0.017828 	 grad= 0.267044
Iter: 126 	 x= 0.127457 	 y= 0.017020 	 grad= 0.260925
Iter: 127 	 x= 0.124505 	 y= 0.016245 	 grad= 0.254914
Iter: 128 	 x= 0.121606 	 y= 0.015501 	 grad= 0.249010
Iter: 129 	 x= 0.118759 	 y= 0.014788 	 grad= 0.243212
Iter: 130 	 x= 0.115964 	 y= 0.014104 	 grad= 0.237518
Iter: 131 	 x= 0.113221 	 y= 0.013448 	 grad= 0.231929
Iter: 132 	 x= 0.110528 	 y= 0.012819 	 grad= 0.226441
Iter: 133 	 x= 0.107885 	 y= 0.012216 	 grad= 0.221056
Iter: 134 	 x= 0.105292 	 y= 0.011639 	 grad= 0.215770
Iter: 135 	 x= 0.102748 	 y= 0.011086 	 grad= 0.210584
Iter: 136 	 x= 0.100252 	 y= 0.010557 	 grad= 0.205496
Iter: 137 	 x= 0.097805 	 y= 0.010051 	 grad= 0.200505
Iter: 138 	 x= 0.095404 	 y= 0.009566 	 grad= 0.195609
Iter: 139 	 x= 0.093050 	 y= 0.009102 	 grad= 0.190808
Iter: 140 	 x= 0.090743 	 y= 0.008658 	 grad= 0.186101
Iter: 141 	 x= 0.088481 	 y= 0.008234 	 grad= 0.181486
Iter: 142 	 x= 0.086264 	 y= 0.007829 	 grad= 0.176962
Iter: 143 	 x= 0.084091 	 y= 0.007441 	 grad= 0.172528
Iter: 144 	 x= 0.081963 	 y= 0.007071 	 grad= 0.168183
Iter: 145 	 x= 0.079877 	 y= 0.006718 	 grad= 0.163925
Iter: 146 	 x= 0.077834 	 y= 0.006380 	 grad= 0.159754
Iter: 147 	 x= 0.075834 	 y= 0.006058 	 grad= 0.155669
Iter: 148 	 x= 0.073874 	 y= 0.005751 	 grad= 0.151667
Iter: 149 	 x= 0.071956 	 y= 0.005457 	 grad= 0.147749
Iter: 150 	 x= 0.070078 	 y= 0.005178 	 grad= 0.143912
Iter: 151 	 x= 0.068240 	 y= 0.004911 	 grad= 0.140157
Iter: 152 	 x= 0.066441 	 y= 0.004657 	 grad= 0.136480
Iter: 153 	 x= 0.064681 	 y= 0.004414 	 grad= 0.132882
Iter: 154 	 x= 0.062959 	 y= 0.004184 	 grad= 0.129362
Iter: 155 	 x= 0.061274 	 y= 0.003964 	 grad= 0.125917
Iter: 156 	 x= 0.059626 	 y= 0.003754 	 grad= 0.122548
Iter: 157 	 x= 0.058014 	 y= 0.003555 	 grad= 0.119252
Iter: 158 	 x= 0.056439 	 y= 0.003366 	 grad= 0.116029
Iter: 159 	 x= 0.054898 	 y= 0.003185 	 grad= 0.112877
Iter: 160 	 x= 0.053392 	 y= 0.003014 	 grad= 0.109796
Iter: 161 	 x= 0.051920 	 y= 0.002851 	 grad= 0.106784
Iter: 162 	 x= 0.050482 	 y= 0.002696 	 grad= 0.103840
Iter: 163 	 x= 0.049076 	 y= 0.002548 	 grad= 0.100963
Iter: 164 	 x= 0.047703 	 y= 0.002408 	 grad= 0.098152
Iter: 165 	 x= 0.046362 	 y= 0.002276 	 grad= 0.095406
Iter: 166 	 x= 0.045052 	 y= 0.002149 	 grad= 0.092724
Iter: 167 	 x= 0.043773 	 y= 0.002030 	 grad= 0.090104
Iter: 168 	 x= 0.042524 	 y= 0.001916 	 grad= 0.087546
Iter: 169 	 x= 0.041305 	 y= 0.001808 	 grad= 0.085049
Iter: 170 	 x= 0.040115 	 y= 0.001706 	 grad= 0.082611
Iter: 171 	 x= 0.038954 	 y= 0.001609 	 grad= 0.080231
Iter: 172 	 x= 0.037821 	 y= 0.001517 	 grad= 0.077908
Iter: 173 	 x= 0.036716 	 y= 0.001430 	 grad= 0.075642
Iter: 174 	 x= 0.035637 	 y= 0.001348 	 grad= 0.073431
Iter: 175 	 x= 0.034586 	 y= 0.001270 	 grad= 0.071275
Iter: 176 	 x= 0.033560 	 y= 0.001196 	 grad= 0.069172
Iter: 177 	 x= 0.032560 	 y= 0.001126 	 grad= 0.067120
Iter: 178 	 x= 0.031586 	 y= 0.001060 	 grad= 0.065121
Iter: 179 	 x= 0.030635 	 y= 0.000998 	 grad= 0.063171
Iter: 180 	 x= 0.029709 	 y= 0.000939 	 grad= 0.061271
Iter: 181 	 x= 0.028807 	 y= 0.000883 	 grad= 0.059419
Iter: 182 	 x= 0.027928 	 y= 0.000830 	 grad= 0.057614
Iter: 183 	 x= 0.027072 	 y= 0.000780 	 grad= 0.055856
Iter: 184 	 x= 0.026238 	 y= 0.000733 	 grad= 0.054144
Iter: 185 	 x= 0.025426 	 y= 0.000688 	 grad= 0.052476
Iter: 186 	 x= 0.024635 	 y= 0.000646 	 grad= 0.050851
Iter: 187 	 x= 0.023865 	 y= 0.000607 	 grad= 0.049270
Iter: 188 	 x= 0.023116 	 y= 0.000570 	 grad= 0.047730
Iter: 189 	 x= 0.022386 	 y= 0.000534 	 grad= 0.046231
Iter: 190 	 x= 0.021677 	 y= 0.000501 	 grad= 0.044773
Iter: 191 	 x= 0.020987 	 y= 0.000470 	 grad= 0.043354
Iter: 192 	 x= 0.020315 	 y= 0.000440 	 grad= 0.041973
Iter: 193 	 x= 0.019662 	 y= 0.000413 	 grad= 0.040630
Iter: 194 	 x= 0.019027 	 y= 0.000387 	 grad= 0.039324
Iter: 195 	 x= 0.018409 	 y= 0.000362 	 grad= 0.038054
Iter: 196 	 x= 0.017809 	 y= 0.000339 	 grad= 0.036819
Iter: 197 	 x= 0.017226 	 y= 0.000317 	 grad= 0.035618
Iter: 198 	 x= 0.016659 	 y= 0.000297 	 grad= 0.034451
Iter: 199 	 x= 0.016108 	 y= 0.000278 	 grad= 0.033317
Iter: 200 	 x= 0.015573 	 y= 0.000259 	 grad= 0.032216
Iter: 201 	 x= 0.015053 	 y= 0.000243 	 grad= 0.031145
Iter: 202 	 x= 0.014548 	 y= 0.000227 	 grad= 0.030105
Iter: 203 	 x= 0.014057 	 y= 0.000212 	 grad= 0.029096
Iter: 204 	 x= 0.013582 	 y= 0.000198 	 grad= 0.028115
Iter: 205 	 x= 0.013119 	 y= 0.000184 	 grad= 0.027163
Iter: 206 	 x= 0.012671 	 y= 0.000172 	 grad= 0.026239
Iter: 207 	 x= 0.012236 	 y= 0.000161 	 grad= 0.025342
Iter: 208 	 x= 0.011814 	 y= 0.000150 	 grad= 0.024472
Iter: 209 	 x= 0.011404 	 y= 0.000140 	 grad= 0.023627
Iter: 210 	 x= 0.011007 	 y= 0.000130 	 grad= 0.022808
Iter: 211 	 x= 0.010622 	 y= 0.000121 	 grad= 0.022014
Iter: 212 	 x= 0.010248 	 y= 0.000113 	 grad= 0.021244
Iter: 213 	 x= 0.009886 	 y= 0.000105 	 grad= 0.020497
Iter: 214 	 x= 0.009535 	 y= 0.000098 	 grad= 0.019772
Iter: 215 	 x= 0.009195 	 y= 0.000091 	 grad= 0.019071
Iter: 216 	 x= 0.008866 	 y= 0.000085 	 grad= 0.018391
Iter: 217 	 x= 0.008547 	 y= 0.000079 	 grad= 0.017732
Iter: 218 	 x= 0.008237 	 y= 0.000073 	 grad= 0.017093
Iter: 219 	 x= 0.007938 	 y= 0.000068 	 grad= 0.016475
Iter: 220 	 x= 0.007648 	 y= 0.000063 	 grad= 0.015876
Iter: 221 	 x= 0.007367 	 y= 0.000058 	 grad= 0.015296
Iter: 222 	 x= 0.007096 	 y= 0.000054 	 grad= 0.014735
Iter: 223 	 x= 0.006833 	 y= 0.000050 	 grad= 0.014192
Iter: 224 	 x= 0.006579 	 y= 0.000047 	 grad= 0.013666
Iter: 225 	 x= 0.006332 	 y= 0.000043 	 grad= 0.013157
Iter: 226 	 x= 0.006094 	 y= 0.000040 	 grad= 0.012665
Iter: 227 	 x= 0.005864 	 y= 0.000037 	 grad= 0.012189
Iter: 228 	 x= 0.005642 	 y= 0.000034 	 grad= 0.011729
Iter: 229 	 x= 0.005427 	 y= 0.000032 	 grad= 0.011283
Iter: 230 	 x= 0.005219 	 y= 0.000029 	 grad= 0.010853
Iter: 231 	 x= 0.005018 	 y= 0.000027 	 grad= 0.010437
Iter: 232 	 x= 0.004824 	 y= 0.000025 	 grad= 0.010035
Iter: 233 	 x= 0.004636 	 y= 0.000023 	 grad= 0.009647
Iter: 234 	 x= 0.004455 	 y= 0.000021 	 grad= 0.009272
Iter: 235 	 x= 0.004280 	 y= 0.000020 	 grad= 0.008910
Iter: 236 	 x= 0.004111 	 y= 0.000018 	 grad= 0.008560
Iter: 237 	 x= 0.003948 	 y= 0.000017 	 grad= 0.008222
Iter: 238 	 x= 0.003791 	 y= 0.000016 	 grad= 0.007896
Iter: 239 	 x= 0.003639 	 y= 0.000014 	 grad= 0.007582
Iter: 240 	 x= 0.003493 	 y= 0.000013 	 grad= 0.007278
Iter: 241 	 x= 0.003351 	 y= 0.000012 	 grad= 0.006985
Iter: 242 	 x= 0.003215 	 y= 0.000011 	 grad= 0.006703
Iter: 243 	 x= 0.003084 	 y= 0.000010 	 grad= 0.006430
Iter: 244 	 x= 0.002957 	 y= 0.000010 	 grad= 0.006167
Iter: 245 	 x= 0.002835 	 y= 0.000009 	 grad= 0.005914
Iter: 246 	 x= 0.002717 	 y= 0.000008 	 grad= 0.005670
Iter: 247 	 x= 0.002604 	 y= 0.000007 	 grad= 0.005435
Iter: 248 	 x= 0.002495 	 y= 0.000007 	 grad= 0.005208
Iter: 249 	 x= 0.002390 	 y= 0.000006 	 grad= 0.004990
Iter: 250 	 x= 0.002289 	 y= 0.000006 	 grad= 0.004780
Iter: 251 	 x= 0.002191 	 y= 0.000005 	 grad= 0.004578
Iter: 252 	 x= 0.002098 	 y= 0.000005 	 grad= 0.004383
Iter: 253 	 x= 0.002007 	 y= 0.000004 	 grad= 0.004195
Iter: 254 	 x= 0.001921 	 y= 0.000004 	 grad= 0.004015
Iter: 255 	 x= 0.001837 	 y= 0.000004 	 grad= 0.003841
Iter: 256 	 x= 0.001757 	 y= 0.000003 	 grad= 0.003674
Iter: 257 	 x= 0.001680 	 y= 0.000003 	 grad= 0.003514
Iter: 258 	 x= 0.001606 	 y= 0.000003 	 grad= 0.003360
Iter: 259 	 x= 0.001534 	 y= 0.000003 	 grad= 0.003211
Iter: 260 	 x= 0.001466 	 y= 0.000002 	 grad= 0.003069
Iter: 261 	 x= 0.001400 	 y= 0.000002 	 grad= 0.002932
Iter: 262 	 x= 0.001337 	 y= 0.000002 	 grad= 0.002800
Iter: 263 	 x= 0.001276 	 y= 0.000002 	 grad= 0.002674
Iter: 264 	 x= 0.001218 	 y= 0.000002 	 grad= 0.002552
Iter: 265 	 x= 0.001162 	 y= 0.000001 	 grad= 0.002436
Iter: 266 	 x= 0.001108 	 y= 0.000001 	 grad= 0.002324
Iter: 267 	 x= 0.001057 	 y= 0.000001 	 grad= 0.002217
Iter: 268 	 x= 0.001008 	 y= 0.000001 	 grad= 0.002114
Iter: 269 	 x= 0.000960 	 y= 0.000001 	 grad= 0.002015
Iter: 270 	 x= 0.000915 	 y= 0.000001 	 grad= 0.001921
Iter: 271 	 x= 0.000872 	 y= 0.000001 	 grad= 0.001830
Iter: 272 	 x= 0.000830 	 y= 0.000001 	 grad= 0.001743
Iter: 273 	 x= 0.000790 	 y= 0.000001 	 grad= 0.001660
Iter: 274 	 x= 0.000752 	 y= 0.000001 	 grad= 0.001580
Iter: 275 	 x= 0.000715 	 y= 0.000001 	 grad= 0.001504
Iter: 276 	 x= 0.000680 	 y= 0.000001 	 grad= 0.001431
Iter: 277 	 x= 0.000647 	 y= 0.000000 	 grad= 0.001361
Iter: 278 	 x= 0.000615 	 y= 0.000000 	 grad= 0.001294
Iter: 279 	 x= 0.000584 	 y= 0.000000 	 grad= 0.001229
Iter: 280 	 x= 0.000555 	 y= 0.000000 	 grad= 0.001168
Iter: 281 	 x= 0.000527 	 y= 0.000000 	 grad= 0.001110
Iter: 282 	 x= 0.000500 	 y= 0.000000 	 grad= 0.001053
Iter: 283 	 x= 0.000474 	 y= 0.000000 	 grad= 0.001000
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>Use PyTorch to find a local minimum of the function <span class="math notranslate nohighlight">\(f(x,y) = 1-x-y + x^2 + 2y^2\)</span>.</p>
<p>What would be a good stopping criterion? In the previous example, we used the absolute value of the gradient. This needs to be updated.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="9-BLUE.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">8. </span>Best linear unbiased estimator and the bias-variance decomposition</p>
      </div>
    </a>
    <a class="right-next"
       href="11-Shrinkage-methods.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">10. </span>Improving linear regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-direction-of-steepest-descent">9.1. The direction of steepest descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gradient-descent-algorithm">9.2. The gradient descent algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-a-basic-gradient-descent">9.3. Implementing a basic gradient descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-pytorch">9.4. Using PyTorch</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dominique Guillot
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>