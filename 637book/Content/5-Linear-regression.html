
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4. Linear Regression &#8212; Math 637 Mathematical Techniques in Data Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Content/5-Linear-regression';</script>
    <link rel="canonical" href="/637book/Content/5-Linear-regression.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. Lab 1: Linear regression and the cars data" href="6-Lab-Cars.html" />
    <link rel="prev" title="3. Supervised vs Unsupervised learning" href="4-Supervised-Unsupervised.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="1-intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/637-logo.png" class="logo__image only-light" alt="Math 637 Mathematical Techniques in Data Science - Home"/>
    <script>document.write(`<img src="../_static/637-logo.png" class="logo__image only-dark" alt="Math 637 Mathematical Techniques in Data Science - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="1-intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2-Python.html">1. Setting up Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="3-Basic-Python.html">2. Basic Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="4-Supervised-Unsupervised.html">3. Supervised vs Unsupervised learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">4. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="6-Lab-Cars.html">5. Lab 1: Linear regression and the cars data</a></li>
<li class="toctree-l1"><a class="reference internal" href="7-Learning-outside-training.html">6. Learning outside the training set</a></li>
<li class="toctree-l1"><a class="reference internal" href="8-Lab2-train-test.html">7. Lab 2: Training vs testing error</a></li>
<li class="toctree-l1"><a class="reference internal" href="9-BLUE.html">8. Best linear unbiased estimator and the bias-variance decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-Lab3-gradient-descent.html">9. Lab 3: Gradient descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="11-Shrinkage-methods.html">10. Improving linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-Model-selection.html">11. Model selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-Lab4-lasso.html">12. Lab 4: using the LASSO</a></li>
<li class="toctree-l1"><a class="reference internal" href="14-Computing-lasso.html">13. Computing the LASSO solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-Lab5-coordinate-descent.html">14. Lab 5: Coordinate descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="16-LASSO-theoretical.html">15. Theoretical guarantees for the LASSO</a></li>
<li class="toctree-l1"><a class="reference internal" href="17-categorical-data.html">16. Analyzing categorical data</a></li>
<li class="toctree-l1"><a class="reference internal" href="18-Lab6-nearest-neighbors.html">17. Lab 6: nearest neighbors</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Content/5-Linear-regression.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-linear-regression-model">4.1. The linear regression model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-predicting-car-prices">4.1.1. Example: predicting car prices</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions">4.1.2. Loss functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-optimal-coefficients">4.1.3. Finding the optimal coefficients</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-regression">
<h1><span class="section-number">4. </span>Linear Regression<a class="headerlink" href="#linear-regression" title="Link to this heading">#</a></h1>
<p>The first family of models we will study are linear regression models. While such models may seem elementary at first sight, they are very useful and can often be used as a baseline before trying more sophisticated techniques. Another important advantage of linear models is that they are easy to <strong>interpret</strong>. In many situations, more complicated models (e.g., deep learning) may give better results, but provide no explanation of how they get their results. When working with large datasets, some of these models may also able to infer information such as gender and race, and use it indirectly. In some areas in the industry, this is a deal breaker. For example, the way mortgages are written is very regulated. Only specific information can typically be used to accept or decline a loan. On the other hand, in other areas (e.g., quantitative finance) building a model that produces excellent results can be the only thing that matters. Still, in general, being able to understand how a model arrived at a certain prediction is typically very important and desirable.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="../_images/black-box.png"><img alt="../_images/black-box.png" src="../_images/black-box.png" style="width: 300px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 4.1 </span><span class="caption-text">Knowing how a model reaches a certain conclusion is often desirable. Very complex models tend to act like black boxes.</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="the-linear-regression-model">
<h2><span class="section-number">4.1. </span>The linear regression model<a class="headerlink" href="#the-linear-regression-model" title="Link to this heading">#</a></h2>
<p>In linear regression, we attempt to predict a dependent variable <span class="math notranslate nohighlight">\(Y\)</span> using a linear combination of predictors <span class="math notranslate nohighlight">\(X_1, X_2, \dots, X_p\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-e-lin-reg">
<span class="eqno">(4.1)<a class="headerlink" href="#equation-e-lin-reg" title="Link to this equation">#</a></span>\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p. 
\]</div>
<p>The coefficients <span class="math notranslate nohighlight">\(\beta_0, \beta_1, \dots, \beta_p\)</span> are called the <em>regression coefficients</em>. The first coefficient, <span class="math notranslate nohighlight">\(\beta_0\)</span>, is called the <em>intercept</em> of the model.</p>
<p>In order to train a linear regression model, one needs several simultaneous observations of <span class="math notranslate nohighlight">\(Y\)</span> and of <span class="math notranslate nohighlight">\(X_1, \dots, X_p\)</span>. We therefore assume several <em>training pairs</em> <span class="math notranslate nohighlight">\((y_i, \mathbf{x_i})\)</span> are provided to train the model, where <span class="math notranslate nohighlight">\(\mathbf{x_i} = (x_{i,1}, x_{i,2}, \dots, x_{i,p}) \in \mathbb{R}^p\)</span> are the feature observations that correspond to the output <span class="math notranslate nohighlight">\(y_i\)</span> for <span class="math notranslate nohighlight">\(i=1,\dots, n\)</span>. Let <span class="math notranslate nohighlight">\(f(x) = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p\)</span>. Given the training data, our goal is to estimate the values of the regression coefficients so that <span class="math notranslate nohighlight">\(y_i \approx f(x_i)\)</span> for all <span class="math notranslate nohighlight">\(i=1,\dots,n\)</span>.</p>
<section id="example-predicting-car-prices">
<h3><span class="section-number">4.1.1. </span>Example: predicting car prices<a class="headerlink" href="#example-predicting-car-prices" title="Link to this heading">#</a></h3>
<p>The file <a class="reference download internal" download="" href="../_downloads/c3db5829009749bfee8a213b89345458/JSE_Car_Lab.csv"><span class="xref download myst">JSE_Car_Lab.csv</span></a> contains the price of <span class="math notranslate nohighlight">\(n=805\)</span> cars as well as <span class="math notranslate nohighlight">\(p=11\)</span> features for each car. The data was compiled by Kelley Blue Book.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/data-cars.jpg"><img alt="../_images/data-cars.jpg" src="../_images/data-cars.jpg" style="width: 800px;" />
</a>
</figure>
<p>It is very natural to try to predict the price of the cars using a linear combination of the different numerical features. For example, one might expect the car mileage to play a significant role in predicting the value of the car.</p>
<p>In order to use the non-numerical features (e.g., the car make), one needs to <em>encode</em> them in some way. A popular way to encode such variables is to use a <strong>one-hot encoding</strong>. For example, suppose there are only 3 possible car makes (say, Acura, Buick, and Chevrolet). It would be tempting to encode these car makes directly using a number (e.g., 1 = Acura, 2 = Buick, 3 = Chevrolet). However, in a linear model, doubling the value of a variable will double its impact on the price. This type of relationship cannot be justified here. Instead, a one-hot encoding adds three columns to the dataset, where each column indicates whether the car is of a given brand (1) or not (0). Thus, each row has precisely one “1” that indicates the make of the car.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Acura</p></th>
<th class="head"><p>Buick</p></th>
<th class="head"><p>Chevrolet</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
</div>
<p>For example, in the table above, the first row represents an Acura, the second row a Buick, and the third row a Chevrolet. Notices that this approach replaces the make column by <span class="math notranslate nohighlight">\(3\)</span> new columns (the new value of <span class="math notranslate nohighlight">\(p\)</span> is now <span class="math notranslate nohighlight">\(13\)</span>).</p>
<p>Using a one-hot encoding of non-numerical variables has the advantage of treating each possible value of the variable equally. One downside thought is that the number of features of the data can increase significantly if the variable takes a lot of different values (since one column is added for each possible value).</p>
<p>In the next chapter, you will analyze the JSE_Car_Lab dataset and will construct your own linear models to predict the price of cars.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Constructing a model for the price of the cars is very useful. Once a good model has been constructed, it can be used to estimate the market value of <strong>any</strong> car. This provides very useful information to car buyers and sellers.</p>
</div>
<div class="admonition-removing-the-intercept admonition" id="n-lin-reg-no-intercept">
<p class="admonition-title">Removing the intercept</p>
<p>When working with a linear model, we can always append a column whose entries are all <span class="math notranslate nohighlight">\(1\)</span> to the dataset, and discard the intercept from the model. Observe that by doing that, the regression coefficient <span class="math notranslate nohighlight">\(\beta_1\)</span> becomes the intercept. This simple modification makes it easier to make calculations with linear models and will be used below.</p>
</div>
</section>
<section id="loss-functions">
<span id="sec-loss-function"></span><h3><span class="section-number">4.1.2. </span>Loss functions<a class="headerlink" href="#loss-functions" title="Link to this heading">#</a></h3>
<p>In order to measure how well a model is doing, we need to fix a way to measure the error made by the model on a given dataset. Such a measure of error is called a <strong>loss function</strong>.</p>
<p>When predicting continuous data (as in the cars problem above), a loss function that is commonly used is the <em>mean squared error</em> (MSE):</p>
<div class="math notranslate nohighlight">
\[
MSE(\beta_0, \beta_1, \dots, \beta_p) = \frac{1}{n} \sum_{i=1}^n (y_i - f(\mathbf{x_i}))^2. 
\]</div>
<p>Note that the MSE depends on the regression coefficients since <span class="math notranslate nohighlight">\(f\)</span> is a function of <span class="math notranslate nohighlight">\(\beta_0, \beta_1, \dots, \beta_p\)</span>.</p>
<p>When predicting an output variable taking finitely many possible values the <em>cross-entropy</em> loss function is typically used. We will discuss cross-entropy and loss functions in the chapter on <a class="reference internal" href="17-categorical-data.html#c-categorical"><span class="std std-ref">Analyzing categorical data</span></a>.</p>
<p>When fitting a model, the goal is to minimize the loss function in order for the model to match the data as best as possible. To find the optimal regression coefficients in Equation <a class="reference internal" href="#equation-e-lin-reg">(4.1)</a>, we therefore need to solve the optimization problem:</p>
<div class="math notranslate nohighlight" id="equation-e-lin-reg-optim">
<span class="eqno">(4.2)<a class="headerlink" href="#equation-e-lin-reg-optim" title="Link to this equation">#</a></span>\[
\min_{\beta_0, \beta_1, \dots, \beta_p \in \mathbb{R}} L(\beta_0, \beta_1, \dots, \beta_p) = \min_{\beta_0, \beta_1, \dots, \beta_p \in \mathbb{R}} \frac{1}{n} \sum_{i=1}^n (y_i - f(\mathbf{x_i}))^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, x_2, \dots, x_p)\)</span>.</p>
</section>
<section id="finding-the-optimal-coefficients">
<span id="sec-finding-optimal-coefficients"></span><h3><span class="section-number">4.1.3. </span>Finding the optimal coefficients<a class="headerlink" href="#finding-the-optimal-coefficients" title="Link to this heading">#</a></h3>
<p>There are different approach to solve the optimization problem <a class="reference internal" href="#equation-e-lin-reg-optim">(4.2)</a>.</p>
<p>Recall that when a smooth functions of several variables reaches a local minimum at an interior point of its domain, its gradient has to be the zero vector. We will therefore compute the gradient of the above loss function and set it to zero to find candidates for the minimum.</p>
<p>To simplify notation, let <span class="math notranslate nohighlight">\(y\)</span> be the vector contains all the outputs of the regression, and let <span class="math notranslate nohighlight">\(X\)</span> be the matrix with rows <span class="math notranslate nohighlight">\(\mathbf{x_1}^T, \mathbf{x_2}^T, \dots, \mathbf{x_n}^T\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y = (y_1, y_2, \dots, y_n)^T \qquad X = \begin{pmatrix}
\mathbf{x_1} \\
\mathbf{x_2} \\
\vdots \\
\mathbf{x_n}
\end{pmatrix}.
\end{split}\]</div>
<p>Notice that <span class="math notranslate nohighlight">\(X = (x_{i,j})\)</span> is an <span class="math notranslate nohighlight">\(n \times p\)</span> matrix and <span class="math notranslate nohighlight">\(y \in \mathbb{R}^n\)</span>. To simplify the calculations, we will assume the first column of <span class="math notranslate nohighlight">\(X\)</span> has all its entries equal to <span class="math notranslate nohighlight">\(1\)</span> and the linear model has no intercept (See the <a class="reference internal" href="#n-lin-reg-no-intercept"><span class="std std-ref">(Removing the intercept)</span></a> box).</p>
<p>Observe that the loss function can be re-written using a matrix vector product:</p>
<div class="math notranslate nohighlight">
\[
L(\beta_1, \dots, \beta_p) = \frac{1}{n} \|y - X\beta\|_2^2, 
\]</div>
<p>where <span class="math notranslate nohighlight">\(\|\mathbf{z}\|_2^2 = \sum_{i=1}^n z_i^2\)</span> for <span class="math notranslate nohighlight">\(z \in \mathbb{R}^n\)</span>. Expanding the expression, we obtain</p>
<div class="math notranslate nohighlight">
\[
L(\beta_1, \dots, \beta_p) = \frac{1}{n} \sum_{i=1}^n \left(y_i - \sum_{j=1}^p x_{i,j} \beta_j\right)^2.
\]</div>
<p>For <span class="math notranslate nohighlight">\(k=1,\dots,p\)</span>, differentiating with respect to <span class="math notranslate nohighlight">\(\beta_k\)</span>  yields</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial L}{\partial \beta_k} = -\frac{2}{n} \sum_{i=1}^n x_{i,k} \left(y_i - \sum_{j=1}^p x_{i,j} \beta_j\right). 
\]</div>
<p>Finally, setting the gradient to <span class="math notranslate nohighlight">\(0\)</span>, we obtain:</p>
<div class="math notranslate nohighlight" id="equation-e-normal">
<span class="eqno">(4.3)<a class="headerlink" href="#equation-e-normal" title="Link to this equation">#</a></span>\[
\sum_{i=1}^n x_{i,k} y_i = \sum_{i=1}^n \sum_{j=1}^p x_{i,k} x_{i,j} \beta_j \qquad (k=1,\dots,p).
\]</div>
<div class="admonition-matrix-vector-and-matrix-matrix-products admonition">
<p class="admonition-title">Matrix-vector and Matrix-matrix products</p>
<p>Recall that when we multiply a matrix <span class="math notranslate nohighlight">\(A = (a_{i,j}) \in \mathbb{R}^{n \times p}\)</span> by a vector <span class="math notranslate nohighlight">\(x = (x_1, \dots, x_p)^T \in \mathbb{R}^p\)</span>, the <span class="math notranslate nohighlight">\(k\)</span>-th entry of <span class="math notranslate nohighlight">\(Ax\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
(Ax)_k = \sum_{j=1}^p a_{k,j} x_j. 
\]</div>
<p>Similarly, if <span class="math notranslate nohighlight">\(B\)</span> is a <span class="math notranslate nohighlight">\(p \times r\)</span> matrix, then the <span class="math notranslate nohighlight">\((k,l)\)</span>-th entry of the matrix product <span class="math notranslate nohighlight">\(AB\)</span> is</p>
<div class="math notranslate nohighlight">
\[
(AB)_{k,l} = \sum_{j=1}^p a_{k,j} b_{j,l}. 
\]</div>
<p>Finally, recall that the <span class="math notranslate nohighlight">\((k,l)\)</span> entry of <span class="math notranslate nohighlight">\(A^T\)</span> is <span class="math notranslate nohighlight">\(a_{l,k}\)</span>.</p>
</div>
<p>Now, Equation <a class="reference internal" href="#equation-e-normal">(4.3)</a> can be written in a better way using matrix-vector and matrix-matrix products. First observe that the left-hand side is the <span class="math notranslate nohighlight">\(k\)</span>-th entry of <span class="math notranslate nohighlight">\(X^T y\)</span>. Similarly, the right-hand side is the <span class="math notranslate nohighlight">\(k\)</span>-th entry of <span class="math notranslate nohighlight">\(X^TX \beta\)</span>, where <span class="math notranslate nohighlight">\(\beta = (\beta_1, \dots, \beta_p)^T\)</span>. We therefore conclude that the gradient of the loss function is <span class="math notranslate nohighlight">\(0\)</span> if and only if:</p>
<div class="math notranslate nohighlight">
\[
\boxed{X^TX \beta = X^T y}.
\]</div>
<p>The above linear system is called the <strong>normal equations</strong> associated to the linear regression problem. Since the loss function has to admit a minimum at some <span class="math notranslate nohighlight">\(\beta \in \mathbb{R}^p\)</span>, we are guaranteed that the normal equations have at least one solution. However, unless <span class="math notranslate nohighlight">\(X^TX\)</span> has full rank (and is therefore invertible), the solution may not be unique. However, with some extra work, one can show that each solution of the normal equations achieves the minimum value of the loss function. We can therefore solve the normal equations to obtain all the minima of the loss function.</p>
<p>When <span class="math notranslate nohighlight">\(X^TX\)</span> is invertible, the unique minimum of the loss function is:</p>
<div class="math notranslate nohighlight">
\[
\boxed{\widehat{\beta} = (X^TX)^{-1} X^T y}.
\]</div>
<p>We often call <span class="math notranslate nohighlight">\(\widehat{\beta}\)</span> the <em>least squares</em> estimator as it minimizes the sum of squares of the error.</p>
<p>In conclusion, the above work shows that finding the optimal coefficients in linear regression is equivalent to solving a linear system of equations. In the next chapter, we will see how this can be done with Python.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="4-Supervised-Unsupervised.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Supervised vs Unsupervised learning</p>
      </div>
    </a>
    <a class="right-next"
       href="6-Lab-Cars.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Lab 1: Linear regression and the cars data</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-linear-regression-model">4.1. The linear regression model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-predicting-car-prices">4.1.1. Example: predicting car prices</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions">4.1.2. Loss functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-optimal-coefficients">4.1.3. Finding the optimal coefficients</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dominique Guillot
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>