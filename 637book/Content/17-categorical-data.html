
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>16. Analyzing categorical data &#8212; Math 637 Mathematical Techniques in Data Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Content/17-categorical-data';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="17. Lab 6: nearest neighbors" href="18-Lab6-nearest-neighbors.html" />
    <link rel="prev" title="15. Theoretical guarantees for the LASSO" href="16-LASSO-theoretical.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="1-intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/637-logo.png" class="logo__image only-light" alt="Math 637 Mathematical Techniques in Data Science - Home"/>
    <script>document.write(`<img src="../_static/637-logo.png" class="logo__image only-dark" alt="Math 637 Mathematical Techniques in Data Science - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="1-intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2-Python.html">1. Setting up Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="3-Basic-Python.html">2. Basic Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="4-Supervised-Unsupervised.html">3. Supervised vs Unsupervised learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="5-Linear-regression.html">4. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="6-Lab-Cars.html">5. Lab 1: Linear regression and the cars data</a></li>
<li class="toctree-l1"><a class="reference internal" href="7-Learning-outside-training.html">6. Learning outside the training set</a></li>
<li class="toctree-l1"><a class="reference internal" href="8-Lab2-train-test.html">7. Lab 2: Training vs testing error</a></li>
<li class="toctree-l1"><a class="reference internal" href="9-BLUE.html">8. Best linear unbiased estimator and the bias-variance decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-Lab3-gradient-descent.html">9. Lab 3: Gradient descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="11-Shrinkage-methods.html">10. Improving linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-Model-selection.html">11. Model selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-Lab4-lasso.html">12. Lab 4: using the LASSO</a></li>
<li class="toctree-l1"><a class="reference internal" href="14-Computing-lasso.html">13. Computing the LASSO solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-Lab5-coordinate-descent.html">14. Lab 5: Coordinate descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="16-LASSO-theoretical.html">15. Theoretical guarantees for the LASSO</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">16. Analyzing categorical data</a></li>
<li class="toctree-l1"><a class="reference internal" href="18-Lab6-nearest-neighbors.html">17. Lab 6: nearest neighbors</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Content/17-categorical-data.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Analyzing categorical data</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">16.1. Loss function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-entropy">16.1.1. Cross-entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example">16.1.1.1. Example</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-nearest-neighbors-model">16.2. The nearest neighbors model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">16.2.1. Example</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="analyzing-categorical-data">
<span id="c-categorical"></span><h1><span class="section-number">16. </span>Analyzing categorical data<a class="headerlink" href="#analyzing-categorical-data" title="Link to this heading">#</a></h1>
<p>Many of the problems we considered so far involved predicting a <em>continuous</em> variable (e.g., the price of a car). In many real-world problems, we are instead interested in predicting a <em>categorical</em> variable, i.e., a variable that can take only finitely many values. While the difference may seem minor from a mathematical perspective, different techniques must be used to deal with the categorical setting.</p>
<section id="loss-function">
<h2><span class="section-number">16.1. </span>Loss function<a class="headerlink" href="#loss-function" title="Link to this heading">#</a></h2>
<p>When working with continuous data, we often use the mean-squared error (MSE)</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n} \sum_{i=1}^n (\widehat{y}_i - y_i)^2
\]</div>
<p>as a measure of error, where <span class="math notranslate nohighlight">\(\widehat{y}_i\)</span> is the predicted value of sample <span class="math notranslate nohighlight">\(y_i\)</span>. When fitting a model (e.g., linear regression), we then choose the model parameters to minimize that error (i.e., we choose the parameters to fit the training data as best as possible).</p>
<p>Suppose now that the output <span class="math notranslate nohighlight">\(Y\)</span> can only take finitely many values, say <span class="math notranslate nohighlight">\(\{1, \dots, K\}\)</span>. Think of these as labels for data points. For example, suppose we have pictures of cats, dogs, and birds that we would like to automatically classify using a model. Let us label the images using “1” for cats, “2” for dogs, and “3” for birds. We can then aim to train a model to predict the class of each image as accurately as possible.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Digital images can be seen as a collection of <em>pixels</em>, small squares with different colors. An image can therefore be encoded as a matrix whose <span class="math notranslate nohighlight">\((i,j)-th\)</span> entry is a number representing the color of the corresponding pixel.</p>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="../_images/pokemon.png"><img alt="../_images/pokemon.png" src="../_images/pokemon.png" style="width: 200px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 16.1 </span><span class="caption-text">An illustration of the pixels of an image</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
<div class="legend">
<p>An image of dimension <span class="math notranslate nohighlight">\(m \times n\)</span> can be reshaped into an <span class="math notranslate nohighlight">\(mn \times 1\)</span> vector by stacking the columns (or the rows) of the corresponding matrix. We can thus think of an image as a vector. The image classification problem therefore involves taking a vector representing an image as input, and returning the correct image label.</p>
</div>
</figcaption>
</figure>
</div>
<p>Suppose the correct label of a sample <span class="math notranslate nohighlight">\(y_1\)</span> is <span class="math notranslate nohighlight">\(1\)</span> (i.e., the first image is a cat). In the MSE setting, a predicted value of <span class="math notranslate nohighlight">\(3\)</span> (bird) for <span class="math notranslate nohighlight">\(y_1\)</span> is worst than a predicted value of <span class="math notranslate nohighlight">\(2\)</span> (dog) since <span class="math notranslate nohighlight">\((3-1)^2 &gt; (2-1)^2\)</span>. However, here, the labels <span class="math notranslate nohighlight">\(1,2,3\)</span> are completely arbitrary so a predicted label of <span class="math notranslate nohighlight">\(3\)</span> is not really worse than a predicted label <span class="math notranslate nohighlight">\(2\)</span>. It therefore makes sense to use a different loss function.</p>
<section id="cross-entropy">
<h3><span class="section-number">16.1.1. </span>Cross-entropy<a class="headerlink" href="#cross-entropy" title="Link to this heading">#</a></h3>
<p>Instead of predicting the label of a given sample, many categorical models predict a probability distribution on the different labels. In that context, a very common loss function is <strong>cross-entropy</strong>. Cross-entropy can be seen as a measure of distance between two probability distribution. Here, a probability distribution on <span class="math notranslate nohighlight">\(\{1,\dots,K\}\)</span> is a collection of numbers <span class="math notranslate nohighlight">\((p_1, \dots, p_K)\)</span> such that</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(0 \leq p_i \leq 1\)</span> for all <span class="math notranslate nohighlight">\(i=1, \dots, K\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{i=1}^K p_i = 1\)</span>.</p></li>
</ol>
<div class="admonition-definition-cross-entropy admonition">
<p class="admonition-title">Definition (cross-entropy)</p>
<p>Let <span class="math notranslate nohighlight">\(p, q\)</span> be two probability distributions on <span class="math notranslate nohighlight">\(\{1,\dots, K\}\)</span>. The <em>cross-entropy</em> <span class="math notranslate nohighlight">\(H(p,q)\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
H(p,q) := -\sum_{i=1}^K p_i \log q_i, 
\]</div>
<p>where we use the convention <span class="math notranslate nohighlight">\(0 \cdot \log 0 = 0\)</span>.</p>
</div>
<p>Now, instead of using labels such as <span class="math notranslate nohighlight">\(1, 2, 3, etc.\)</span> for the different categories, we use a <strong>one-hot encoding</strong>. This means that if <span class="math notranslate nohighlight">\(y_i\)</span> has label <span class="math notranslate nohighlight">\(j\)</span>, we represent it as a <span class="math notranslate nohighlight">\(K\)</span> dimensional vector with a <span class="math notranslate nohighlight">\(1\)</span> in the <span class="math notranslate nohighlight">\(j\)</span>-th position, and zeros everywhere else:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
y_i = (0,0,\dots,0, &amp;\underbrace{1},0,0,\dots,0).\\
&amp;j \textrm{-th}
\end{align*}\]</div>
<p>Equivalently, we can think of <span class="math notranslate nohighlight">\(y_i\)</span> as the probability distribution taking value <span class="math notranslate nohighlight">\(j\)</span> with probability <span class="math notranslate nohighlight">\(1\)</span>. We can now compare the predicted probability distribution</p>
<div class="math notranslate nohighlight">
\[
\widehat{y}_i = (\widehat{y}_1^{(1)}, \dots, \widehat{y}_i^{(K)})
\]</div>
<p>with <span class="math notranslate nohighlight">\(y_i\)</span> using cross-entropy:</p>
<div class="math notranslate nohighlight">
\[
H(y_i, \widehat{y}_i) = -\sum_{j=1}^K y_i^{(j)} \log \widehat{y}_i^{(j)}. 
\]</div>
<p>Finally, we can average the cross-entropy over all the samples to measure how the model is doing</p>
<div class="math notranslate nohighlight">
\[
L(y, \widehat{y}) = \frac{1}{n} \sum_{i=1}^n H(y_i, \widehat{y}_i) = -\frac{1}{n} \sum_{i=1}^n \sum_{j=1}^K y_i^{(j)} \log \widehat{y}_i^{(j)}. 
\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Cross-entropy finds its origin in information theory, where it is used to measure the expected number of bits needed to encode samples from a true discrete distribution <span class="math notranslate nohighlight">\(p\)</span> when using a coding scheme optimized for a model distribution <span class="math notranslate nohighlight">\(q\)</span>. It is commonly used in probability theory to compare probability distributions.</p>
</div>
<section id="example">
<h4><span class="section-number">16.1.1.1. </span>Example<a class="headerlink" href="#example" title="Link to this heading">#</a></h4>
<p>Suppose with work with data having <span class="math notranslate nohighlight">\(3\)</span> different categories. Assume one sample has label <span class="math notranslate nohighlight">\(y = (1,0,0)\)</span>. Consider two possible predictions <span class="math notranslate nohighlight">\((1/3,1/3,1/3)\)</span> and <span class="math notranslate nohighlight">\((1/4, 1/2, 1/4)\)</span>. Let us compute which distribution is closest to <span class="math notranslate nohighlight">\(y\)</span> with respect to cross-entropy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">):</span>
    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">CE</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">p</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">q</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">CE</span> <span class="o">-=</span> <span class="n">p</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">q</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
    <span class="k">return</span><span class="p">(</span><span class="n">CE</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">])</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">])</span>

<span class="n">CE1</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y1</span><span class="p">)</span>
<span class="n">CE2</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">CE1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">CE2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0986122886681098
1.3862943611198906
</pre></div>
</div>
</div>
</div>
<p>We conclude that <span class="math notranslate nohighlight">\((1/3,1/3,1/3)\)</span> is “closer” to <span class="math notranslate nohighlight">\(y\)</span> than <span class="math notranslate nohighlight">\((1/4,1/2,1/4)\)</span>.</p>
</section>
</section>
</section>
<section id="the-nearest-neighbors-model">
<h2><span class="section-number">16.2. </span>The nearest neighbors model<a class="headerlink" href="#the-nearest-neighbors-model" title="Link to this heading">#</a></h2>
<p>A simple approach to predict the labels in the categorical setting is to use the neighbors of a points to guide the prediction. Suppose the predictors <span class="math notranslate nohighlight">\(x_1, \dots x_n\)</span> belong to the <span class="math notranslate nohighlight">\(N\)</span>-dimensional Euclidean space <span class="math notranslate nohighlight">\(\mathbb{R}^N\)</span> and assume the response <span class="math notranslate nohighlight">\(y_1, \dots, y_n\)</span> are categorical and encoded using a one-hot encoding as described above. For <span class="math notranslate nohighlight">\(x \in \mathbb{R}^N\)</span> and an integer <span class="math notranslate nohighlight">\(1 \leq k \leq n-1\)</span>, let <span class="math notranslate nohighlight">\(N_k(x)\)</span> denote the set of <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors of <span class="math notranslate nohighlight">\(x\)</span> (i.e., the <span class="math notranslate nohighlight">\(k\)</span> points closer to <span class="math notranslate nohighlight">\(x\)</span> among <span class="math notranslate nohighlight">\(x_1, \dots, x_n\)</span>).</p>
<div class="admonition-definition-k-nearest-neighbors-predictor admonition">
<p class="admonition-title">Definition (<span class="math notranslate nohighlight">\(k\)</span> nearest neighbors predictor)</p>
<p>In the above setting, the <em><span class="math notranslate nohighlight">\(k\)</span> nearest neighbors predictor</em> is</p>
<div class="math notranslate nohighlight">
\[
\widehat{Y}(x) = \frac{1}{k} \sum_{x_m \in N_k(x)} y_m.
\]</div>
</div>
<p>In other words, the <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors predictor returns the proportion of samples of each category in the <span class="math notranslate nohighlight">\(k\)</span> neighborhood of <span class="math notranslate nohighlight">\(x\)</span>. New points can then be classified using the category with the largest proportion of neighbors. The nearest neighbors approach thus uses a “majority vote” based on the value of the nearest neighbors to make the prediction. This makes sense in scenarios where “similar” points typically have the same label.</p>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="../_images/Fig2p2.png"><img alt="../_images/Fig2p2.png" src="../_images/Fig2p2.png" style="width: 300px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 16.2 </span><span class="caption-text">An illustration of the <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors classifier with <span class="math notranslate nohighlight">\(k=15\)</span>.</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="id1">
<h3><span class="section-number">16.2.1. </span>Example<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>Consider the following categorical dataset:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X = 
\begin{bmatrix}
2 &amp; 5 \\
-1 &amp; 3 \\
4 &amp; 0 \\
0 &amp; -2 \\
3 &amp; 1
\end{bmatrix}, \quad
y =
\begin{bmatrix}
0 \\
2 \\
1 \\
0 \\
2
\end{bmatrix}.
\end{split}\]</div>
<p>Let us use Python to find the <span class="math notranslate nohighlight">\(3\)</span> nearest classifier of the new point <span class="math notranslate nohighlight">\(x = [1,1]\)</span>. We first enter the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Feature matrix (5 samples, 2 features)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">])</span>

<span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># Number of samples</span>

<span class="c1"># Response vector</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="c1"># Let us convert y to a one-hot encoding</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">sparse_output</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">y_one_hot</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>We can verify that <span class="math notranslate nohighlight">\(y\)</span> was encoded properly:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">y_one_hot</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1. 0. 0.]
 [0. 0. 1.]
 [0. 1. 0.]
 [1. 0. 0.]
 [0. 0. 1.]]
</pre></div>
</div>
</div>
</div>
<p>Let us now compute the 3 nearest neighbors predictor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Point to predict</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># Compute the distance between x and the samples</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:])</span>

<span class="c1"># Find the index of the 3 nearest neighbors</span>
<span class="n">I</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Returns the indices of d from smallest to largest</span>

<span class="n">I3</span><span class="o">=</span> <span class="n">I</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>  <span class="c1"># 3 nearest neighbors</span>

<span class="c1"># Average the labels of the nearest neighbors</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">y_one_hot</span><span class="p">[</span><span class="n">I3</span><span class="p">,:]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final prediction: &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">yhat</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.33333333 0.         0.66666667]]
Final prediction: 
2
</pre></div>
</div>
</div>
</div>
<p>Since the label with the largest proportion of <span class="math notranslate nohighlight">\(3\)</span>-neighbors of <span class="math notranslate nohighlight">\(x\)</span> is “2”, we classify the new point <span class="math notranslate nohighlight">\(x\)</span> as “2”. In some sense, we are relatively confident in our classification since <span class="math notranslate nohighlight">\(2/3\)</span> of the closest neighbors have this label.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One can think of several variants of nearest neighbors. For example, instead of using the <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors, one could use all the neighbors at distance at most <span class="math notranslate nohighlight">\(d\)</span> for some given value of <span class="math notranslate nohighlight">\(d &gt; 0\)</span>. In that case, if the number of such neighbors is small for a given <span class="math notranslate nohighlight">\(x\)</span>, one can decide to return no classification as there are not enough neighbors to make an informed decision. Another variant involves using all neighbors in the prediction, but to weight them with a weight that decreases with distance, say</p>
<div class="math notranslate nohighlight">
\[
\widehat{Y}(x) = \sum_{i=1}^n d_i y_i, 
\]</div>
<p>where <span class="math notranslate nohighlight">\(d_i = d_i(x) \geq 0\)</span>. The classical <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors classifier arises in that way by setting <span class="math notranslate nohighlight">\(d_i = 1/k\)</span> for the <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors and <span class="math notranslate nohighlight">\(d_i = 0\)</span> otherwise. Another possible choice for <span class="math notranslate nohighlight">\(d_i\)</span> is</p>
<div class="math notranslate nohighlight">
\[
d_i(x) = \frac{e^{-\|x-x_i\|_2^2}}{\sum_{i=1}^n e^{-\|x-x_i\|_2^2}}, 
\]</div>
<p>where points in the training set have a weight that decreases exponentially fast with distance. The denominator is used to make the weights sum to <span class="math notranslate nohighlight">\(1\)</span>.</p>
</div>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>Implement your own nearest neighbors classifier in Python. Your function should take <span class="math notranslate nohighlight">\(X, y, k, x\)</span> as input and return the <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors classifier for <span class="math notranslate nohighlight">\(x\)</span>. Also implement some of the variants described above.</p>
</div>
<p>When using nearest neighbors, a value of <span class="math notranslate nohighlight">\(k\)</span> needs to be chosen carefully. Although a small <span class="math notranslate nohighlight">\(k\)</span> leads to a small training error, the model may not generalize well (large test error). The value of <span class="math notranslate nohighlight">\(k\)</span> is typically chosen using <a class="reference internal" href="7-Learning-outside-training.html#sec-cross-validation"><span class="std std-ref">Cross validation</span></a>.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="16-LASSO-theoretical.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">15. </span>Theoretical guarantees for the LASSO</p>
      </div>
    </a>
    <a class="right-next"
       href="18-Lab6-nearest-neighbors.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">17. </span>Lab 6: nearest neighbors</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">16.1. Loss function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-entropy">16.1.1. Cross-entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example">16.1.1.1. Example</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-nearest-neighbors-model">16.2. The nearest neighbors model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">16.2.1. Example</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dominique Guillot
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>