
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>13. Computing the LASSO solution &#8212; Math 637 Mathematical Techniques in Data Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Content/14-Computing-lasso';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="14. Lab 5: Coordinate descent" href="15-Lab5-coordinate-descent.html" />
    <link rel="prev" title="12. Lab 4: using the LASSO" href="13-Lab4-lasso.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="1-intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/637-logo.png" class="logo__image only-light" alt="Math 637 Mathematical Techniques in Data Science - Home"/>
    <script>document.write(`<img src="../_static/637-logo.png" class="logo__image only-dark" alt="Math 637 Mathematical Techniques in Data Science - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="1-intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2-Python.html">1. Setting up Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="3-Basic-Python.html">2. Basic Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="4-Supervised-Unsupervised.html">3. Supervised vs Unsupervised learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="5-Linear-regression.html">4. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="6-Lab-Cars.html">5. Lab 1: Linear regression and the cars data</a></li>
<li class="toctree-l1"><a class="reference internal" href="7-Learning-outside-training.html">6. Learning outside the training set</a></li>
<li class="toctree-l1"><a class="reference internal" href="8-Lab2-train-test.html">7. Lab 2: Training vs testing error</a></li>
<li class="toctree-l1"><a class="reference internal" href="9-BLUE.html">8. Best linear unbiased estimator and the bias-variance decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-Lab3-gradient-descent.html">9. Lab 3: Gradient descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="11-Shrinkage-methods.html">10. Improving linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-Model-selection.html">11. Model selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-Lab4-lasso.html">12. Lab 4: using the LASSO</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">13. Computing the LASSO solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-Lab5-coordinate-descent.html">14. Lab 5: Coordinate descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="16-LASSO-theoretical.html">15. Theoretical guarantees for the LASSO</a></li>
<li class="toctree-l1"><a class="reference internal" href="17-categorical-data.html">16. Analyzing categorical data</a></li>
<li class="toctree-l1"><a class="reference internal" href="18-Lab6-nearest-neighbors.html">17. Lab 6: nearest neighbors</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Content/14-Computing-lasso.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Computing the LASSO solution</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#coordinate-descent">13.1. Coordinate descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#digression-convex-functions">13.2. Digression: convex functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-sets-and-functions">13.2.1. Convex sets and functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characterizations-of-convex-functions">13.2.2. Characterizations of convex functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sufficient-condition-for-the-convergence-of-coordinate-descent">13.2.3. Sufficient condition for the convergence of coordinate descent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-coordinate-wise-lasso-problem">13.2.4. The coordinate-wise LASSO problem</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-the-differentiable-part">13.2.4.1. Minimizing the differentiable part</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#digression-subdifferential-calculus">13.2.4.2. Digression: subdifferential calculus</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#back-to-the-lasso-solution">13.2.4.3. Back to the LASSO solution</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="computing-the-lasso-solution">
<h1><span class="section-number">13. </span>Computing the LASSO solution<a class="headerlink" href="#computing-the-lasso-solution" title="Link to this heading">#</a></h1>
<p>The LASSO if often used in high-dimensional problems. As we saw, cross-validation involves solving many lasso problems so it is important to be able to solve the LASSO problem effectively. We will now discuss possible strategies to compute the solution. Recall that the LASSO problem is:</p>
<div class="math notranslate nohighlight">
\[
\widehat{\beta}_\textrm{LASSO} = \mathop{\textrm{argmin}}_{\beta \in \mathbb{R}^p} \|y - X\beta\|_2^2 + \alpha \|\beta\|_1.
\]</div>
<p>Notice that the objective function to optimize is not differentiable. As a result, a simple gradient descent cannot be used.</p>
<section id="coordinate-descent">
<span id="s-coordinate-descent"></span><h2><span class="section-number">13.1. </span>Coordinate descent<a class="headerlink" href="#coordinate-descent" title="Link to this heading">#</a></h2>
<p>A popular approach that is often used to compute the LASSO solution is to use coordinate descent.</p>
<p><strong>Objective:</strong> Minimize a function <span class="math notranslate nohighlight">\(f: \mathbb{R}^p \to \mathbb{R}\)</span>.</p>
<p><strong>Strategy:</strong> Starting with an initial guess <span class="math notranslate nohighlight">\(x^{(0)} = (x_1^{(0)}, x_2^{(0)}, \dots, x_p^{(0)})\)</span>, for <span class="math notranslate nohighlight">\(k=0,1,\dots\)</span>, minimize each coordinate separately while cycling through the coordinates.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
x^{(k+1)}_1 &amp;= \mathop{\textrm{argmin}}_x f(x, x_2^{(k)}, x_3^{(k)}, \dots, x_p^{(k)}) \\
x^{(k+1)}_2 &amp;= \mathop{\textrm{argmin}}_x f(x_1^{(k+1)}, x, x_3^{(k)}, \dots, x_p^{(k)}) \\
x^{(k+1)}_3 &amp;= \mathop{\textrm{argmin}}_x f(x_1^{(k+1)}, x_2^{(k+1)}, x, x_4^{(k)}, \dots, x_p^{(k)})\\
&amp;\vdots \\
x^{(k+1)}_p &amp;= \mathop{\textrm{argmin}}_x f(x_1^{(k+1)}, x_2^{(k+1)}, \dots , x_{p-1}^{(k+1)}, x).
\end{align*}\]</div>
<p>This approach can be very efficient when the coordinate-wise problems are easy to solve (e.g. if they admit a closed-form solution). As we will see, this is the case for the LASSO problem.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="../_images/Coordinate_descent.png"><img alt="../_images/Coordinate_descent.png" src="../_images/Coordinate_descent.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.1 </span><span class="caption-text">An illustration of the coordinate descent process. Source: Wikipedia (Nicoguaro)</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Unfortunately, coordinate descent does <strong>not</strong> always converge. For example, in the example below, observe that if one start with the initial guess <span class="math notranslate nohighlight">\((-2,-2)\)</span>, then coordinate descent immediately stops since the function is minimized at that point with respect to the <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> axis directions. However, <span class="math notranslate nohighlight">\((-2,-2)\)</span> is not a local minimum of the function.</p>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="../_images/Nonsmooth_coordinate_descent.png"><img alt="../_images/Nonsmooth_coordinate_descent.png" src="../_images/Nonsmooth_coordinate_descent.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.2 </span><span class="caption-text">An example where coordinate descent does not converge to a local minimum. Source: Wikipedia (Nicoguaro)</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>However, under supplementary assumptions, one can show that coordinate descent converges to a local minimum of the objective function. To explain the context where it works, we first briefly discuss convex functions.</p>
</section>
<section id="digression-convex-functions">
<h2><span class="section-number">13.2. </span>Digression: convex functions<a class="headerlink" href="#digression-convex-functions" title="Link to this heading">#</a></h2>
<section id="convex-sets-and-functions">
<h3><span class="section-number">13.2.1. </span>Convex sets and functions<a class="headerlink" href="#convex-sets-and-functions" title="Link to this heading">#</a></h3>
<p>A set <span class="math notranslate nohighlight">\(\Omega \subseteq \mathbb{R}^n\)</span> is <strong>convex</strong> if for any <span class="math notranslate nohighlight">\(x, y \in \Omega\)</span> and any <span class="math notranslate nohighlight">\(0 \leq t \leq 1\)</span>,</p>
<div class="math notranslate nohighlight">
\[
tx + (1-t)y \in \Omega.
\]</div>
<p>Equivalently, given any two points in <span class="math notranslate nohighlight">\(\Omega\)</span>, the line joining them is entirely contained in <span class="math notranslate nohighlight">\(\Omega\)</span>.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/convex-set.png"><img alt="../_images/convex-set.png" src="../_images/convex-set.png" style="width: 500px;" />
</a>
</figure>
<p>Let <span class="math notranslate nohighlight">\(\Omega \subseteq \mathbb{R}^n\)</span> be convex. A function <span class="math notranslate nohighlight">\(f: \Omega \to \mathbb{R}\)</span> is said to be <strong>convex</strong> if for all <span class="math notranslate nohighlight">\(x, y \in \Omega\)</span> and all <span class="math notranslate nohighlight">\(0 \leq t \leq 1\)</span>,</p>
<div class="math notranslate nohighlight">
\[
f(t x + (1-t) y) \leq t \cdot f(x) + (1-t) \cdot f(y).
\]</div>
<p>In other words, the function is <strong>under</strong> the line joining any two points (the function “bends up”).</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/Fconvex.png"><img alt="../_images/Fconvex.png" src="../_images/Fconvex.png" style="width: 600px;" />
</a>
</figure>
<p><strong>Examples.</strong> Some examples of convex functions include <span class="math notranslate nohighlight">\(f(x) = x^2\)</span>, <span class="math notranslate nohighlight">\(f(x) = e^x\)</span>, <span class="math notranslate nohighlight">\(f(x) = |x|\)</span>, <span class="math notranslate nohighlight">\(f(x) = \| y - Ax\|_2^2\)</span>.</p>
</section>
<section id="characterizations-of-convex-functions">
<h3><span class="section-number">13.2.2. </span>Characterizations of convex functions<a class="headerlink" href="#characterizations-of-convex-functions" title="Link to this heading">#</a></h3>
<p>Convex functions can be recognized in several ways.</p>
<p><strong>Theorem.</strong> A function <span class="math notranslate nohighlight">\(f: \Omega \to \mathbb{R}\)</span> is convex if and only if its <em>epigraph</em></p>
<div class="math notranslate nohighlight">
\[
\textrm{epi}(f) := \{(x, y) \in \Omega \times \mathbb{R} : y \geq f(x)\}
\]</div>
<p>is a convex set.</p>
<p><strong>Theorem.</strong> A differentiable function <span class="math notranslate nohighlight">\(f\)</span> is convex if and only if</p>
<div class="math notranslate nohighlight">
\[
f(x) \geq f(y) + \nabla f(y)^T (x-y) \qquad \forall x, y \in \Omega.
\]</div>
<p><strong>Theorem.</strong> A twice differentiable function <span class="math notranslate nohighlight">\(f\)</span> is convex if and only if its Hessian matrix <span class="math notranslate nohighlight">\(\displaystyle \left(\frac{\partial^2 f}{\partial x_i \partial x_j}\right)\)</span> is positive semidefinite on <span class="math notranslate nohighlight">\(\Omega\)</span>.</p>
<p>In particular, the following criterion is very useful for recognizing convex smooth functions in one variable.</p>
<p><strong>Corollary.</strong> If <span class="math notranslate nohighlight">\(f: (a,b) \to \mathbb{R}\)</span> is twice differentiable, then</p>
<div class="math notranslate nohighlight">
\[
f \textrm{ is convex} \iff f''(x) \geq 0 \qquad \forall x \in (a,b).
\]</div>
<p>Some important operations that preserve convexity:</p>
<ol class="arabic simple">
<li><p>If <span class="math notranslate nohighlight">\(f_1, \dots, f_n\)</span> are convex and <span class="math notranslate nohighlight">\(w_i \geq 0\)</span>, then <span class="math notranslate nohighlight">\(\sum_{i=1}^n w_i f_i\)</span> is convex.</p></li>
<li><p>In particular, sums of convex functions are convex.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> is convex and <span class="math notranslate nohighlight">\(g: \mathbb{R} \to \mathbb{R}\)</span> is convex and non-decreasing, then <span class="math notranslate nohighlight">\(g(f(x))\)</span> is convex.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(f_1, \dots, f_n\)</span> are convex, then <span class="math notranslate nohighlight">\(\max_{i=1}^n f_i\)</span> is convex.</p></li>
</ol>
<p><strong>Reference:</strong> For more details about convex sets and functions, please see Boyd &amp; Vandenberghe. <a href="https://stanford.edu/~boyd/cvxbook/" target="_blank">Convex optimization</a>, Cambridge university press, 2004 (available for free).</p>
</section>
<section id="sufficient-condition-for-the-convergence-of-coordinate-descent">
<h3><span class="section-number">13.2.3. </span>Sufficient condition for the convergence of coordinate descent<a class="headerlink" href="#sufficient-condition-for-the-convergence-of-coordinate-descent" title="Link to this heading">#</a></h3>
<p>The following result provides a sufficient condition for the convergence of coordinate descent to a local minimum of the objective function.</p>
<p><strong>Theorem.</strong> (See Tseng, 2001). Suppose</p>
<div class="math notranslate nohighlight">
\[
f(x_1, \dots, x_p) = f_0(x_1, \dots, x_p) + \sum_{i=1}^p f_i(x_i) \qquad (f \in \mathbb{R}^p)
\]</div>
<p>satisfies</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(f_0: \mathbb{R}^p \to \mathbb{R}\)</span> is convex and continuously differentiable.</p></li>
<li><p><span class="math notranslate nohighlight">\(f_i: \mathbb{R} \to \mathbb{R}\)</span> is convex <span class="math notranslate nohighlight">\((i=1,\dots,p)\)</span>.</p></li>
<li><p>The set <span class="math notranslate nohighlight">\(X^0 := \{x \in \mathbb{R}^p : f(x) \leq f(x^0)\}\)</span> is compact.</p></li>
<li><p><span class="math notranslate nohighlight">\(f\)</span> is continuous on <span class="math notranslate nohighlight">\(X^0\)</span>.</p></li>
</ol>
<p>Then every limit point of the sequence <span class="math notranslate nohighlight">\((x^{(k)})_{k \geq 1}\)</span> generated by cyclic coordinate descent converges to a global minimum of <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p>Observe that the theorem makes the assumption that the non-differentiable part of the objective is <em>separable</em>, i.e., it can be written as a sum of functions that depend only on one of the variables. This is the case of the LASSO objective (take <span class="math notranslate nohighlight">\(f_i(x) = |x_i|\)</span>).</p>
<p>As a consequence of the theorem, we obtain that coordinate descent converges to a global minimum for the LASSO problem.</p>
<p><strong>Corollary.</strong> For the LASSO problem, the coordinate descent iterations converge to a global minimum of the objective function.</p>
<p>It remains to see how the coordinate-wise LASSO problem can be efficiently solved.</p>
</section>
<section id="the-coordinate-wise-lasso-problem">
<h3><span class="section-number">13.2.4. </span>The coordinate-wise LASSO problem<a class="headerlink" href="#the-coordinate-wise-lasso-problem" title="Link to this heading">#</a></h3>
<p>In order to be able to minimize the LASSO objective using coordinate descent, we need to be able to minimize the objective efficiently each coordinate at a time. Recall that the LASSO objecive is:</p>
<div class="math notranslate nohighlight">
\[
\|y - X\beta\|_2^2 + \lambda \|\beta\|_1. 
\]</div>
<p>Choose <span class="math notranslate nohighlight">\(1 \leq i \leq p\)</span>. Fix <span class="math notranslate nohighlight">\(x_j\)</span> for all <span class="math notranslate nohighlight">\(j \ne i\)</span>. We want to optimize the differentiable part of the objective over <span class="math notranslate nohighlight">\(x_i\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\min_{x_i} \|y - X\beta\|_2^2 + \lambda \sum_{k=1}^p |\beta_k| \\ 
&amp;=\min_{x_i} \sum_{l=1}^n \left(y_l - \sum_{m=1}^p X_{lm} \beta_m\right)^2 + \lambda \sum_{k=1}^p |\beta_k|.
\end{align*}\]</div>
<p>We first address the case of the differentiable part <span class="math notranslate nohighlight">\(\|y-X\beta\|_2^2\)</span>.</p>
<section id="minimizing-the-differentiable-part">
<h4><span class="section-number">13.2.4.1. </span>Minimizing the differentiable part<a class="headerlink" href="#minimizing-the-differentiable-part" title="Link to this heading">#</a></h4>
<p>Let us first evaluate the gradient of the differentiable part of the objective:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial}{\partial x_i}  \sum_{l=1}^n \left(y_l - \sum_{m=1}^p X_{lm} \beta_m\right)^2 &amp;=  \sum_{l=1}^n 2 \left(y_l - \sum_{m=1}^p X_{lm} \beta_m\right) \times (-X_{li}) \\
&amp;= 2 X_i^T (X \beta-y) \\
&amp;= 2 X_i^T (X_{-i} \beta_{-i}-y) + 2 X_i^T X_i \beta_i.
\end{align*}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(X_i\)</span> denotes the <span class="math notranslate nohighlight">\(i\)</span>-th column of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(X_{-i}\)</span> is the matrix obtained by deleting the <span class="math notranslate nohighlight">\(i\)</span>-th column of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>In order to deal with the non-differentiable part, we need to briefly discuss subdifferential calculus.</p>
</section>
<section id="digression-subdifferential-calculus">
<h4><span class="section-number">13.2.4.2. </span>Digression: subdifferential calculus<a class="headerlink" href="#digression-subdifferential-calculus" title="Link to this heading">#</a></h4>
<p>Suppose <span class="math notranslate nohighlight">\(f\)</span> is convex and differentiable. Then</p>
<div class="math notranslate nohighlight">
\[
f(y) \geq f(x) + \nabla f(x)^T (y-x).
\]</div>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="../_images/Boyd_Fig3p2.png"><img alt="../_images/Boyd_Fig3p2.png" src="../_images/Boyd_Fig3p2.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.3 </span><span class="caption-text">Boyd &amp; Vandenberghe, Figure 3.2.</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>We say that <span class="math notranslate nohighlight">\(g\)</span> is a <strong>subgradient</strong> of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span> if</p>
<div class="math notranslate nohighlight">
\[
f(y) \geq f(x) + g^T (y-x) \qquad \forall y.
\]</div>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="../_images/Boyd_subgrad.png"><img alt="../_images/Boyd_subgrad.png" src="../_images/Boyd_subgrad.png" style="width: 700px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.4 </span><span class="caption-text">Boyd, lecture notes.</span><a class="headerlink" href="#id4" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>We define</p>
<div class="math notranslate nohighlight">
\[
\partial f(x) := \{\textrm{all subgradients of } f \textrm{ at } x\}.
\]</div>
<p>The subgradient generalizes the usual gradient (derivative) for convex functions that are not necessarily differentiable.</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\partial f(x)\)</span> is a closed convex set (can be empty).</p></li>
<li><p><span class="math notranslate nohighlight">\(\partial f(x) = \{\nabla f(x)\}\)</span> if <span class="math notranslate nohighlight">\(f\)</span> is differentiable at <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\partial f(x) = \{g\}\)</span>, then <span class="math notranslate nohighlight">\(f\)</span> is differentiable at <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(\nabla f(x) = g\)</span>.</p></li>
</ol>
<p>In particular, when <span class="math notranslate nohighlight">\(f\)</span> is differentiable at <span class="math notranslate nohighlight">\(x\)</span>, observe that the subgradient <span class="math notranslate nohighlight">\(\partial f(x)\)</span> is the set <span class="math notranslate nohighlight">\(\{\nabla f(x)\}\)</span> that contains only the gradient of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span>. However, when <span class="math notranslate nohighlight">\(f\)</span> is not differentiable at <span class="math notranslate nohighlight">\(x\)</span>, its subgradient can contain more than one point.</p>
<p>The subgradient behaves as expected with respect to rescaling and adding functions:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\partial (\alpha f) = \alpha \partial f\)</span> if <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\partial(f_1 + f_2) = \partial f_1 + \partial f_2\)</span>.</p></li>
</ol>
<p><strong>Example:</strong> Let us compute the subgradient of the absolute value function.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/abs.png"><img alt="../_images/abs.png" src="../_images/abs.png" style="width: 300px;" />
</a>
</figure>
<p>When <span class="math notranslate nohighlight">\(x \ne 0\)</span>, the function is differentiable and so the subgradient contains only the value of the derivative. At <span class="math notranslate nohighlight">\(x=0\)</span> however, any slope between <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span> yields a line that remains under the curve. We therefore have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\partial f(x) = \begin{cases} \{-1\} &amp; \textrm{ if } x &lt; 0 \\ [-1,1] &amp; \textrm{ if } x = 0 \\ \{1\} &amp; \textrm{ if } x &gt; 0. \end{cases}
\end{split}\]</div>
<p>Now, derivatives provide a very powerful tools to locate the minima of a function. Indeed, recall that if <span class="math notranslate nohighlight">\(f\)</span> is convex and differentiable, then</p>
<div class="math notranslate nohighlight">
\[
f(x^\star) = \min_x f(x) \Leftrightarrow 0 = \nabla f(x^\star).
\]</div>
<p>The subdifferential calculus allows us to use a similar approach to location the minimum of convex (not necessarily differentiable) functions.</p>
<p><strong>Theorem.</strong> Let <span class="math notranslate nohighlight">\(f\)</span> be a (not necessarily differentiable) convex function. Then</p>
<div class="math notranslate nohighlight">
\[
f(x^\star) = \inf_x f(x) \Leftrightarrow 0 \in \partial f(x^\star).
\]</div>
<p><em>Proof.</em> We have <span class="math notranslate nohighlight">\(f(y) \geq f(x^\star)\)</span> for all <span class="math notranslate nohighlight">\(y\)</span> if and only if</p>
<div class="math notranslate nohighlight">
\[
f(y) \geq f(x^\star) + 0 \cdot (y-x^\star).
\]</div>
<p>This is equivalent to saying <span class="math notranslate nohighlight">\(0 \in \partial f(x^\star)\)</span>. ∎</p>
<p>Despite its simplicity, this is a very powerful and important result. We will now use it to compute the coordinate-wise solution of the LASSO problem.</p>
</section>
<section id="back-to-the-lasso-solution">
<span id="s-lasso-soln"></span><h4><span class="section-number">13.2.4.3. </span>Back to the LASSO solution<a class="headerlink" href="#back-to-the-lasso-solution" title="Link to this heading">#</a></h4>
<p>We can now complete our calculation to identify the coordinate-wise LASSO solution.</p>
<p>The function</p>
<div class="math notranslate nohighlight">
\[
f(x_i) := \|y - X\beta\|_2^2 + \alpha \sum_{k=1}^p |\beta_k|
\]</div>
<p>is convex. Its minimum is obtained if <span class="math notranslate nohighlight">\(0 \in \partial f(x^\star)\)</span>.</p>
<p>Let</p>
<div class="math notranslate nohighlight">
\[
g := \frac{\partial}{\partial x_i} \|y - X\beta\|_2^2=  2\left(X_i^T (X_{-i} \beta_{-i}-y) + X_i^T X_i \beta_i\right).
\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\partial f(x_i) = \begin{cases}
\{g- \lambda\} &amp; \textrm{ if } \beta_i &lt; 0 \\
[g -\lambda, g + \lambda] &amp; \textrm{ if } \beta_i = 0 \\
\{g + \lambda\} &amp; \textrm{ if } \beta_i &gt; 0
\end{cases}.
\end{split}\]</div>
<p>We will find out when each of the above conditions hold.</p>
<p>First,</p>
<div class="math notranslate nohighlight">
\[
g-\lambda = 0 \Leftrightarrow \beta_i = \frac{2 X_i^T (y-X_{-i} \beta_{-i}) + \lambda}{X_i^T X_i} = g^\star + \frac{\lambda}{\|X_i\|_2^2}.
\]</div>
<p>We therefore conclude that if <span class="math notranslate nohighlight">\(\beta_i = g^\star + \frac{\lambda}{\|X_i\|_2^2} &lt; 0\)</span>, i.e., if <span class="math notranslate nohighlight">\(g^\star &lt; -\frac{\lambda}{\|X_i\|_2^2}\)</span>, then <span class="math notranslate nohighlight">\(0 \in \partial f(\beta_i)\)</span>.</p>
<p>Similarly,</p>
<div class="math notranslate nohighlight">
\[
g+\lambda = 0 \Leftrightarrow \beta_i = \frac{2 X_i^T (y-X_{-i} \beta_{-i}) - \lambda}{X_i^T X_i} = g^\star - \frac{\lambda}{\|X_i\|_2^2}.
\]</div>
<p>Therefore, if <span class="math notranslate nohighlight">\(\beta_i = g^\star - \frac{\lambda}{\|X_i\|_2^2} &gt; 0\)</span>, i.e., if <span class="math notranslate nohighlight">\(g^\star &gt; \frac{\lambda}{\|X_i\|_2^2}\)</span>, then <span class="math notranslate nohighlight">\(0 \in \partial f(\beta_i)\)</span>.</p>
<p>What have thus proved so far:</p>
<ol class="arabic simple">
<li><p>There exists <span class="math notranslate nohighlight">\(\beta_i &lt; 0\)</span> such that <span class="math notranslate nohighlight">\(0 \in \{g-\lambda\}\)</span> if and only if <span class="math notranslate nohighlight">\(g^\star &lt; -\frac{\lambda}{\|X_i\|_2^2}\)</span>. In that case, the unique such <span class="math notranslate nohighlight">\(\beta_i\)</span> is</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\beta_i = g^\star + \frac{\lambda}{\|X_i\|_2^2}.
\]</div>
<ol class="arabic simple" start="2">
<li><p>There exists <span class="math notranslate nohighlight">\(\beta_i &gt; 0\)</span> such that <span class="math notranslate nohighlight">\(0 \in \{g+\lambda\}\)</span> if and only if <span class="math notranslate nohighlight">\(g^\star &gt; \frac{\lambda}{\|X_i\|_2^2}\)</span>. In that case, the unique such <span class="math notranslate nohighlight">\(\beta_i\)</span> is</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\beta_i = g^\star - \frac{\lambda}{\|X_i\|_2^2}.
\]</div>
<p>The remaining case is: <span class="math notranslate nohighlight">\(\beta_i = 0\)</span> and <span class="math notranslate nohighlight">\(0 \in [g-\lambda, g+\lambda]\)</span>.</p>
<p>Recall that</p>
<div class="math notranslate nohighlight">
\[
g = 2\left(X_i^T (X_{-i} \beta_{-i}-y) + X_i^T X_i \beta_i\right).
\]</div>
<p>Setting <span class="math notranslate nohighlight">\(\beta_i = 0\)</span>, we obtain:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
0 \in [g-\lambda, g+\lambda] &amp;\iff  g-\lambda \leq 0 \textrm{ and } g+\lambda \geq 0 \\
&amp;\iff -\lambda \leq 2X_i^T (y-X_{-i} \beta_{-i}) \leq \lambda \\
&amp;\iff  -\frac{\lambda}{\|X_i\|_2^2} \leq \frac{2 X_i^T (y-X_{-i} \beta_{-i})}{X_i^T X_i} \leq \frac{\lambda}{\|X_i\|_2^2} \\
&amp;= -\frac{\lambda}{\|X_i\|_2^2} \leq g^\star \leq \frac{\lambda}{\|X_i\|_2^2}.
\end{align*}\]</div>
<p><strong>Conclusion:</strong> <span class="math notranslate nohighlight">\(\beta_i = 0\)</span> and <span class="math notranslate nohighlight">\(0 \in [g-\lambda, g+\lambda]\)</span> hold precisely when</p>
<div class="math notranslate nohighlight">
\[
 -\frac{\lambda}{\|X_i\|_2^2} \leq g^\star \leq \frac{\lambda}{\|X_i\|_2^2}.
\]</div>
<p>We have shown the following:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
0 \in \partial f(\beta_i) \textrm{ if } \begin{cases}
\beta_i = g^\star + \frac{\lambda}{\|X_i\|_2^2} &amp;\textrm{ and } g^\star &lt; -\frac{\lambda}{\|X_i\|_2^2} \\
\beta_i = g^\star - \frac{\lambda}{\|X_i\|_2^2} &amp;\textrm{ and } g^\star &gt; \frac{\lambda}{\|X_i\|_2^2} \\
\beta_i = 0 &amp;\textrm{ and } -\frac{\lambda}{\|X_i\|_2^2} \leq g^\star \leq \frac{\lambda}{\|X_i\|_2^2}.
\end{cases}
\end{split}\]</div>
<p>Therefore, the minimum of <span class="math notranslate nohighlight">\(f(\beta_i)\)</span> is obtained at</p>
<div class="math notranslate nohighlight">
\[\begin{split}
x^\star = \begin{cases}
g^\star + \frac{\lambda}{\|X_i\|_2^2} &amp; \textrm{ if } g^\star &lt; -\frac{\lambda}{\|X_i\|_2^2} \\
g^\star - \frac{\lambda}{\|X_i\|_2^2} &amp; \textrm{ if } g^\star &gt; \frac{\lambda}{\|X_i\|_2^2} \\
0 &amp; \textrm{ if }  -\frac{\lambda}{\|X_i\|_2^2} \leq g^\star \leq \frac{\lambda}{\|X_i\|_2^2}.
\end{cases}
\end{split}\]</div>
<p>In other words,</p>
<div class="math notranslate nohighlight">
\[
x^\star = \eta^S_{\lambda/\|X_i\|_2^2}(g^\star) = \eta^S_{\lambda/\|X_i\|_2^2} \left(\frac{X_i^T (y-X_{-i} \beta_{-i}) }{X_i^T X_i}\right), 
\]</div>
<p>where <span class="math notranslate nohighlight">\(\eta_\epsilon\)</span> is the <em>soft-thresholding</em> function given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\eta^S_\epsilon(x) = \begin{cases}
x - \epsilon &amp; \textrm{ if } x &gt; \epsilon \\
x + \epsilon &amp; \textrm{ if } x &lt; -\epsilon \\
0 &amp; \textrm{ if } -\epsilon \leq x \leq \epsilon
\end{cases}.
\end{split}\]</div>
<p>The soft-thresholding shrinks the value of <span class="math notranslate nohighlight">\(x\)</span> until it hits zero (and then leaves it at zero). This is in contrast to the hard-thresholding function that sets large elements to <span class="math notranslate nohighlight">\(0\)</span>.</p>
<p><strong>Hard-thresholding:</strong></p>
<div class="math notranslate nohighlight">
\[
\eta^H_\epsilon(x) = x {\bf 1}_{|x| &gt; \epsilon}.
\]</div>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/fig_thres_hard.png"><img alt="../_images/fig_thres_hard.png" src="../_images/fig_thres_hard.png" style="width: 300px;" />
</a>
</figure>
<p><strong>Soft-thresholding:</strong></p>
<div class="math notranslate nohighlight">
\[
\eta^S_\epsilon(x) = \textrm{sgn}(x) (|x| - \epsilon)_+
\]</div>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/fig_thres_soft.png"><img alt="../_images/fig_thres_soft.png" src="../_images/fig_thres_soft.png" style="width: 300px;" />
</a>
</figure>
<p>In conclusion, to solve the lasso problem using coordinate descent:</p>
<ol class="arabic simple">
<li><p>Pick an initial point <span class="math notranslate nohighlight">\(\beta\)</span>.</p></li>
<li><p>Cycle through the coordinates and perform the updates</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\beta_i \rightarrow \eta^S_{\lambda/\|X_i\|_2^2} \left(\frac{2 X_i^T (y-X_{-i} \beta_{-i}) }{X_i^T X_i}\right).
\]</div>
<ol class="arabic simple" start="3">
<li><p>Continue until convergence (i.e., stop when the coordinates vary less than some threshold).</p></li>
</ol>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="13-Lab4-lasso.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">12. </span>Lab 4: using the LASSO</p>
      </div>
    </a>
    <a class="right-next"
       href="15-Lab5-coordinate-descent.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">14. </span>Lab 5: Coordinate descent</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#coordinate-descent">13.1. Coordinate descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#digression-convex-functions">13.2. Digression: convex functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-sets-and-functions">13.2.1. Convex sets and functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characterizations-of-convex-functions">13.2.2. Characterizations of convex functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sufficient-condition-for-the-convergence-of-coordinate-descent">13.2.3. Sufficient condition for the convergence of coordinate descent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-coordinate-wise-lasso-problem">13.2.4. The coordinate-wise LASSO problem</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-the-differentiable-part">13.2.4.1. Minimizing the differentiable part</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#digression-subdifferential-calculus">13.2.4.2. Digression: subdifferential calculus</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#back-to-the-lasso-solution">13.2.4.3. Back to the LASSO solution</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dominique Guillot
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>