
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>10. Improving linear regression &#8212; Math 637 Mathematical Techniques in Data Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Content/11-Shrinkage-methods';</script>
    <link rel="canonical" href="/637book/Content/11-Shrinkage-methods.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11. Model selection" href="12-Model-selection.html" />
    <link rel="prev" title="9. Lab 3: Gradient descent" href="10-Lab3-gradient-descent.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="1-intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/637-logo.png" class="logo__image only-light" alt="Math 637 Mathematical Techniques in Data Science - Home"/>
    <script>document.write(`<img src="../_static/637-logo.png" class="logo__image only-dark" alt="Math 637 Mathematical Techniques in Data Science - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="1-intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2-Python.html">1. Setting up Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="3-Basic-Python.html">2. Basic Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="4-Supervised-Unsupervised.html">3. Supervised vs Unsupervised learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="5-Linear-regression.html">4. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="6-Lab-Cars.html">5. Lab 1: Linear regression and the cars data</a></li>
<li class="toctree-l1"><a class="reference internal" href="7-Learning-outside-training.html">6. Learning outside the training set</a></li>
<li class="toctree-l1"><a class="reference internal" href="8-Lab2-train-test.html">7. Lab 2: Training vs testing error</a></li>
<li class="toctree-l1"><a class="reference internal" href="9-BLUE.html">8. Best linear unbiased estimator and the bias-variance decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-Lab3-gradient-descent.html">9. Lab 3: Gradient descent</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">10. Improving linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-Model-selection.html">11. Model selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-Lab4-lasso.html">12. Lab 4: using the LASSO</a></li>
<li class="toctree-l1"><a class="reference internal" href="14-Computing-lasso.html">13. Computing the LASSO solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-Lab5-coordinate-descent.html">14. Lab 5: Coordinate descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="16-LASSO-theoretical.html">15. Theoretical guarantees for the LASSO</a></li>
<li class="toctree-l1"><a class="reference internal" href="17-categorical-data.html">16. Analyzing categorical data</a></li>
<li class="toctree-l1"><a class="reference internal" href="18-Lab6-nearest-neighbors.html">17. Lab 6: nearest neighbors</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Content/11-Shrinkage-methods.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Improving linear regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#subset-selection">10.1. Subset selection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#shrinkage-methods">10.2. Shrinkage methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression-tikhonov-regularization">10.2.1. Ridge regression/Tikhonov regularization:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression">10.3. LASSO regression</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="improving-linear-regression">
<h1><span class="section-number">10. </span>Improving linear regression<a class="headerlink" href="#improving-linear-regression" title="Link to this heading">#</a></h1>
<p>We saw before that the least squares estimator is the best linear unbiased estimator for the regression coefficients. We will now look at some <em>biased</em> estimators that often result in a better <a class="reference internal" href="9-BLUE.html#s-bias-variance"><span class="std std-ref">bias-variance trade-off</span></a> and, therefore, in a lower MSE.</p>
<section id="subset-selection">
<span id="sec-subset-selection"></span><h2><span class="section-number">10.1. </span>Subset selection<a class="headerlink" href="#subset-selection" title="Link to this heading">#</a></h2>
<p>Consider the linear regression model</p>
<div class="math notranslate nohighlight">
\[
Y = \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p.
\]</div>
<p>Our goal is to estimate the coefficients <span class="math notranslate nohighlight">\(\beta_1, \dots, \beta_p\)</span>. When the number of variables is large, one may expect several of the variables to not contribute significantly to predicting <span class="math notranslate nohighlight">\(Y\)</span>. For example, in the <a class="reference internal" href="6-Lab-Cars.html#sec-lab1-cars"><span class="std std-ref">cars dataset</span></a>, some of the variables may not be good predictors for the price of the cars. Similarly, in the <a class="reference internal" href="4-Supervised-Unsupervised.html#e-gene-expression"><span class="std std-ref">gene expression example</span></a>, most of the genes considered are probably unrelated to a given cancer. The difficulty in such problems is to identify which predictors are relevant.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In terms of getting the smallest <em>training error</em> possible, the model with the most variables will always do better. However, in terms of <em>testing error</em> a model with a smaller number of variables will often perform better.</p>
</div>
<p>A first (perhaps naive) approach is to try all possible subsets of predictors and measure their test error. Recall that a set of size <span class="math notranslate nohighlight">\(p\)</span> has <span class="math notranslate nohighlight">\(2^p\)</span> subsets (each element can be picked or not picked, leading to 2 choices per element).  Quite surprisingly, the leaps and bounds procedure (<a href="https://www.jstor.org/stable/1267601" target="_blank">Furnival and Wilson</a>, 1974) makes it feasible to perform the regression for all subsets for <span class="math notranslate nohighlight">\(p\)</span> as large as <span class="math notranslate nohighlight">\(30\)</span> or <span class="math notranslate nohighlight">\(40\)</span> by using a clever implementation that avoids computing things several times. However, when <span class="math notranslate nohighlight">\(p\)</span> is larger, evaluating the test error associated to all subsets of predictors quickly becomes impossible.</p>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>Split the cars dataset from Lab 1 into a training and a testing set. You may want to restrict the predictors to the numerical ones for simplicity. For each subset of predictors, fit a linear model on the training set, and save its error on the test set. Use this to reproduce the figure below and find the subset of predictors that yields the best test error.</p>
<p>The following code may be useful to loop over subsets:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">itertools</span>
<span class="k">def</span> <span class="nf">findsubsets</span><span class="p">(</span><span class="n">S</span><span class="p">,</span><span class="n">m</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">set</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>
    
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">):</span>
    <span class="n">S</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">findsubsets</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">),</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">sets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">)):</span>
        <span class="c1"># Fit model on training data for given subset</span>
        <span class="n">scores</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span>    <span class="c1"># Compute test score</span>
</pre></div>
</div>
</div>
<p>The following figure displays the test error on the cars dataset for all possible subsets of the numerical predictors.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="../_images/cars-best-subset.png"><img alt="../_images/cars-best-subset.png" src="../_images/cars-best-subset.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 10.1 </span><span class="caption-text">Test error for subset selection on the cars dataset.</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="shrinkage-methods">
<h2><span class="section-number">10.2. </span>Shrinkage methods<a class="headerlink" href="#shrinkage-methods" title="Link to this heading">#</a></h2>
<p>In order to restrict the number of nonzero regression coefficients or their size, a popular approach is to add a penalty (or “price to pay”) for including a nonzero coefficient. For example, let <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span> be a parameter and consider the following problem:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}^0 = \textrm{argmin}_{\beta \in \mathbb{R}^p} \left(\|y - X\beta\|_2^2 + \lambda \sum_{i=1}^p {\bf 1}_{\beta_i \ne 0}\right).
\]</div>
<p>Here,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
{\bf 1}_{\beta_i \ne 0} = \begin{cases}
1 &amp; \textrm{ if } \beta_i \ne 0 \\
0 &amp; \textrm{ otherwise}.
\end{cases}
\end{split}\]</div>
<p>Observe how each nonzero coefficient <span class="math notranslate nohighlight">\(\beta_i\)</span> adds <span class="math notranslate nohighlight">\(\lambda\)</span> to the objective function to minimize. When optimizing over <span class="math notranslate nohighlight">\(\beta \in \mathbb{R}^p\)</span>, there is thus a “competition” between minimizing the error <span class="math notranslate nohighlight">\(\|y-X\beta\|^2\)</span> and the price <span class="math notranslate nohighlight">\(\lambda\)</span> paid for including each variable. When <span class="math notranslate nohighlight">\(\lambda\)</span> is large enough, each variable needs to significantly reduce the error in order to justify the cost of including it into the model. Otherwise, it is better to set <span class="math notranslate nohighlight">\(\beta_i = 0\)</span>.</p>
<p>When <span class="math notranslate nohighlight">\(\lambda = 0\)</span>, we recover the least squares model, while as <span class="math notranslate nohighlight">\(\lambda\)</span> increases, the model selects various subsets of variables. When <span class="math notranslate nohighlight">\(\lambda \to \infty\)</span>, all regression coefficients become <span class="math notranslate nohighlight">\(0\)</span> as it becomes too costly to include variables in the model.</p>
<p>In theory, the above solves our problem of identifying relevant subsets of variables. However, unfortunately, the above optimization problem is a combinatorial optimization problem that cannot be solved efficiently. We therefore need to look for alternatives.</p>
<section id="ridge-regression-tikhonov-regularization">
<h3><span class="section-number">10.2.1. </span>Ridge regression/Tikhonov regularization:<a class="headerlink" href="#ridge-regression-tikhonov-regularization" title="Link to this heading">#</a></h3>
<p>Ridge regression solves the following problem:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{\beta}^{\textrm{ridge}} &amp;= \textrm{argmin}_{\beta \in \mathbb{R}^p} \|y - X\beta\|_2^2 + \lambda \sum_{i=1}^p \beta_i^2 \\
&amp;= \textrm{argmin}_{\beta \in \mathbb{R}^p} \|y - X\beta\|_2^2 + \lambda \|\beta\|_2^2.
\end{align*}\]</div>
<p>Notice how the term <span class="math notranslate nohighlight">\(\sum_{i=1}^p {\bf 1}_{\beta_i \ne 0}\)</span> that previously counted the number of nonzero regression coefficients was replaced by a measure of “how large” the coefficients are (using the <span class="math notranslate nohighlight">\(\ell_2\)</span> norm <span class="math notranslate nohighlight">\(\|x\|_2^2 = \sum_{i=1}^p x_i^2\)</span>). Thus, the above problem penalizes having large regression coefficients.</p>
<p>One can show that the Ridge regression problem is equivalent to solving</p>
<div class="math notranslate nohighlight">
\[\begin{split}
&amp;\min_{\beta \in \mathbb{R}^p} \|y - X\beta\|_2^2 \\
&amp;\textrm{such that } \|\beta\|_2 \leq t
\end{split}\]</div>
<p>for some <span class="math notranslate nohighlight">\(t &gt; 0\)</span> that depends on <span class="math notranslate nohighlight">\(\lambda\)</span>. Ridge regression thus solves the usual regression problem, but looks for a solution in a ball of radius <span class="math notranslate nohighlight">\(t\)</span>. The solution can easily be written in closed form by using our previous <a class="reference internal" href="5-Linear-regression.html#sec-finding-optimal-coefficients"><span class="std std-ref">calculations</span></a> for least squares. Indeed, we have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla_\beta \left(\|y - X\beta\|_2^2 + \lambda \sum_{i=1}^p \beta_i^2\right) &amp;= 2 (X^TX \beta - X^Ty) + 2 \lambda \beta \\
&amp;= 2\left((X^TX + \lambda I)\beta - X^Ty\right).
\end{align*}\]</div>
<p>Therefore, the critical points satisfy</p>
<div class="math notranslate nohighlight">
\[
(X^T X + \lambda I)\beta = X^T y.
\]</div>
<p>Notice that the matrix <span class="math notranslate nohighlight">\((X^T X + \lambda I)\)</span> is positive definite, and therefore invertible. The unique solution to the Ridge regression problem is therefore</p>
<div class="math notranslate nohighlight">
\[
\beta^{\textrm{ridge}} = (X^T X + \lambda I)^{-1} X^T y.
\]</div>
<div class="admonition-positive-semidefinite-and-positive-definite-matrices admonition">
<p class="admonition-title">Positive semidefinite and positive definite matrices</p>
<p>A symmetric matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times n}\)</span> is said to be positive semidefinite if <span class="math notranslate nohighlight">\(x^T A x \geq 0\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span>. Similarly, it is said to be positive definite if <span class="math notranslate nohighlight">\(x^T A x &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n \setminus \{\mathbf{0}\}\)</span>. Such matrices appear naturally in many areas of sciences and engineering.</p>
<p>Equivalently, one can show that a matrix is positive semidefinite if and only if all its eigenvalues are nonnegative (and positive definite if and only if its eigenvalues are all positive).</p>
<p>The matrix <span class="math notranslate nohighlight">\(X^TX\)</span> above is positive semidefinite since for any <span class="math notranslate nohighlight">\(x \in \mathbb{R}^p\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
x^T (X^TX)x = (Xx)^t(Xx) = \|Xx\|_2^2 \geq 0.
\]</div>
<p>Thus, the eigenvalues of <span class="math notranslate nohighlight">\(X^TX\)</span> are non-negative. Adding <span class="math notranslate nohighlight">\(\lambda I\)</span> to it adds <span class="math notranslate nohighlight">\(\lambda\)</span> to each eigenvalue. Thus <span class="math notranslate nohighlight">\(X^TX + \lambda I\)</span> has positive eigenvalues and, in particular, is invertible (since <span class="math notranslate nohighlight">\(0\)</span> is not one of its eigenvalues).</p>
</div>
<div class="admonition-ridge-regression-a-university-of-delaware-success-story admonition">
<p class="admonition-title">Ridge regression: a University of Delaware success story!</p>
<p>Amazingly, Ridge regression was originally developed by Arthur Hoerl and Robert “Bob” Kennard at the University of Delaware. See the following <a href="https://lerner.udel.edu/seeing-opportunity/blue-hens-revolutionize-precursor-to-machine-learning/" target="_blank">article</a> for more details.</p>
</div>
<p>The Scikot-learn package has a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html" target="_blank">Ridge</a> object that can be used to solve the Ridge regression problem:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
</pre></div>
</div>
<p>The object has similar methods (fit, predict, etc.) as the linear regression object. See the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html" target="_blank">documentation</a> for more details.</p>
<p>Notice that, compared to least squares, the Ridge regression solution is unique. Adding <span class="math notranslate nohighlight">\(\lambda I\)</span> to the <span class="math notranslate nohighlight">\(X^TX\)</span> term thus provides a rigorous way to “regularize” the linear regression problem. This is particularly important when the number of samples <span class="math notranslate nohighlight">\(n\)</span> is smaller than the number of varibles <span class="math notranslate nohighlight">\(p\)</span>. In that case, the matrix <span class="math notranslate nohighlight">\(X^TX\)</span> is not invertible and the least squares solution is not unique. However, the Ridge regression coefficients are typically not equal to <span class="math notranslate nohighlight">\(0\)</span>. Instead, as the penalty parameter increases, the coefficients shrink towards <span class="math notranslate nohighlight">\(0\)</span> (without typically being exactly zero). Ridge regression is thus used as a way to <strong>regularize</strong> regression and not as a way to select the best predictors to use. As we will see in the next section, another modification of our original problem (called LASSO regression) allows us to select predictors.</p>
<p>To illustrate how Ridge regression behaves, let us try it on a standard dataset. We will use the <a href="https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset" target="_blank">diabetes</a> dataset that comes with scikit-learn. Please make sure that you understand the code below and can run it by yourself.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Load the diabetes dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Standardize the features</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Range of alpha (penalty) values</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Store coefficients for each alpha</span>
<span class="n">coefs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>

<span class="c1"># Convert list to NumPy array for plotting</span>
<span class="n">coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">coefs</span><span class="p">)</span>

<span class="c1"># Plot the coefficients as a function of the regularization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">coefs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">coefs</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Feature </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Alpha (Regularization strength)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Coefficient values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Ridge coefficients as a function of regularization&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

</pre></div>
</div>
<p>We obtain the following figure displying the behavior of the coefficients as a function of the penalty parameter.</p>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="../_images/ridge-example.png"><img alt="../_images/ridge-example.png" src="../_images/ridge-example.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 10.2 </span><span class="caption-text">Behavior of the Ridge regression coefficients as a function of the penalty parameter for the diabetes dataset.</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Observe how the coefficients shift towards zero as the penalty parameter increases. However, they are typically not equal to <span class="math notranslate nohighlight">\(0\)</span> for a given choice of the penalty parameter.</p>
</section>
</section>
<section id="lasso-regression">
<h2><span class="section-number">10.3. </span>LASSO regression<a class="headerlink" href="#lasso-regression" title="Link to this heading">#</a></h2>
<p>In contrast to Ridge regression, LASSO (Least Absolute Shrinkage and Selection Operator) regression does set coefficients to <span class="math notranslate nohighlight">\(0\)</span> and selected relevant subsets of predictors in the linear regression problem. LASSO solves the following problem:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{\beta}^{\textrm{lasso}} &amp;= \textrm{argmin}_{\beta \in \mathbb{R}^p} \left(\|y - X\beta\|_2^2 + \lambda \sum_{i=1}^p |\beta_i|\right) \\
&amp;= \textrm{argmin}_{\beta \in \mathbb{R}^p} \left(\|y - X\beta\|_2^2 + \lambda \|\beta\|_1\right).
\end{align*}\]</div>
<p>Observe that the penalty term now uses the <span class="math notranslate nohighlight">\(\ell_1\)</span> norm instead of the <span class="math notranslate nohighlight">\(\ell_2\)</span> norm as in Ridge regression. This small modification has a profound effect on the properties of the solution. As for Ridge regression, one can show that the above problem is equivalent to solving</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\min_{\beta \in \mathbb{R}^p} \|y-X\beta\|_2^2 \\
&amp;\textrm{such that } \|\beta\|_1 \leq t.
\end{align*}\]</div>
<p>Thus, LASSO regression minimizes the usual error, but looks for a solution in the <span class="math notranslate nohighlight">\(\ell_1\)</span> ball of radius <span class="math notranslate nohighlight">\(t\)</span>. Observe that the shape of balls in the <span class="math notranslate nohighlight">\(\ell_1\)</span> norm is different from the shape of the balls in the <span class="math notranslate nohighlight">\(\ell_2\)</span> norms. For example, let <span class="math notranslate nohighlight">\(v = (x,y) \in \mathbb{R}^2\)</span>. The standard <span class="math notranslate nohighlight">\(\ell_2\)</span> unit ball is</p>
<div class="math notranslate nohighlight">
\[
\|v\|_2 \leq 1 \iff \sqrt{x^2 + y^2} \leq 1 \iff x^2 + y^2 \leq 1. 
\]</div>
<p>This is the unit disk. However, for the <span class="math notranslate nohighlight">\(\ell_1\)</span> norm, we have</p>
<div class="math notranslate nohighlight">
\[
\|v\|_1 \leq 1 \iff |x| + |y| \leq 1. 
\]</div>
<p>Observe that the ball is bounded by the four lines <span class="math notranslate nohighlight">\(\pm x \pm y = 1\)</span> (consider four cases, depending on the signs of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>).</p>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="../_images/l1-ball.png"><img alt="../_images/l1-ball.png" src="../_images/l1-ball.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 10.3 </span><span class="caption-text">The <span class="math notranslate nohighlight">\(\ell_1\)</span> unit ball in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>.</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Let us now look at how the solution of the Ridge and the LASSO problem differ. Recall that each problem minimizes the error <span class="math notranslate nohighlight">\(\|y-X\beta\|_2^2\)</span> of the linear regression, but over <span class="math notranslate nohighlight">\(\ell_2\)</span> and <span class="math notranslate nohighlight">\(\ell_1\)</span> balls respectively. We illustrate the difference in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>.</p>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="../_images/Fig3p11.png"><img alt="../_images/Fig3p11.png" src="../_images/Fig3p11.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 10.4 </span><span class="caption-text">ESL, Fig. 3.11.</span><a class="headerlink" href="#id4" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>On the figure <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> is the least squares solution of the regression problem, i.e., the value of <span class="math notranslate nohighlight">\(\beta\)</span> that minimizes <span class="math notranslate nohighlight">\(\|y-X\beta\|_2^2\)</span>. The red curves are <a href="https://en.wikipedia.org/wiki/Level_set" target="_blank">level curves</a> of <span class="math notranslate nohighlight">\(\|y-X\beta\|_2^2\)</span>, i.e., on each red curve, the value of the error is the same. Observe that the value of the error on each red curve is larger than the error at <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> since <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> is where the error is minimized. As we move away from <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>, the error keeps increasing. The Ridge and LASSO solutions are obtained when a level curve intersects a unit ball (in the <span class="math notranslate nohighlight">\(\ell_2\)</span> norm for Ridge and in the <span class="math notranslate nohighlight">\(\ell_1\)</span> norm for LASSO). Observe how the shape of the <span class="math notranslate nohighlight">\(\ell_1\)</span> ball makes it more likely that the intersection will occur at a “corner” of the ball, i.e., at a location where one of the regression coefficients is equal to <span class="math notranslate nohighlight">\(0\)</span>. The above argument is far from a formal proof that the LASSO solution has many coefficients equal to <span class="math notranslate nohighlight">\(0\)</span>, but it provides some intuition to explain what happens.</p>
<p>Let us repeat our experiment with the diabetes dataset to see how the LASSO coefficients behave. We can again use scikit-learn:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
</pre></div>
</div>
<p>See the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html" target="_blank"> documentation</a>.</p>
<p>Let us modify our previous code to use this object instead of the Ridge object.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Load the diabetes dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Standardize the features</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Range of alpha (penalty) values</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Store coefficients for each alpha</span>
<span class="n">coefs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>

<span class="c1"># Convert list to NumPy array for plotting</span>
<span class="n">coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">coefs</span><span class="p">)</span>

<span class="c1"># Plot the coefficients as a function of the regularization</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">coefs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">coefs</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Feature </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Alpha (Regularization strength)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Coefficient values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Lasso coefficients as a function of regularization&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

</pre></div>
</div>
<p>We now obtain the following figure:</p>
<figure class="align-default" id="id5">
<a class="reference internal image-reference" href="../_images/lasso-example.png"><img alt="../_images/lasso-example.png" src="../_images/lasso-example.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 10.5 </span><span class="caption-text">Behavior of the LASSO regression coefficients as a function of the penalty parameter for the diabetes dataset.</span><a class="headerlink" href="#id5" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Observe how each cofficients becomes <span class="math notranslate nohighlight">\(0\)</span> once the penalty parameter reaches a certain point. Another surprising feature of the solution is that the coefficients are piecewise linear. This is very different from the Ridge solution! As we keep increasing the penalty parameter, more and more regression coefficients are set to <span class="math notranslate nohighlight">\(0\)</span>. In practice, one typically solves the LASSO problem for a small number of penalty parameters (say 10 or 100) and looks at the nonzero regression coefficients. These provide relevant candidate subsets of predictors to try instead of trying all possible subsets as in <a class="reference internal" href="#sec-subset-selection"><span class="std std-ref">subset selection</span></a>. One can then choose the model resulting in the smallest test error. In some problems, it is very interesting to look at the predictors selected by the LASSO. For example, in the <a class="reference internal" href="4-Supervised-Unsupervised.html#e-gene-expression"><span class="std std-ref">gene expression example</span></a>, it is very interesting to look at the literature and consult with experts to see if the genes selected by the LASSO are known to be related to a given cancer.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Keep in mind that the LASSO is normally used <strong>only to select relevant predictors</strong>. The estimated regression coefficients are not usually directly used. Instead, after using the LASSO to pick relevant variables, one typically fits a standard linear regression model to the subset of predictors.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="10-Lab3-gradient-descent.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">9. </span>Lab 3: Gradient descent</p>
      </div>
    </a>
    <a class="right-next"
       href="12-Model-selection.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Model selection</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#subset-selection">10.1. Subset selection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#shrinkage-methods">10.2. Shrinkage methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression-tikhonov-regularization">10.2.1. Ridge regression/Tikhonov regularization:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression">10.3. LASSO regression</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dominique Guillot
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>